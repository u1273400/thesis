A language model for the Wakirike language is developed in this chapter.  This model draws upon the premise that the grammar of a language is expressed in the character sequence pattern which is ultimately expressed in words and therefore the abstract grammar rules can be extracted and learned by a character-based RNN neural network.

\section{Data Preparation}
The Wakirike New Testament Bible served as the source of data for the deep neural network training.  As there wasn't a readily available soft or on-line copy of the Wakirike new testament Bible for use, this was typed, giving rise to a complete corpus word size of 668,522 words and a character count of 6,539,176 characters. The data set was then divided into 11 parts. Two parts dedicated for testing and validation and the remaining nine parts were used for training.

The Unicode representations of the character set consisting of letters and punctuation marks are one-hot encoded and batched for sequential input, each batch having a character sequence length of 30 characters.


\section{GRU Training}

The modified LSTM RNN known as the Gated Recurrent Unit (GRU) is employed for the neural network model in order to optimise network performance in terms of resource conservation.  GRUs have been shown to give similar performance to regular LSTMs however, with a lighter system resource footprint \citep{cho2014learning}. The GRU RNN used to train the Wakirike text corpus comprised an internal network size of 512 nodes for each layer and was 3 layers deep. Externally, 30 GRUs represented  the number of recurrent connections each connection representing a time step bearing contextual for the recurrent input sequence. 

To mitigate for over-fitting, due to the multi-layered high-dimensional depth of this neural network, a small learning rate of 0.001 was used. To further marginalise over-fitting the popular and effective dropout method \citep{srivastava2014dropout} for regularising deep neural networks was kept at 20\% such that only 80\% activations were propagated from one layer to the next and the remaining 20\% were zeroed out.

\section{Output Language Generation}
The neural network is trained for 10 epochs and achieves a prediction accuracy of 85\% on held-out data.  With the GRU model it is possible to seed this network with an input character and select from the top-N candidates thus causing the Neural network to generate its own sentences.  In this scenario, the network is said to perform language generation by immanently constructing its own sentences.  The generated language was found to be intelligibly similar to that of the training data. 

A clever use of this new corpus generated by the GRU language model of this work was to determine a word-based perplexity metric for the GRU neural language model. In this work, the word-based perplexity metric was determined from the output language generated by first estimating the word based perplexity on the training data.  The same perplexity calculation was then used on the generated neural language model corpus. The corpus size of the neural language model was made to be equivalent to that of the training data, that is containing 6,539,176 characters.  The perplexity calculation was based on a modified Kneser-Key 5-gram model with smoothing \citep{Heafield-estimate}.  The results discussed below showed that the GRU-based RNN model generated a superior model compared to the n-gram model that better matched the training data.

The evaluation of the GRU language model of the Wakirike language was performed using a perplexity measurement metric. The Perplexity metric applies the language model to a test data-set and measures how probable the test data-set is. Perplexity is a relative measure given by the formula:
%
\begin{equation}
PP(W)=P(w_1,w_2\dots w_N)^\frac{1}{N}
\label{ch5_eq1_ppx}
\end{equation}
%
%
\begin{equation}
PP(W)=\sqrt[N]{\prod_{i=1}^N\frac{1}{P(w_i|w_{i-1})}}
\label{ch5_eq2_ppx}
\end{equation}
%
Where $w_1,\dots,w_N$ are the sequence of words. The language model with the lower relative perplexity score is therefore expected to yield better approximation of the data when applied to unseen data generally.

The result of the training of the GRU-Cell Recurrent Neural Network on low-resourced Wakirike Language gave impressive and intelligible results and showed better results when measured with standard n-gram language models. The results showed that it is indeed possible to use a GRU-based RNN on a low resource character sequence corpus to produce a Wakirike language model.

A character based perplexity metric is possible using  the negative log likelihood of the character sequence.
\begin{equation}
    PP(X)=exp\left\{−\frac{\sum_{t=1}^T\log P(x_t|x_{1:t−1})}{T}\right\}
\label{ch5_eq3_ppx}
\end{equation}

However, our base-line language model is a 5-gram word-based language model.  Therefore, comparing a word based model to a character based model requires a conversion step. In this work, the conversion step involved using the GRU language model generated a corpus which was rescored by re-estimating with a 5-gram word-based language model

Table ~\ref{tab:example} shows the Results of the Perplexity model of the LSTM Wakirike Language model and an equivalent 5-gram Language model with interpolation and Keysner smoothing \citep{Heafield-estimate} for various lengths of the held-out data.


\begin{table}
  \caption{Perplexity Calculation results}
  \label{tab:example}
\begin{tabular}{lrr}
\toprule
Language Model & Perplexity  \\
\midrule
Held-out data size (characters) & 998 & 99\\
\midrule
LSTM RNN & 1.6398 & 1.7622\\
3-gram with Keysner Soothing and interpolation & 1.8046 & 1.9461\\
\bottomrule
\end{tabular}
\end{table}
\section{Chapter Summary}
This chapter shows the application of a character-based Gated Recurrent Unit RNN on the low resource language of Wakirike to generate a language model for the Wakirike language.  The data-set and preparation and the details of the network were discussed.  The output of this model was used to hallucinate the Wakirike language which was then scored against word-based perplexity to obtain a metric against the baseline language model.

It can be inferred that the GRU character-model developed has an improved language model and because it is based on a character-model, which is fine-grained when compared to a word model, it is likely to generalise data better when used in practice and is less biased than a word-based model.  This can be observed from the fact that the output corpus produced a larger vocabulary size.
