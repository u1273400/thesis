\startblue
Throughout the development of this thesis, the establishment of deep learning as a strategy where computers learn through representation of patterns at varying degrees of complexity has been an underlying theme.  It was also emphasised how this is achieved by internal layer-wise encapsulations. Structures discussed in Chapter \ref{ch2litrev}, such as layer-wise stacking of neural network type architectures such as the \acrfull{rbm} and \acrfull{dbn} were used to implement such representations.  

In this chapter, the end-to-end Bi-directional Recurrent Neural Network model is described.  \acrshort{birnn} for speech recognition tasks is  employed here as opposed to regular \acrshort{rnn}s or \acrshort{dbn}s mentioned above in the preceding paragraph.  \acrshort{birnn}s are used because of the contextual nature of speech.  In Chapter \ref{ch6_wlm} it was demonstrated  how deep stacking of \acrshort{gru}s outperform single-layer \acrshort{rnn}s for extended sequences. That is to say, words in a sentence or paragraph are contextual to the sentence/paragraph over particularly long sequences and, these word contexts are better captured by the GRU architecture.  More importantly, \acrshort{birnn}'s have a forward and backward \acrshort{rnn} and these give the neural network the ability to analyse (look-up) the words from the backward RNN not currently seen by the forward RNN in the sentence succinctly giving the BiRNN parameters a contextual feature \citep{graves2006connectionist}.  

In addition to the procedure for designing sequence-to-sequence \acrshort{rnn}s outlined in Section \ref{sec_341_rnnproc}, this Chapter describes the training data, data preprocessing, derivation of feature vectors and output decoding.  First, speech features developed by making use of the deep scattering convolution networks \acrshort{dsn} is discussed.   The \acrshort{dsn}s are used as inputs to the end-to-end model.  Two end-to-end networks are then described.  The core \acrshort{birnn} network and a second \acrshort{birnn} network augmented with an RNN-transducer and an attention mechanism. A formal presentation of the speech neural network model parameters and architecture is given and the decoding algorithm is also detailed in sections contained within this chapter.  Finally, the results are presented and the findings from the model results discussed.\stopblue

\section{Deep Scattering Features}\label{sec_c7_wparams}
\textcolor{blue}{In Chapter 4, we derived a fast wavelet transform from a low pass filter and a high pass filter.  The speech features used for the BiRNN is obtained from successive wavelet-modulus operations of a deep scattering network 2 layers deep.  This 2-layer \acrshort{dsn} comprises a first-order scatter transform. The wavelet modulus operator is derived from the combination of a low pass filter and a band pass filter}.  Hyper parameters of the system included the window period for each sampled sub section, $T$;  The Q-band value for the band pass filter and the number of wavelets $J$ at each scattering layer for the total number of layers, $M=2$.

The matlab scatnet toolbox \citep{anden2014scatnet}, used to determine the scatter coefficient features for this research, provides optimal values for hyper parameters for audio signal processing into scatter features.  In this regime the value for the hyper parameter $T=512$ samples per window. This corresponds to a window of $50$ milliseconds for the audio signals sampled at $8000 Hz$.  For the zeroth scattering layer the $Q$-band parameter was $Q=8$ and the first scattering layer took the value  $Q=1$.  Finally $J$ is pre-calculated based on the value of $T$.  These after Scat-Net processing produce a feature-vector having $165$ dimensions.  These feature vectors in turn are used as inputs to the bi-direction neural network model whose architecture is described in succeeding sections.

\textcolor{blue}{For the second end-to-end architecture involving a transducer with attention mechanism, a period of  is used to capture a window of 4 seconds for audio signals sampled at 16000Hz. The same Q-band parameters having Q=8 for the zeroth layer and Q=1 for successive layers are used, In addition, the total number of layers deep was M=3 giving rise to a 2nd-order Scatter Network. This produced a feature-vector having 250 dimensions.}

\section{CTC-BiRNN Architecture}\label{sec_c7_birnn}

\textcolor{blue}{The \acrshort{ctc}-\acrshort{birnn} sequence model design follows the synchronous \acrshort{mimo} design described in Section \ref{sec_postalign}. As a result of the CTC-decoder implementation in Section \ref{sec_c7_ctc_decoder}, however, the decoder converts \acrshort{birnn} model from a synchronous \acrshort{mimo} to an asynchronous one.}

The core of the system is a bidirectional recurrent neural network (BiRNN) trained to ingest scatter coefficients described in the previous section, in order to generate English text transcriptions.  An end-to-end system therefore specifies that utterances $x$ and the corresponding label $y$ be sampled from a training set such that the sample $S = {(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), . . .}$.   In our end-to-end model, each utterance, $x^{(i)}$ is a processed feature vector consisting of $165$ dimensions.  Recall, every window passes through a scattering transform to yield an input of vector of $p=165$ features; consequently,   $x^{(i)}_{t,p}$ denotes the $p$-th feature in a scatter transform at time $t$.  

GPU training of the speech model architecture developed above was conducted using Mozilla Deepspeech \citep{mozilla_2019} CTC bi-directional RNN implementation along with the accompanying Mozilla Common voice data set  \citep{ardila2019common}.  The Common Voice Data set project consists of voice samples in short recordings approximately $4$ seconds each.  The complete data set is about $250$ hours of recording divided into training, test and development subsets.  The BiRNN, given the input sequence, $x$, outputs a sequence of probabilities $y_t=\mathbb{P}(c_t|x)$,  where $c_t \in a,b,c,\dots,z,space,apostrophe,blank$. 

The actual architecture of our core Bi-RNN is similar to the deepspeech system described in \cite{hannun2014deep}. This structure constitutes 5 hidden layers and one output layer.  The first three layers are regular DNNs followed by a bi-directional recurrent layer. As such, the output of the first three layers are computed by:
\begin{equation}
    h^{(l)}_t = g(W^{(l)} h^{(l−1)}_t + b^{(l)})\label{ch06_01_l1-3}
\end{equation}

$g(\cdot) = min\{max\{0,z\},20\}$  is the clipped rectified linear unit and $W^{(l)},b^{(l)}$ are weight matrix and bias parameters for layer  as described in sections \ref{dnn} and \ref{deepspeech} respectively.

It was shown in chapter \ref{ch3RNN} the recurrent layer comprise a forward and backward RNNs whose equations are repeated here for reference
\begin{equation}
    h^{(f)}_t = g(W^{(4)} h^{(3)}_t + W^{(f)}_r h^{(f)}_{t−1} + b^{(4)})
    \label{ch06_02_fwd}
\end{equation}
\begin{equation}
h^{(b)}_t = g(W^{(4)} h^{(3)}_t + W^{(b)}_r h^{(b)}_{t+1} + b^{(4)})    \label{ch06_03_bwd}
\end{equation}

Consequently, $h^{(f)}$ is the sequential computation from $t=1$ to $t=T^{(i)}$ for the $i$-th utterance and $h^{(b)}$ is the reverse computation from $t=T^{(i)}$ to $t=1$.  In addition the output from layer five is summarily given as the combined outputs from the recurrent layer:
\begin{equation}
h^{(5)} = g(W^{(5)} h^{(4)} + b^{(5)})    \label{ch06_04_l5}
\end{equation}
where $h^{(4)} = h^{(f)} + h^{(b)}$. The output of the Bi-RNN on layer 6 is a standard soft-max layer that outputs a predicted character over probabilities for each time slice $t$ and character $k$ in the alphabet:
\begin{equation}
h^{(6)}_{t,k} = \hat{y}_{t,k} \equiv \mathbb{P}(c_t = k \mid x) = \frac{\exp{ \left( (W^{(6)} h^{(5)}_t)_k + b^{(6)}_k \right)}}{\sum_j \exp{\left( (W^{(6)} h^{(5)}_t)_j + b^{(6)}_j \right)}})    \label{ch06_05_l6}
\end{equation}

$b^{(6)}_k$ takes on the -th bias and $(W^{(6)} h^{(5)}_t)_k$ is the matrix product of the $k$-th element.  The error of the outputs are then computed using the CTC loss function \cite{graves_2014} as described in chapter \ref{ch3RNN}.  A summary of our model is illustrated in Figure \ref{fig_6_1_ctc_scatter}.
\begin{figure}
\centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=14cm]{thesis/images/ctc_scatter.png}\\
  \caption{Deep scattering Bi-RNN Model} \label{fig_6_1_ctc_scatter}
\end{figure}

\subsection{CTC Decoding} \label{sec_c7_ctc_decoder}

In chapter three the CTC loss function algorithm was established as being able to maximise the probability of two cases.  The first case of transiting to a blank and the second case of transiting to a non blank.  In this section, this concept is used to enable decoding of the network output from posterior distribution output to character sequences which can be measured against a reference transcription using either character error rate (CER) or word error rate (WER).

Recall, all the output symbols are in the alphabet $\Sigma$ and augmented with the blank symbol. The posterior output of the CTC network is the probability of the symbol given the speech feature input $p(c|x_t)$ at time $t$ for $t=1,\dots,T$ and $T$ is the length of the input sequence.  Also recall two further sets of probabilities also being maintained by the model are the probability of a blank character $p_b$ and that of a non blank character $p_{nb}$.

Several strategies have been employed to obtain a translation string from the output of the deep neural network.  The prefix beam search employed by the CTC decoder of this research is derived from an initial greedy approximation, where at each time step determine the argument that maximises the  probability $p(c|x_t)$ at each time step. Let $C=(c_1,\dots,c_T$ be the character string then, the greedy approach has 
\begin{equation}
    c_t=arg\max_{c\in\Sigma}p(c|x_t)
\end{equation}
However, this simple approximation is unable to collapse repeating sequences and remove blank symbols. In addition, the approximation is unable to include the constraint of a lexicon or language model.

The prefix beam search algorithm \cite{hannun2014first} adopted in this work incorporates a language model derived from a lexicon in addition to keeping track of the various likelihoods used for decoding.  For the language model constraint, the transcription $W$ is recovered from acoustic input $X$ at time $t$ by choosing the word which maximising the posterior probability:
\begin{equation}
W_i=arg\max_{W_i \in \Sigma_W} p_{net}(W;X)p_{lm}(W)
\label{eqn_c6_decoder01}
\end{equation}
In equation \ref{eqn_c6_decoder01}, the Bayes product of language model prior $p_{lm}$ and the network output $p_{net}$ are utilised to maximise the probability of a particular character-word sequence in the lexicon given by $\Sigma_W$.  The overall calculation used to derive the final posterior distribution includes word insertion factors ($\alpha$ and $\beta$) used to balance the highly constrained n-gram language model.

The second strategy adopted by the prefix beam search which improves the decoding algorithm is the beam search strategy.  With this approach, the search maintains all possible paths; however, it retains only $k$ number paths which maximise the output sequence probability.  Improvements gained with this method are seen when certain maximal paths are made obsolete owing to new information derived from the multiple paths in being maintained in memory. 

The recursive prefix beam search algorithm illustrated in Figure \ref{fig_c6_decoder01} attempts to find the string formulated in equation \ref{eqn_c6_decoder01}.  Two sets prefixes $A_{prev}$ and $A_{nxet}$ are initialised, such that at $A_{nxet}$ maintains the prefixes in the current time-step while $A_{prev}$ maintains only $k$-prefixes from the previous time-step.  Note that at the end of each time step $A_{prev}$ is updated with only -most probable prefixes from $A_{nxet}$. Therefore while,  $A_{nxet}$ contains all the possible new paths from based on $A_{prev}$ as a Cartesian product of $A_{prev} \times \Sigma \in \mathcal{Z}^k\times\mathcal{Z}^{|\Sigma|}$ where $|\Sigma|$ is the length of $\Sigma$. The probabilities of each prefix obtained at each time step are the sum of the probability of non-blank plus the probability of a blank symbol.
\begin{sidewaysfigure}[ht]
    \includegraphics[width=22cm]{ctc}
    \caption{Prefix beam search algorithm}
    \label{fig_c6_decoder01}
\end{sidewaysfigure}

At every  time step and for every prefix $\ell$ currently in $A_{prev}$, a character from the alphabet $\Sigma$ is presented to the prefix. The prefix is only extended only when the presented symbol is not a blank or a space. $A_{nxet}$ and $A_{prev}$ maintain a list of active prefixes at the previous time step and proposed prefixes at the next time step respectively, The prefix probability is given by multiplying the word insertion term by the sum of the blank and non-blank symbol probabilities.
\begin{equation}
p(\ell|x_{1:t})=(p_{nb}(\ell|x_{1:t})+p_b(\ell|x_{1:t}))|W(\ell)|^\beta
\label{eqn_c6_decoder03}
\end{equation}

$W(\cdot)$ is obtained by segmenting all the characters in the sequence with the space-character symbol and truncating any characters trailing the  set of words in the sequence.  The prefix distribution however varies slightly depending on network output character being presented.

$\ell_{end}$ is the variable representing the last symbol in the prefix sequence in $A_{prev}$. If the symbol presented is the same as $\ell_{end}$ then the probability of a non-blank symbol,$p_{nb}=0$ . If the symbol being presented is blank then we do not extend the prefix.  Finally, if the symbol being presented is a space then we invoke the language model as follows
\begin{equation}
p(\ell^+|x_{1:t})=p(W(\ell^+)|W(\ell))^\alpha(p_{nb}(\ell|x_{1:t})+p_b(\ell|x_{1:t}))|W(\ell)|^\beta
\label{eqn_c6_decoder03}
\end{equation}

Note that $p(W(\ell^+)|W(\ell))$ is set to $0$ if the current word $W(\ell^+)$ is not in the lexicon. This becomes a constraint to enforce all character strings to consist only of words in the lexicon.  Furthermore,  $p(W(\ell^+)|W(\ell))$ is extended to include all the character sequences representing number of words considered by the n-gram language model by constituting the last $n-1$ words in character sequence $W(\ell)$.

\subsection{Model Hyper parameters}
The hidden layer matrix for each layer comprised 1024 hidden units (6.6M free parameters).  The weights are initialised from a uniform random distribution having a standard deviation of 0.046875.  The Adam optimisation algorithm \citep{kingma2014adam} was used with initial learning rate of, and a momentum of 0.95 was deployed to optimise the learning rate.

The network was trained for a total of five to fifty epochs over the training set for experiments conducted. The training time for Python GPU implementation is shown in Table \ref{tab_c6_01_training}.  For decoding with prefix search we use a beam size of $200$ and cross-validated with a held-out set to find optimal settings for the parameters $\alpha$ and $\beta$. Figure \ref{fig_6_3_wer} shows word error rates for various GPU configurations and audio data-set sizes.

\startblue
\section{Summary of \acrshort{birnn} Experiment Design}
The details of the \acrshort{birnn} model has been outlined in Sections \ref{sec_c7_birnn} and \ref{sec_c7_ctc_decoder}.  This section now summarises the process of the \acrshort{birnn} experiment from data collection to output text transcriptions.  Recall once again, as this is an end-to-end experiment the input-end will comprise raw audio speech utterances and at the output-end will be the character sequences which are resolved into words using a language model. The design of this experiment therefore utilises the \acrshort{birnn} to ingest raw audio utterances as preprocessed scatter-transforms and outputs text transcription which can be compared against the original audio transcriptions.

The audio clips and the corresponding transcriptions are downloaded with a script and the subsequent locations are stored into a configuration file. Being an end to end process, no further data pre-processing is required except conversion of the audio file from a binary format to a numeric-text format.  From this numeric text format, the scatter-transforms are computed when loaded from the configuration file.  

The \acrshort{ctc}-algorithm discussed in Sections \ref{sec_c4_ctcloss} and \ref{sec_c7_ctc_decoder} is responsible for correcting fuzzy alignments between audio input and output text.  This relationship according to Section \ref{sec_341_rnnproc} is an asynchronous \acrshort{mimo}, but the \acrshort{birnn} represents a synchronous \acrshort{mimo}. Hence a synchronous \acrshort{mimo} is combined with the CTC-decoder such that while \acrshort{birnn} takes care of the \acrlong{mimo} relationship the \acrshort{ctc} decoder takes care of collapsing output characters and, therefore, restoring the asynchronous relationship between outputs and inputs.  For steps 1 and 2 of Section \ref{sec_341_rnnproc} we use a \acrshort{birnn}.  Details of the internal structure of the \acrshort{birnn} from Sections \ref{sec_c7_birnn} and \ref{sec_c7_ctc_decoder} are used for step 3.  This includes 3 hidden regular \acrshort{dnn} layers and two bi-directional \acrshort{lstm}s hidden-layers and a softmax output layer.  Neural network saturation parameters are selected based on desired training time, similar research practice, and accuracy expectations in line with the research objectives. They include weight initialization having a mean of 0 and standard deviation of 0.046875; clipped \acrshort{relu} non-linear function (see Chapter \ref{ch4DSN}); learning rate of 0.001 with 0.95 momentum.  Finally, the adam optimiser and CTC-loss function (Section \ref{sec_c4_ctcloss}) are incorporated at the training output stages.

The \acrshort{ctc} decoder described in Section \ref{sec_c7_ctc_decoder} is then used to determine the characters from the softmax output.  The \acrshort{ctc} decoder had a look ahead beam search parameter of 200 characters.

\section{BiRNN with Attention Transducer end-to-end Architecture}\label{sec_7_5_blstm_t}

The core of this model is a CTC-Transformer+Attention Transducer model.  Together these two architectures achieve joint speech training and decoding.  The CTC-Transformer model is based on a Bi-LSTM similar to what is obtainable in the DeepSpeech model. There are up to 11 variants of Attention networks implemented in \acrshort{espnet}, however, the results of the experiments done this work experiment was determined from the attention model described in \cite{chorowski2015attention}.  Moreover, the multi-objective training was performed with equal weights on both the CTC-transformer and the Attention-Transducer.  Finally the system was trained for 20-200 epochs depending on the design goals and accuracy required.

This model uses the asynchronous \acrshort{mimo} model.  Although this model would require more neural network layers and about 2 times more \acrshort{rnn} units than the synchronous \acrshort{mimo}-\acrshort{rnn} model, ultimately, addition of Attention-based models ensures faster convergence and ultimately faster time to train.  This can only be achieved using asynchronous \acrshort{mimo} models which are the only models that support attention-mechanism.

Using a weighting function, $\alpha$, one can control how much bias either the CTC-Transform or the Attention-Transducer will get during training.  The joint training helps to improve robustness as well as achieve fast convergence.

\begin{equation}
    \mathcal{L}=\alpha\mathcal{L}^{ctc}+(1-\alpha)\mathacal{L}^{att}
    \label{eqn_c7_esp00}
\end{equation}

At the same time joint decoding of labels is integrated with the character based RNN-language modelling. The log probability of the RNNLM-integrated decoding of character labels is as follows

\begin{equation}
    \log p(y_n|y_{1:n−1},\mathbf{h}_{1:T‘})=\log p^{hyp}(y_n|y_{1:n−1},\mathbf{h}_{1:T‘})+\beta\log p^{lm}(y_n|y_{1:n−1})
    \label{eqn_c7_esp01}
\end{equation}
Where joint decoding, $\log p^{hyp}(y_n|y_{1:n−1},\mathbf{h}_{1:T‘})$ is given by
\begin{equation}
    \log p^{hyp}(y_n|y_{1:n−1},\mathbf{h}_{1:T‘})\\
    & = \alpha\log p^{ctc}(y_n|y_{1:n-1},\mathbf{h}_{1:T'})+(1-\alpha)\log p^{att}(y_n|y_{1:n-1},\mathbf{h}_{1:T'})
\end{equation}


\section{Summary of \acrshor{birnn} with Attention Transducer Experiment Design}
According to the stepwise procedure of deriving \acrshort{rnn} sequence-to-sequence models , the \acrshort{birnn} with Attention Transducer model incorporates an asynchronous \acrshort{mimo} design for Step 1 (Section \ref{sec_341_rnnproc}).  For steps 2 and 3 (Section \ref{sec_341_rnnproc}), 6 layers similar to the \acrshort{birnn}-only design consisting 3 regular \acrshort{dnn} layers, 2 \acrshort{blstm} and the output softmax layer. Neural network components  The number of neurons for the hidden layers were 2048 neurons.  Network saturation parameters were similar to the \acrshort{birnn}-only experiment setup having the number of epochs between 20-200 epochs and in line with research objectives.  As the training was significantly faster in this setup, more epochs could be incorporated into the experiments.  The initial experiment had only 20 epochs using baseline experiments and to determine how fast the training would be.  Subsequent experiments were between 100 and 200 epochs according to desired accuracy and research objectives.  Weights were initialized between 1 and -1 according to a normal distribution.  Learning rate was 0.001; Non linear function was a clipped \acrshort{relu}.  Cost function and optimizer was ctc-loss and ada-grad variant. Decoding was done according to the joint decoding function described in Section \ref{sec_7_5_blstm_t}.  Finally, a drop out of 10\% was used to avoid over fitting of the model.

\section{Speech Model Baselines}
The model baselines were trained alongside their scatter transform counterparts.  In addition, we adopted the model produced by the Mozilla DeepSpeech team.  This model had a similar architecture with 5 hidden units and 2048 hidden units.  This baseline was trained on Librespeech corpus and the common voice data corpora \citep{panayotov2015librispeech, ardila2019common}.   Study by \cite{hannun2014first} reported successful character error rate (CER)  using deep neural network (DNN), recurrent deep neural network with only forward temporal connections (RDNN), and also bi-directional recurrent neural networks (BRDNN). The models used in their study had 5 hidden layers having either 1,824 or 2,048 hidden units in each hidden layer.  

Word Error Rates obtained by this additional model were optimised after 75 epochs, learning rate of 0.0001 and a dropout rate of 15\%.  In addition, the language model hyper parameters for alpha and beta were 0.75 and 1.85 respectively.  This achieved 8\% WER. This model was developed using MFCC features of the training corpora.

\section{Speech Model Simulations}

Speech model training experiments were carried out on the two different end-to-end models as well as on different \acrshort{gpu} configurations. The first set of experiments was performed for the \acrshort{birnn}-only model.  The first \acrshort{gpu} configuration for the \acrshort{birnn}-only model consisted of 2 \acrshort{gpu}s having a total of 10 gigabytes of memory. The second \acrshort{gpu} configuration comprised 5 \acrshort{gpu}s having a total of 15 gigabytes of memory. Experiments for the BiRNN end-to-end model with transducer and attention were also performed using a \acrshort{gpu} configuration having 4 gigabytes of memory. 

\subsection{\acrshort{birnn}-only end-to-end model Experiments}
For the \acrshort{birnn}-only end-to-end experiments, GPU configuration experiments were carried out on varying-size subsets of the common voice corpus.   The various GPU configurations along with the training times are shown in Table \ref{tab_c6_01_training}.

\begin{table}
  \caption{\acrshort{birnn}-only Experiments}
  \label{tab_c6_01_training}
\begin{tabular}{lccc}
\toprule
Experiment & Hours of speech & Total training time & Training status\\
\midrule
1. 2xGPU 10GB RAM & 1 & 7 days & Complete\\
2. 2xGPU 10GB RAM & 10 & 40 days & Not complete\\
3. 5xGPU 15GB RAM & 2 & 2 days & Complete\\
4. 5xGPU 15GB RAM & 40 & 40 days & Not complete\\
5. 1xCPU 16GB RAM & 20 & 20 days & Not complete\\
6. 1xGPU 2GB RAM & 20 & 20 days & Not complete\\
\bottomrule
\end{tabular}
\end{table}

It can be seen in Table \ref{tab_c6_01_training}, only two experiments had reached completion.    The others had to be stopped for exceeding reasonable training times of 20 and 40 days. Out of the four experiments that did not complete, all the GPU-based experiments had trained for up to 20 epochs and quantitative metrics were taken for these experiments.  Table \ref{tab_c6_02_training} shows the details for the \acrfull{wer} accuracy metrics for a total of four experiments. The number of hours of speech, corpus type and total number of epochs are also shown. Accuracy curves are shown in figure \ref{fig_6_3_wer}.

\begin{table}
  \caption{\acrshort{birnn}-only Experiments Summary}
  \label{tab_c6_02_training}
\begin{tabular}{lccccc}
\toprule
Experiment & Hours & Corpus & epochs & Metric & Score\\& of speech\\
\midrule
1. 2xGPU 10GB RAM & 1 & CV LVCSR & 40 & WER(\%) & 100+\\
2. 2xGPU 10GB RAM & 10 & CV LVCSR & 25 & WER(\%) & 100+\\
3. 5xGPU 15GB RAM & 2 & CV LVCSR & 40 & WER(\%) & 100\\
4. 5xGPU 15GB RAM & 40 &  CV LVCSR & 40 & WER(\%) & 87\\
\bottomrule
\end{tabular}
\end{table}


\begin{figure}
\centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=14cm]{thesis/images/brnn_only.png}\\
  \caption{\acrshort{birnn}-only Experiments Error curve, where w\<x\<y\<z are taken arbitrarily across the total
number of epochs} \label{fig_6_3_wer}
\end{figure}


\subsection{\acrshort{birnn} with Attention Transducer Experiments}
The \acrfull{espnet} \citep{watanabe2018espnet} provides building blocks for \acrshort{blstm} transducer with attention mechanism  described in Section \ref{sec_7_5_blstm_t}.  Two experiments involving a much smaller audio corpus guaranteed to converge quickly at training and a larger Italian language speech corpus \citep{foxvorge2019} used for these experiments.  The AN4 (alphanumeric) corpus by Carnegie Mellon University \citep{acero1990acoustical}, is a small vocabulary speech corpus having only 948 training utterances and 140 test utterances.

The speech corpora utterances are 16-bit linearly sampled at 16kHz, each recording made with near-field microphone quality.  The compressed tar file comes with a variety of audio formats including raw wav format, the NIST sphere format and those already encoded as Mel cepstral coefficients.

The end-to-end architecture at the core of ESPNet is the CTC-Transformer+Attention Transducer model.  Together these two architectures achieve joint multi-objective speech training and decoding.  The CTC-Transformer model is based on a \acrshort{blstm} and is similar to what is obtainable in the DeepSpeech model.  There are up to 11 variants of Attention networks implemented in \acrshort{espnet}, however, the results of the \acrshort{espnet} experiment performed was determined from the model described in \cite{chorowski2015attention}.  Moreover, the multi-objective training was performed with equal weights on both the CTC-transformer and the Attention-Transducer.  

\begin{table}
  \caption{\acrshort{birnn} with attention and transducer Experiments}
  \label{tab_c6_03_training}
\begin{tabular}{lcccc}
\toprule
Experiment & Hours & Training  & Epochs & Training \\
& of speech & time & & status \\
\midrule
1. 1xGPU 4GB (log mel.) & 1 & 15 minutes & 20 & Complete\\
2. 1xCPU 16GB (scatter feat) & 1 & 3 days & 100 & Complete\\
3. 1xGPU 4GB (log mel) & ~10 & 11 hours & 200 & Complete\\
4. 1xGPU 4GB (scatter feat) & ~10 & 38 hours & 200 & Complete\\
\bottomrule
\end{tabular}
\end{table}
Experiments were carried out using ESPNet default parameters which included those for character based-Recurrent Neural Network language model RNN-LM, multi-channel feature input and multi-objective learning using both CTC-Transformer and Attention-Transducer networks.

With this minimal default setting, the test set had a final recognition score of 9.5\% character error rate (CER).  The next Chapter discusses how the baseline can be scaled and remodelled for integrating scattering features.

\begin{table}
  \caption{\acrshort{birnn} with attention and transducer Experiments Summary}
  \label{tab_c6_04_training}
\begin{tabular}{lcccc}
\toprule
Experiment & Hours & Corpus & Metric & Score\\& of speech\\
\midrule
1. 1xGPU 4GB (log mel) & 1 & AN4 SVCSR & WER(\%) & 12.9\\
2. 1xGPU 4GB (scatter feat.) & 1 & AN4 SVCSR & WER(\%) & 26.8 \\
3. 1xGPU 4GB (scatter feat) & ~10 & Voxforge-italian (LVSCR) & WER(\%) & 76.7 \\
4. 1xGPU 4GB (log mel) & ~10 &  Voxforge-italian (LVSCR) & WER(\%) & 72.4\\
\bottomrule
\end{tabular}
\end{table}

\section{Model Results Interpretation}
Interpretation of Model Results
Experiments carried out to train the end-to-end \acrshort{asr} were performed on the following system configurations
\begin{enumerate}
    \item GPU (GTX1050) with 2GB RAM
    \item GPU (GTX1050) with 4GB RAM
    \item GPU (GTX1060) with 6GB RAM
    \item GPU (GTX1070) with 3GB RAM
    \item CPU with 16GB RAM
\end{enumerate}

\subsection{Bi-RNN-only experiment discussion}
Configurations with CPU were used as control experiments to compare the GPU efficiency with the CPU being compensated with more memory.  The higher memory allowed the CPU configurations to remain accessible during training unlike the GPU systems that used up most of the system resources bringing the computer system close to a grinding halt and making the GPU systems difficult to access while training was in progress.  In as much as a number of the experiments done exceeded 10 days, our goal was not to exceed more than 10 days training for speech models.  What the GPU lacked in memory resources was compensated for in computational speed gained due to their capacity for parallel.  By changing the batch size, memory resource requirement and computational parallelism was simultaneously managed for all experiments.  Therefore for CPU training computational speedup was attempted by increasing the batch size and for GPU training batch sizes were reduced so as not to quickly deplete the small memories.  For \acrshort{birnn}-only experiments, regardless of the batchsize allocations, only 2GPU configurations  (1 and 3 in Table \ref{tab_c6_01_training}) completed training for the given amounts of epochs and training data.

Table \ref{tab_c6_02_training} shows four GPU-only configurations.  These GPU training configuration experiments had completed at least 20 epochs.  Training metrics for these configurations are plotted in Figure \ref{fig_6_3_wer}.   A reduction in training loss is observed once the data was increased to two hours of training.  This gives an indication of the model learning on the amount of data given.  Even though the speech models were trained on English language only.  We can simulate low resource settings in the English language by limiting the amount of data available during training.  Moreover, word error rates (WER) only showed improvement on the 40 hours data set.  This also indicates that a threshold of about 40 hours is required for the model to begin to converge for a Large Scale Vocabulary Continuous Speech Recognition (LSVCSR) system

The results showed that the training of the model was moving towards a very slow convergence as indicated by the slow decrements in training loss.  Initial experiments were performed on single GPU Units.  Batch size settings for these experiments were very small to fit into the limited RAM sizes on the GPUs.  At a later stage, multi-GPU units were utilised as a strategy to speed up training by increasing the batch sizes to run on the combined memory.  This however did not result in the anticipated speed up. It is suspected that this outcome may have been as a result of latency copying model parameters between GPU units and CPU multiple times during training.

\subsection{Bi-RNN with Transducer and attention mechanism experiment discussion}
Results from Bi-RNN with Transducer and attention experiments had shown greater promise in terms of completion of training within the time constraints set. Results shown in table \ref{tab_c6_03_training} show that both CPU and GPU training completed training for less than 20 hours of training and total number of epochs.

We used a \acrfull{svcsr} corpus of English language and a \acrfull{lvcsr} of Italian and achieved a decent score of 26.8\% for the \acrshort{svcsr} corpus and a high error score of 76.7\% for the \acrshort{lvcsr} corpus.  The high error score for the \ref{lvcsr} from the observed results was attributed to the fact that the amount of data given ~10 hours is not sufficient for meaningful convergence.  This is also evidenced by the baseline result having a similar high error rate of 72.4\%.  A similar effect was also observed in the Bi-RNN-only experiments such that after training of 40 hours of data for 40 epochs the error was at 87\%.

From the training curves (Figures \ref{fig_6_2_loss} and \ref{fig_6_3_wer} however, we can see that at 40 epochs the Bi-RNN with transducer and attention mechanism experiments had a faster rate of convergence and this led to experiments being completed within the time limits set.

\section{Chapter Summary}

In this chapter the details of the combination of an end-to-end deep bi-RNN architecture and deep scattering features were elaborated on.  The architecture described follows a five-layer structure consisting of a feed-forward neural network in the first three layers and the last two consisting of recurrent structures flowing in two different directions.  A 163-dimension 1st-order feature vector of deep-scattering encoding derived from a sampled raw audio file is fed into this network.

A second set of experiments comprising a similar architecture containing a \acrshort{bilstm} this time with encoder and decoder architectures and a transducer is also developed and tested.

The result showed the second set of experiments having the transducer architecture with attention mechanism are able to train faster but out of all the results, although some models came close to their respective baselines none actually performed better than the baseline models.  In Chapter \ref{ch8_future} we address ways that the results may be improved.

\stopblue

\begin{figure}
\centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=14cm]{thesis/images/scatter_res.PNG}\\
  \caption{Training Loss, where $w<x<y<z$ are taken arbitrarily across the
total number of epochs} \label{fig_6_2_loss}
\end{figure}

