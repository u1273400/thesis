
\textcolor{blue}{
The details of the language models developed for the Wakirike language is discussed in this chapter.  The language models developed draw  upon the premise that the grammar of a language is expressed in the character sequence pattern ultimately rendered in word sequences.  The two models developed in this chapter follow RNN implementations discussed in chapter \ref{ch3RNN}. }

\section{Data Preparation}
\startblue
A published version of the Wakirike New Testament Bible was obtained and used as the data source for RNN training of the language model.  There was no readily available soft or on-line copy of the Wakirike new testament bible. As such, the Wakirike New Testament Bible text corpus text was entered into the ASR system from the physical copy using a text editor to form a text corpus.  The complete corpus had a word count of 668,522 words and a character count of 6,539,176 characters. Following the k-fold cross validation process, \citep{geron2019hands}, the data set was then divided into 11 parts. Two parts dedicated for testing and validation and the remaining nine parts were used for training. As the validation set is not seen during training it can be used to keep track of how well the training is going and that it is not over-fitting the data by simply memorising it.

Preprocessing of the text corpus involved selecting a set of characters as the input feature set and removing all other characters not found in the input feature set.  The Unicode representations of the character set consisted of letters and punctuation marks.  These are one-hot encoded and batched for sequential input.  Neural network parameters which are not automatically determined through back propagation are called hyper-parameters.  These are usually experimentally determined and manually set while configuring the network.  A hyper-parameter for the language model RNN is the input sequence length.  For the language model built a 30 characters-long sequence length is chosen.  This  length is an average phrase sequence.  In these phrases, long-term character dependencies of words can be captured. At the same time, keeping the sequence length at this value, and not longer, will pose less of a burden on the computer system resources during parameter computations.  

Another hyper-parameter for training used was the batch size.  The batch size parameter determined how many 30-character sequences will be trained in parallel in order to speed up the training process.  Increasing the batch size also meant an increase in the size of the matrix multiplications being performed and therefore, the computing power system resource being demanded by the language model.  By experimenting with various batch sizes it was determined a batch size of 200 was suitable for training the language model with respect to the other training parameters.

\section{GRU RNN Architecture}
The modified LSTM RNN known as the Gated Recurrent Unit (GRU) discussed in Chapter \ref{ch3RNN} is employed for the neural network model built in this Chapter.  In order to optimise network performance while conserving computation resources, GRUs have been shown to give similar performance to regular LSTMs however, with a lighter system resource footprint \citep{cho2014learning}. 

The architecture of the GRU RNN used to train the Wakirike text corpus had an internal network size of 512 nodes for each layer and was 3 layers deep. In a study by \citep{goodfellow2013multi}, it was shown that increasing the number of nodes in a neural network will lead to over-fitting, however, simultaneously increasing the network depth mitigates this effect.  In other words, in order to expand the degrees of freedom of a neural network and at the same time constrain the network to generalise well on unseen data, it is necessary to increase the number of neurons in both length and depth.  Experiments carried out in this chapter follow this recommendation. Initial experiments had an internal node size of 128 and a single layer deep.  While this showed promise of converging, the error rate was still high, therefore the network was expanded to the final model above.   Externally, the network model is further  sequenced 30 times, representing the input sequence length hyper-parameter and the number of recurrent connections where each connection represents a sequenced time step. 

Another hyper-parameter sensitive to network size is the learning rate.  The learning rate is selected in such a manner that an increase in the network size makes the learning rate more prone to overshooting.  Therefore, increased degrees of freedom in a neural network will require the learning rate to be made smaller so that it does not overshoot the network saturation point.  Small learning rates of between 0.001 and 0.005 were used. Furthermore, the language model neural network was designed to overcome over-fitting using the dropout (Srivastava et al., 2014) method which has been shown to be effective for regularising deep neural networks.  The hyper-parameter for dropout was kept at 20\% such that only 80\% of neural network activations are propagated from one layer to the next, whereas the remaining 20\% were randomly zeroed out.  Intuitively, dropout works by forcing the remaining active neurons to infer what is missing in the activations that have been dropped and ultimately leads to better generalisations as activations are based on inference than on memory.

\stopblue


\section{Output Language Generation}
The neural network was trained for 10 epochs and achieved a prediction accuracy of 85\% on held-out data.  With this GRU character-based language model, it is possible to seed this network with an input character and select from the top-N candidates thus causing the Neural network to generate its own sentences.  In this scenario, the network is said to perform language generation by immanently constructing its own sentences.  The generated language output from the GRU language model was found to be intelligible and a reflection of the overall context of the training data.

A clever use of this new corpus generated by the GRU language model of this work was to determine a word-based perplexity metric for the GRU neural language model. In this work, the word-based perplexity metric was determined from the output language generated by first estimating the word based perplexity on the training data.  The same perplexity calculation was then used on the generated neural language model corpus. The corpus size of the neural language model was made to be equivalent to that of the training data, that is containing 6,539,176 characters.  The perplexity calculation was based on a modified Kneser-Key 5-gram model with smoothing \citep{Heafield-estimate}.  The results discussed below showed that the GRU-based RNN model generated a superior model compared to the n-gram model that better matched the training data.

The evaluation of the GRU language model of the Wakirike language was performed using a perplexity measurement metric. The Perplexity metric applies the language model to a test data-set and measures how probable the test data-set is. Perplexity is a relative measure given by the formula:
%
\begin{equation}
PP(W)=P(w_1,w_2\dots w_N)^\frac{1}{N}
\label{ch5_eq1_ppx}
\end{equation}
%
%
\begin{equation}
PP(W)=\sqrt[N]{\prod_{i=1}^N\frac{1}{P(w_i|w_{i-1})}}
\label{ch5_eq2_ppx}
\end{equation}
%
Where $w_1,\dots,w_N$ are the sequence of words. The language model with the lower relative perplexity score is therefore expected to yield better approximation of the data when applied to unseen data generally.

The result of the training of the GRU-Cell Recurrent Neural Network on low-resourced Wakirike Language gave impressive and intelligible results and showed better results when measured with standard n-gram language models. The results showed that it is indeed possible to derive a language model using a GRU-cell RNN on a low resource character sequence corpus for the Wakirike language.

A character based perplexity metric is possible using  the negative log likelihood of the character sequence.
\begin{equation}
    PP(X)=exp\left\{−\frac{\sum_{t=1}^T\log P(x_t|x_{1:t−1})}{T}\right\}
\label{ch5_eq3_ppx}
\end{equation}

However, our base-line language model is a 5-gram word-based language model.  Therefore, comparing a word based model to a character based model requires a conversion step. In this work, the conversion step involved using the GRU language model generated a corpus which was rescored by re-estimating with a 5-gram word-based language model

Table ~\ref{tab:example} shows the Results of the Perplexity model of the LSTM Wakirike Language model and an equivalent 5-gram Language model with interpolation and Keysner smoothing \citep{Heafield-estimate} for various lengths of the held-out data.


\begin{table}
  \caption{Perplexity Calculation results}
  \label{tab:example}
\begin{tabular}{lrr}
\toprule
Language Model & Perplexity  \\
\midrule
Held-out data size (characters) & 998 & 99\\
\midrule
LSTM RNN & 1.6398 & 1.7622\\
5-gram with Keysner Soothing and interpolation & 1.8046 & 1.9461\\
\bottomrule
\end{tabular}
\end{table}
\section{Chapter Summary}
This chapter shows the application of a character-based Gated Recurrent Unit RNN on the low resource language of Wakirike to generate a language model for the Wakirike language.  The data-set and preparation and the details of the network were discussed.  The output of this model was used to hallucinate the Wakirike language which was then scored against word-based perplexity to obtain a metric against the baseline language model.

It can be inferred that the GRU character-model developed has an improved language model and because it is based on a character-model, which is fine-grained when compared to a word model, it is likely to generalise data better when used in practice and is less biased than a word-based model.  This can be observed from the fact that the output corpus produced a larger vocabulary size.
