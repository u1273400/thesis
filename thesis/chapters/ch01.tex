\acrfull{asr} is a subset of Machine Translation that takes a sequence of raw audio information and translates or matches it against the most likely sequence of text as would be interpreted by a human language expert.  In this thesis, Automatic Speech Recognition will also be referred to as 
ASR or speech recognition for short.

It can be argued that while \acrshort{asr} has achieved excellent performance in specific applications, much is left to be desired for general purpose speech recognition \citep{yu2016automatic}. While commercial applications like Google voice search and Apple Siri give evidence that this gap is closing, there still are yet other areas within this research space that speech recognition task is very much an unsolved problem.

It is estimated that there are close to 7,000 human languages in the world \citep{besacier2014automatic} and yet for only a fraction of this number have there been efforts made towards practical ASR systems.  The level of ASR accuracy that has been so far achieved are based on large quantities of speech data and other linguistic resources used to train models for \acrshort{asr}. These models which depend largely on pattern recognition techniques degrade tremendously  when applied to different languages other than the languages that they were trained or designed for  \citep{Rosenberg2017end,besacier2014introduction}. More specifically, the collection of sufficient amounts of linguistic resources required to create accurate models for ASR are particularly laborious and time consuming sometimes extending to decades \citep{goldman2011easyalign,stan2016alisa}.  Research, therefore, geared towards alternative approaches towards developing is ASR systems that are reproducible across languages lacking the resources required to build robust systems is apt.

\section{ASR As a Machine Learning  problem}\label{ASRMLP}
\pagestyle{plain}
\acrlong{asr} can be put into a class of Machine Learning problems described as sequence pattern recognition because an ASR attempts to discriminate a pattern from the sequence of speech utterances. 

One immediate problem realised with this definition leads us to discuss statistical speech models that address how to handle the problem described in the following paragraph.

Speech is a complex phenomena that begins as a cognitive process and ends up as a physical process \citep{becchetti1998}.  The process of automatic speech recognition attempts to reverse engineer steps back from the physical process to the cognitive process giving rise to latent variables or mismatched data or loss of information from interpreting speech information from one physiological layer to the next.

It has been acknowledged in the research community \citep{2015watanabe,deng2013machine}  that work being done in Machine Learning has enhanced the research of automatic speech recognition.   Similarly any progress made in ASR usually constitute contributions to enhancements made in the Machine Learning field.  This also may be attributed to the fact that speech recognition in itself is a sequence pattern recognition problem subclass of machine learning.  Therefore techniques within speech recognition could be applied generally to sequence pattern recognition problems at large.

The two main approaches to Machine Learning problems historically involve two methods rooted in statistical science.  These approaches are generative and discriminative models.  From a computing science perspective, the generative approach is a brute-force approach while the discriminative model uses a rather heuristic approach to Machine Learning. This chapter presents the introductory ideas behind these two approaches and establishes the motivation for the proposed models used in this research for low resource speech recognition, as well as introducing the Wakirike language as the motivating language case study.

\section{Generative-Discriminative Speech Models disambiguation}
\pagestyle{fancy}
In chapter \ref{ch2litrev}, the \acrfull{hmm} is examined as a powerful and major driver behind generative modelling of sequential data like speech.  Generative models are data-sensitive models because they are derived from the data by accumulating as many different features which can be seen and make generalisations based on observed parameters. The discriminative model, on the other hand, has a heuristic approach to form a classification.  Rather than using features of the data directly, the discriminative method attempts to parameterise the data based on initial constraints\citep{lasserre2006principled}. It is therefore concluded that the generative approach uses a bottom-to-top strategy starting with the fundamental structures to determine the overall structure, while the discriminative method uses a top-to-bottom approach starting with the big picture and then drilling down to determine the fundamental structures.

Ultimately, generative models for Machine Learning learning can be interpreted mathematically as a joint distribution that produces the highest likelihood of outputs and inputs based on a predefined decision function.  The outputs for speech recognition being the sequence of words and the inputs for speech being the audio waveform or equivalent speech sequence. More specifically,

\begin{equation}
d_y(\mathbf{x};\lambda)=p(\mathbf{x},y;\lambda)=p(\mathbf{x}|y;\lambda)p(y;\lambda)
\label{eqn1_1}
\end{equation}

where $d_y(\mathbf{x};\lambda)$ is the decision function of $y$ for data labels $\mathbf{x}$.  This joint probability expression given as $p(\mathbf{x}|y;\lambda)$ can also be expressed as the conditional probability product in equation (\ref{eqn1_1}).  In this equation, $\lambda$ predefines the nature of the distribution referred to as model parameters~\citep{deng2013machine}.

Similarly, Machine Learning discriminative models are described mathematically as the conditional probability defined by the generic decision function below:
\begin{equation}
d_y(\mathbf{x};\lambda)=p(y|\mathbf{x};\lambda)
\label{eqn1_2}
\end{equation}

It is clearly seen that the discriminative paradigm follows a much more direct approach to pattern recognition. Although this approach appears cumbersome to model, this research leans towards this direct approach.  However, what the discriminative model gains in discriminative modularity, it loses in the model parameter estimation of ($\lambda$) in equation  (\ref{eqn1_1}) and (\ref{eqn1_2}) \citep{gales2012structured}.  
 As this research investigates, although the generative process is able to generate arbitrary outputs from learned inputs, its major drawback is the direct dependence on the training data from which the model parameters are learned. Specific characteristics of various Machine Learning models are reserved for later chapters, albeit the heuristic nature of the discriminative approach, which means not directly dependent on the training data,  gains over the generative approach as discriminative models are able to better compensate for latent variables.  
 
 In the case of speech signals, the original signal is corrupt and  the intended information message attenuated when the signal undergoes physiologic transformations of the speaking and hearing process and moves from one speech production mechanism mentioned in section \ref{ASRMLP} to the next.  The theme of pattern recognition through arbitrary layers of complexity is reinforced in the notion of deep learning~\cite{deng2014deep} as an attempt to learn patterns from data at multiple levels of abstraction. Thus while shallow Machine Learning models like Hidden Markov Models (HMMs) define latent variables for fixed layers of abstraction, deep Machine Learning models handle hidden/latent information for arbitrary layers of abstraction determined heuristically.  As deep learning mechanisms are typically implemented using \acrlong{dnn}s, this work applies deep \acrlong{rnn}s as an end-to-end discriminative classifier for speech recognition.  This is a so called "end-to-end model" because it adopts the top-to-bottom Machine Learning approaches. Unlike the typical generative classifiers that require sub-word acoustic models, the end-to-end models develop algorithms at higher levels of abstraction as well as the lower levels of abstraction. In the case of the model utilised in this research, the levels of abstraction include sentence/phrase, words and character discrimination. A second advantage of the end-to-end model is that because the traditional generative models require various stages of modeling including an acoustic, language and lexicon, the end-to-end discriminating multiple levels of abstractions simultaneously only requires a single stage process, greatly reducing the quantity of resources required for speech recognition.  From a low resource language perspective this is a desirable behaviour meaning that the model can be learned from an acoustic only source without the need of an acoustic model or a phonetic dictionary.   Thus techniques involving deep learning and end-to-end modelling are proposed and have been found to be self-sufficient \citep{hannun2014deep} with modest results without a language model.  However, applying a language model was observed to serve as a correction factor further improving recognition results \citep{hannun2014deep}. 

\section{Low Resource Languages}
Another challenge observed in complex Machine Learning models for both generative as well as discriminative learning models is the data intensive nature of the work required for robust classification models. \cite{saon2015ibm} recommends around 2000 hours of transcribed speech data for robust speech recognition system. As is covered in the next chapter, for new languages, which are low in training data such as transcribed speech, there are various strategies devised for low resource speech recognition. \cite{besacier2014automatic} outlines various matrices for bench-marking low resource languages.  From the generative speech model interest perspective,  reference is made to languages having less than ideal data in transcribed speech, phonetic dictionary and a text corpus for language modelling.  For end-to-end speech recognition models interests, the data relevant for low resource evaluation is the transcribed speech and a text corpus for language modelling.  It is worth noting that it was observed in \cite{besacier2014automatic} that speaker-base often does not affect a language resource status of a language and was often observed that large speaker bases could in fact lack language/speech recognition resources and that some languages having small speaker bases did in fact have sufficient language/ speech recognition resources.

Speech recognition methods investigated in this work are motivated by the Wakirike language discussed in the next section, which is a low resource language by definition.  Thus this research looked at low research language modelling for the Wakirike language from a corpus of Wakirike text available for analysis.  However, due to the insufficiency of transcribed speech for the Wakirike language, English language was substituted and used as a control variable to study low resource effects of a language when exposed to speech models developed in this work.

\section{The Wakirike Language}
The Wakirike municipality is a fishing community comprising 13 districts in the Niger Delta area of the country of Nigeria in the West African region of the continent of Africa. The first set of migrants to Wakirike settled at the mainland town of Okrika between AD860 and AD1515 at the earliest.  These early settlers had migrated from Central and Western regions of the Niger Delta region of Nigeria.  As the next set of migrants also migrated from a similar region, when the second set of migrants met with the first settlers they exclaimed “we are not different” or “Wakirike” \citep{wakirike}.  

Although the population of the Wakirike community from a 1995 report \citep{ethnologue} is about 248,000, the speaker base is  significantly less than stipulated.  The language is classified as Niger-Congo and Ijoid languages.  The writing orthography is Latin and the language status is 5 (developing) \citep{ethnologue}.  This means that although the language is not yet an endangered language, it still isn't thriving and it is being passed on to the next generation at a limited rate.

The Wakirike language was the focus for this research.  An End-to-end deep neural network language model was built for the Wakirike language based on the availability of the new testament bible printed edition that was available for processing.  The corpus utilized for this thesis work is approximately 9,000 words.

\section{Research aim and objectives}
\startblue
In this work, we develop speech processing models and language models based on deep and recurrent neural network implementations. These models use input features which are of interest to new and low resource languages.  In particular, we develop a language model based on \acrfull{gru} for the Wakirike language and a \acrfull{birnn} speech model for the English and Italian languages.  The aim of this research is therefore to build competitive ASR systems in a resource conservative manner encompassing both system resources as well as training data.

The research objectives were as follows:
\begin{itemize}
    \item Discover fundamental tasks relating to Language learning.  In particular, speech recognition;
    \item Discover building blocks for creating ASR systems generally, and then, pain points for new languages;
    \item Build robust ASR systems using methods that also address resource concerns; and
    \item Build and evaluate resource-friendly, end-to-end ASR systems.
\end{itemize}\stopblue

Within this framework, our focus on language learning tasks was on Automatic Speech recognition while the intention was to achieve the last two objectives through one or more of the following means:
\begin{enumerate}
    \item Reduction of time to train speech models and/or ensure training completes within few hours to few days;
    \item Optimisation of sub-tasks and training architecture within the ASR pipeline;
    \item \textcolor{blue}{Observe and recommend models which perform better or train faster than others;}
    \item Make efficient use of training parallelism;
    \item \textcolor{blue}{Obtain better or close to state-of-the-art performance; and}
    \item Induce model simplicity thereby reducing training and development time without compromising performance.
\end{enumerate}

Furthermore, following the Interspeech 2015 Zero Resource Speech Challenge \citep{versteegh2015zero}, this research also fulfilled the objectives of modelling speech at sub-word, word and syntax level. The Zero Resource Speech Challenge is inspired from infants ability to construct acoustic and language models of speech in an end-to-end manner. At the word and syntax level this research develops a character-based language model that reinforces sub-word, word and syntax level speech model based on Character-Temporal-Classification CTC.

\startblue
\subsection{Research Question}
Considering the recent development of end-to-end systems facilitated by deep-learning and sequence modelling, can we combine the recent pattern recognition strides in deep scattering transform with an end-to-end sequential model that results in a simplified CNN-based but robust speech recognition system which new and low resource languages can leverage?  In addition, can supporting Speech recognition sub-systems be replaced or enhanced or simplified by sequence-oriented end-to-end deep learning models?
\stopblue

\section{Main Contribution to knowledge}
\textcolor{blue}{This work uses a character-based neural language model for the low resourced language of Wakirike.  In addition, this work implements a unique combination of end-to-end deep recurrent neural network models with a robust and state of the art audio signal processing mechanism involving a hierarchical \acrfull{dsn} to engineer high-dimensional features to compete with current acoustic and deep architectures for speech recognition. While the language model had a better perplexity score than the 5-gram baseline, the \acrshort{dsn}-\acrshort{ctc} end-to-end sequence model performed competitively but not better than the baselines with a \acrlong{wer} of 12.9\% and 76.8\%; for \acrshort{svcsr} and \acrshort{lvcsr} tasks respectively.}

\startblue
The main contributions to knowledge of this research include:
\begin{itemize}
    \item Rather than developing separate systems including \acrlong{am}, \acrlong{lm}, phonetic dictionary, aligned text and speaker related data transformation.  Systems developed in this research uses a single end-to-end frame-work which does not require separate sub-system training only input audio and output text sequences for training.
    \item Detailed alignment of text and speech is not required only rough alignment comprising of segmented utterances and equivalent text
    \item Our system is enhanced with a robust character-based \acrlong{rnnlm} which is trained integrally within the super end-to-end model.
    \item Contribution to the Zero Resource challenge \citep{versteegh2015zero} in terms of sub-word modelling of speech features using lightweight \acrfull{dsn} and modelling of syntax level speech with end-to-end speech modelling.  This uses sequential model which simulates speech at the syntax level as opposed to the word-level.
    \item This research also implemented hybrid model subsystems based on alternative sequential models specifically for the wakirike language including
\begin{enumerate}
    \item Wakirike Diacritic text converter from plain Wakirike text 
    \item Phonetic Dictionary for Wakirike language
\end{enumerate}
    \item This research also as a pilot study designed an unsupervised syllable-phone recogniser using auto-correlation and \acrfull{gmm}
\end{itemize}


\section{Scope of the study}
This study acknowledged from the onset that it may not be able to gather the data required to build ASR systems for the Wakirike language as this is an initial problem this work hopes to get a step closer to solving.  As a result, the English and Italian languages were substituted and simulated low resource constraints by reducing the amount of hours of recorded speech data for training or reducing the vocabulary size.  This provides a rough estimate of how much of the target Wakirike language speech data will be required to provide equivalent results based on this multilingual approach.
\stopblue

\section{Thesis outline}
The engineered systems, methods and supporting literature contained in this thesis report follows the following outline and describe the research outputs of an end-to-end speech recogniser and develops the theory based on the building blocks of the research outputs.

Chapter two introduces the speech recognition pipeline and the generative speech model.  Chapter two also outlines the weaknesses in the generative model and describes some of the Machine Learning techniques applied to improve speech recognition performance.  The methods and techniques and description of the various tools and metrics for analysis of the research outputs are described and examined in Chapter three.

Various Low speech recognition methods are reviewed and the relevance of this study is also highlighted.  Chapter four describes \acrfull{rnns}. Starting with \acrfull{mlps}, we go on to specialised recurrent neural networks including \acrfull{lstm} networks and the \acrfull{gru} are detailed. These recurrent neural network units form building blocks of the language model for Wakirike language implemented in this work.

Chapter five explains the wavelet theorem as well as the deep scattering spectrum. The chapter develops the theory from Fourier transform and details the significance of using the scattering transform as a feature selection mechanism for low resource recognition.  

Chapters six and seven give descriptions of the models developed by this thesis and details the experimental setup along with the results obtained. Chapter eight is the conclusion of the work and recommendations for further study. 

\section{Chapter Summary}
Amidst seeming large success of speech-to-text technology referred to as \acrfull{asr}, there are still areas in which ASR technology struggles to perform up to the minimum acceptable level.  Situations such as very noisy environments and far field speech recognition constitute common physical scenarios where \acrshort{asr} performance degrades significantly.  Another non-physical area in which \acrshort{asr} falls short of acceptable performance and chosen as the focus of this research is the area of low-resource speech recognition.  This is the scenario where languages not rich in linguistic resources are unable to use existing resources and algorithms used in languages rich in linguistic and ASR resources, to perform automatic speech recognition. 

As this chapter identifies, the ASR problem is traditionally a \acrlong{ml} problem that models   speech production, transmission and perception and speech models are trained from language-specific data. While these \acrshort{ml} speech models may perform well for the languages the models were trained for, when introduced to a different language, having a different set of learning features, these pre-trained models fall short of expected performances for these new languages. Moreover, if the new languages do not possess a rich set of linguistic features, including resources such as aligned speech and an online text corpus amongst others \citep{besacier2014introduction}, it becomes time-consuming and extremely laborious to develop new \acrshort{asr} models for speech recognition for these so-called \acrshort{asr} ``low-resource'' languages.

This chapter also introduces the Wakirike language as a low resource language and the motivating language for this research.  In addition, the various machine learning architectures used in this research for low resource speech recognition for the Wakirike and for English language are reviewed. In particular, \acrfull{dnns} are highlighted as choice algorithms in speech recognition, and then, the Chapter goes on to describe the research contribution and the outline of this thesis.


