\textcolor{blue}{This research has been inspired by the notion of a \acrfull{llc}. One of the artificial intelligence tasks for \acrshort{llc} is \acrfull{asr}.  This research, therefore, explored the current advancements that have been made in the field of \acrshort{asr} and discovered one of the major challenges in field of \acrshort{asr} is that of new and low resourced languages.  In particular, considering \acrshort{asr} as a machine learning pipeline, this research favours a discriminative approach to \acrshort{asr} over generative approaches.  Although hybrid \acrshort{hmm}-\acrshort{dnn}, which is considered a generative model, does deliver state-of-the art results, the \acrshort{asr} pipeline involves training of different aspects of the \acrshort{asr} pipeline in separate processes.  The end-to-end approach, however, attempts to simplify this by offering a solution that involves training of a single discriminative "end-to-end" model. }

\startblue
As \acrshort{asr} is a machine learning pipeline,  the advancement of Machine Learning has had a direct impact on the development of more efficient speech recognition algorithms and at the same time the advancement of speech recognition helps with the improvement of Machine Learning algorithms, as in general, the methods used in Machine Learning usually are directly transferable to speech processing and vice-versa. This mutual relationship implies that speech recognition is a blossoming research field because there is a tremendous amount of work being done in the Machine Learning community. Particularly in the area of deep learning and neural networks, there is quite a vast array of neural network solutions that have been applied or are yet to be applied to speech recognition. This research examined the effect of using a different set of input features from the state-of-the-art log-mel features. Feeding these scatter transform features into end-to-end ASR DNN with the objective of a comparable fast and efficient speech recognition for new and low resource languages. The major advantage of our system is that in addition to robust end-to-end models, the end-to-end system is trained with unique front-end scatter transform features and the intrinsic integration of a character-based language model help to train low-resource languages for ASR in a resource efficient manner. 

This Chapter summarises the end-to-end and deep sequential models engineered towards low-resource speech recognition developed by this research. In addition, additional models being developed in the research community which are closely related models are mentioned as areas of further research interest.  These include Generative Adversarial Networks (GANs) and Attention-based models. 

\section{Discussion of Output Research models}

Two well established key aspects of \acrshort{asr} from the \acrshort{asr} formulation in Equation \ref{eqn_2_3_bayes_sr} include an \acrfull{am} and a \acrfull{lm}. The objective of this research was to understand these key aspects \acrshort{asr} systems and develop \acrshort{asr} systems that can be accessible by new and or low resource languages.

The research objectives were met by developing speech models based on a neural network end-to-end approach. End-to-end discriminative neural network speech models have now become a well established method in Automatic Speech Recognition.   The Bi-directional Recurrent neural network (\acrshort{birnn}) end-to-end system, developed in this work, in addition, is augmented by features derived from a deep scattering network as opposed to the standard Mel Frequency Cepstral Coefficients(MFCC) features used in state of the art acoustic models.  These specialised deep scattering features, consumed by the \acrshort{birnn} model, represent a light-weight convolution model. 

On their own the end-to-end models do achieve decent results. The results however,  can be improved by the inclusion of a language model.  This work implemented a 5-gram n-gram language model and a character-based recurrent neural network language model (RNNLM) for the Wakirike and English languages and the latter was incorporated into some of speech models developed in this research.

\subsection{Main contribution to knowledge}
This work shows that it is possible to build a speech model from a combination of deep scattering features and a Bi-RNN. There has been no record of deep scattering features being used in end-to-end bi-RNN speech models as far as we are aware.  This thesis therefore demonstrates Deep Scattering features derived from wavelet filter operations on audio data produce viable candidates for end-to-end training of Automatic speech recognition models.  Preliminary experiments showed word error rates were not too far from the baselines having achieved 26.8\% \acrshort{wer} (14\% points from the baseline) for \acrshort{svcsr} and 76.7\% for \acrshort{lvcsr} which was 4\% points from the baseline.

\subsection{Summary of goals Achieved in this work}
The Zero-resource speech recognition challenge \citep{versteegh2015zero}, that mirrors the \acrlong{llc} motivation for this work, seeks \acrshort{asr} systems that model sub-word language and word/super-word language constructs.  This research developed models that met these goals.

Initial pilot studies for this work involved feature engineering of speech signal using convolution (Section \ref{sec_c3_corr}) as a precursor to the scattering transform.  Also, at the pilot stage, various sequence-to-sequence models were developed for sub-word \acrshort{asr} system modeling.  A grapheme-to-phoneme model was trained in Section \ref{sec_c3_g2p}, from which a phonetic dictionary for the Wakirike language was developed. An \acrshort{asr} system post-processor was also developed based on a sub-word sequence-to-sequence \acrshort{rnn} model (Section \ref{sec_c2d}).  This post-processor was to insert diacritic, tonal symbols to \acrshort{asr} output text.  Although the sub-word systems built in this work brought insight to sequence modelling using \acrfull{rnn}s, in practice only the diacritic post-processing tool had a direct application to the final research output.  The \acrfull{g2p} tool   not have a direct application to the research output which eliminates the need of using \acrshort{g2p} mechanisms.  The tool would however be valuable for hybrid \acrshort{gmm}-\acrshort{hmm}/\acrshort{dnn} \acrshort{asr} systems.

Some aspects of \acrshort{asr} system development covered in this work not directly related to Zero-speech challenge included that of speech segmentation and speech-to-text alignment (Section \ref{sec_digitspeech}).  These are pre-input-feature processing tasks which occurs at the data preparation stage.  Once again, although speech segmentation is not directly integrated into our end-to-end system, one of the huge benefits to end-to-end CTC-based systems implemented in our work for new languages is the time saved from the need for accurate aligned speech during training.  CTC-based end-to-end systems is able to achieve alignment based on roughly aligned inputs.  Roughly aligned here means chunks of segmented raw speech audio and their text equivalent.  Hence, alignment is only at the segmentation or utterance stage and no need to align at the word or sub-word level.

Finally, in addition to the speech model built using \acrshort{dsn} features, in Chapter \ref{ch6_wlm}, a word-level language model was developed using a character-based, \acrshort{gru}-\acrshort{rnn} language model.  This model had an improved perplexity to the baseline language model which was based on a statistical 5-gram language model.

Unlike some of the sub-word systems which did not have direct impact on the final research outcome, the \acrshort{gru}-\acrshort{rnn} language character based model developed by this research was integrated into some of the final speech models built in this work.

\section{Limitations of the Study}
This research was able to develop ground-breaking end-to-end speech models for speech recognition. However, only preliminary results have been obtained for deep scattering features.  While a handful of \acrshort{asr} sequence models were developed for the Wakirike language, the Wakirike language, which motivated this research, did not have any end-to-end speech model built for it.  It is the goal of this research, therefore,  in a future study, to develop a speech corpora for the Wakirike language.

A more immediate limitation, was the high system capacity requirements for training \acrlong{lvcsr}.  The training of \acrshort{lvcsr} such as the full common voice data set requires high-end capacity systems and GPU configurations that were not accessible for training.  Notwithstanding, considering the current configurations that are available, a number of improvements to the present models are possible.    One includes further investigation of first and second order scatter network features. While first order features have been recommended, experiments used in this research utilised 2nd-order features.

\stopblue

\section{Directions for future study}
Improvements in this work can be divided into immediate improvements and the longer term extensions for this study.  Immediate improvements for the \acrshort{dsn} speech features include the addition of \acrfull{vtln} and noise-robust speech enhancement.  These feature engineering could immediately improve robustness of the \acrshort{asr} systems developed in this work. Other longer term extensions of this work include data augmentation and model optimisation methods. The following sections mention one feature enhancement technique and two model optimisation techniques that are possible extensions for this work. 

\subsection{Generative adversarial networks (GAN)}

GANs consists of two Networks working as adversaries to one another.  The first, being a generative network, generates content.  The second network is a discriminative network intended to determine the accuracy of the first generative network.  Hence the generative network is generating output less distinguishable for the discriminator while the discriminator uses output from the generator to improve the ability to discriminate output from the generator with the original source data.

GAN networks can have applications where the generative network consists of a speech synthesis network and the discriminating network is  a speech recogniser.  However successive training of these two networks from a data-resource perspective would require an immense amount of data resources for expected performances. 

\subsection{Attention-based Models}

The objective of attention-based networks highlighted by  \cite{vaswani2017attention} is to reduce sequential computation while attaining hidden representation across arbitrary lengths of sequential input. Mechanisms which have been deployed to achieve this includes a combination of convolutional and recurrent schemes \citep{kaiser2016can,kalchbrenner2016neural, gehring2017convolutional}. \cite{vaswani2017attention} introduces a transduction model known as a Transformer based on self attention network with the ability to compute long term dependencies while eliminating sequence aligned RNN and convolutional architectures.

Self attention is a network that intrinsically reduces the need for intensive resource training.  \cite{vaswani2017attention} reports that state of the art BLEU score of 41.0 having used a small fraction of training resources.  While GANs might not be attractive for low resource speech recognition, they still remain an important tool for verification of the output of other networks.  At the same time self attention networks can help to reduce the resource requirements of GANs when used within the context of a GAN.

As a study to further this thesis, these networks are likely candidates for network training using scatter features as input discriminatory functions.  Attention based networks as a means reduce training resources required, while GANs can be used as a means to generate training data.

While we have used attention-based encoder-decoder networks for training of \acrshort{dss} features, considerable resource-saving may be gained by employing self-attention-based networks.  This has been explored by  \cite{salazar2019self} with satisfactory results.

\subsection{Model Pre-training}
The major setback this work suffered was from a reasonable time for training.  This work therefore recommends that speech models or indeed artificial intelligence models should be trainable within a maximum of 20 days and should not generally exceed 10 days training.

An area of neural network training optimisation not developed in this report is that of layer-wise greedy pre-training.  In this process, rather than train the deep neural network structure all in one stage, the network layers are successively added and trained layer by layer, one layer at a time \citep{Goodfellow-et-al-2016}.  The intuition behind this is that this makes the layers saturate much faster as the previous layer has already been saturated before the new layer is being added.

This layer-wise pretraining procedure is thought to speed up training, than training when done with the fully connected network and there have been a few different approaches to pretraining in for deep neural network architectures for speech recognition. 

\cite{hendrycks2019using} introduces an advanced pretraining method in which existing models are retrained in a Generative Adversarial Network (GAN) fashion in order to optimise performance and model robustness.  This is an instance where GANs are being deployed in speech recognition.  This method however is not likely going to help improve model training convergence time.

Another method described in \citep{ramachandran2016unsupervised,} uses a knowledge transfer mechanism where hidden layers in an already existing related network is re-trained with new extended layers to complete training in the new domain.  The effectiveness of such a transfer method will be measured of the how the two domains correlate with one another. It therefore, would be logical to conclude that the more the domains correlate the faster the pretraining model is likely to converge faster.

Finally, \cite{wang2019bridging} proposes a Tandem Connectionist Encoding Network (TCEN) for bridging the gap for fine tuning CTC-Transformer networks along with pretraining of Attention-transducers.

As a further study, amongst other techniques, the most viable method for this study is to investigate the knowledge transfer mechanism by approaching the feature engineering problem as a latent space analysis problem.  Given that during the process of mapping acoustic speech sequences to the MFCC reduced the latent space from a high dimension to a low dimension.  It is reasonable therefore to hypothesize that training from hidden layers of an MFCC deep RNN would converge faster than weights initialised through generic means.

\startblue
\section{Conclusion}

The outcome of this research is an \acrshort{asr} system which facilitates fast and efficient speech recognition using the end-to-end speech recognition and deep scattering speech features.  Another advantage of our \acrshort{asr} system was the intrinsic integration of a character-based language model enabling it to satisfy both low-resource challenge criteria of top level word and sentence modelling through the character-based language model and the sub-word and acoustic modelling of input features.

The word error rates obtained by our model with different data sizes and speech corpora was not as good as the features using log-mel features. However, the results were competitive.

This research demonstrates sequence-to-sequence and end-to-end models as flexible and still relatively untapped effective tools in many aspects of \acrshort{asr}.  By the development of sequence models at the phonetic and syntactic levels of comprehension and by the development of an end-to-end sequence model speech system, we provide examples for low-resource Wakirike and rich-resource English language.
 
\stopblue



