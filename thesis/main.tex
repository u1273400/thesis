\documentclass[12pt,twoside]{report}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage[a4paper,width=150mm,top=25mm,bottom=25mm,bindingoffset=6mm]{geometry}
\usepackage{wrapfig}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{epstopdf}
\usepackage{listings}

% package used by \citep and \citet
\usepackage[sort&compress,comma,authoryear]{natbib}
% \usepackage[options ]{algorithm2e}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{url}
\def\UrlBreaks{\do\/\do-}
\usepackage{breakurl}
\usepackage[breaklinks]{hyperref}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage[acronym, toc]{glossaries} 

\pagestyle{fancy}
\fancyhead{}
\fancyhead[RO,LE]{Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition}
\fancyfoot{}
\fancyfoot[LE,RO]{\thepage}
\fancyfoot[LO,CE]{Chapter \thechapter}
\fancyfoot[CO,RE]{I. J. Alamina}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\title{
    {Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition}\\
    {\includegraphics{university.png}\\
A thesis submitted to the University of Huddersfield in partial fulfilment of the requirements for the degree of Doctor of Philosophy}
}


\author{Iyalla John Alamina}
\date{January 8, 2020}

\begin{document}
\makeglossaries
\input{chapters/glossary.tex}

\maketitle
\spacing{1.5}

\addcontentsline{toc}{section}{Abstract}
\chapter*{Abstract}
This \href{https://docs.google.com/document/d/1jBp2bZf_-yWbqpSeeyGhU8uZ94o-oiHh6g_cRtjaWFQ/edit#heading=h.34qugrunyk2x}{thesis} investigates and acknowledges the various limitations of \acrfull{dnn} techniques when applied to low resource speech recognition.   Various aspects of developing corpora for speech recognition systems are explored.  In particular, various \acrfull{rnn} techniques were explored to implement end-to-end speech and \acrfull{lm}. \acrfull{gru} RNNs were used employed for the language model for a low resourced Wakirike language while bidirectional recurrent neural networks (bi-RNNs) were used to create end-to-end speech recognition model for English language.

Previous systems employed for low resource speech recognition involving deep networks included various knowledge transfer mechanisms including hybrid hidden markov models (HMM) to deep neural networks (HMM-DNN) models and those that are HMM alone-based include subspace Gaussian Mixture Models (GMMs).   These models are based on the HMM generative model and N-gram language models.  However, the model developed in this thesis makes use of an end-to-end discriminative model using the \acrshort{birnn} acoustic/speech model augmented using speech features from a specialised light weight convolution network-the \acrfull{dsn}.  While the light weight \acrshort{dsn} helped to reduce the training complexity, at the same time by focusing on end-to-end with \acrfull{ctc} decoding, the speech model was compressed into a one step process rather than a three-step process requiring an Acoustic Model (AM), Language Model (LM) and phonetic dictionary. The research therefore shows that it is possible to use this compacting strategy in addition to augmented speech features required for speech pattern recognition by deploying deep scattering network features with  higher dimensional vectors when compared to traditional speech features. 

\addcontentsline{toc}{section}{Dedication}
\chapter*{Dedication}
To the praise and glory of our God and of His Christ.

\addcontentsline{toc}{section}{Acknowledgements}
\chapter*{Acknowledgements}
I thank the members supervisory team including Dr David Wilson and Dr Simon Parkinson for the invaluable guidance and keen interest throughout my research.  

I also acknowledge my parents (Prof. Mrs. Jane Alamina and Dr. P. T. Alamina) for immense support shown.  My wife, children (Topaz and Jade) and family members have also stood by given and given all the encouragement I could ever need.  Thank you.  Finally, to all who have said a prayer and have contributed towards my studies or well being, I am grateful to you all.

\addcontentsline{toc}{section}{Copyright Statement}
\chapter*{Copyright statement}
\renewcommand{\theenumi}{\roman{enumi}}%
\begin{enumerate}
    \item The author of this thesis (including any appendices and/or schedules to this thesis) owns any copyright in it (the “Copyright”) and s/he has given The University of Huddersfield the right to use such copyright for any administrative, promotional, educational and/or teaching purposes.
    \item Copies of this thesis, either in full or in extracts, may be made only in accordance with the regulations of the University Library. Details of these regulations may be obtained from the Librarian. This page must form part of any such copies made.
    \item The ownership of any patents, designs, trademarks and any and all other intellectual property rights except for the Copyright (the “Intellectual Property Rights”) and any reproductions of copyright works, for example graphs and tables (“Reproductions”), which may be described in this thesis, may not be owned by the author and may be owned by third parties. Such Intellectual Property Rights and Reproductions cannot and must not be made available for use without the prior written permission of the owner(s) of the relevant Intellectual Property Rights and/or Reproductions
\end{enumerate}

\tableofcontents

\addcontentsline{toc}{section}{List of Figures}
\listoffigures
 
\addcontentsline{toc}{section}{List of Tables}
\listoftables
\addcontentsline{toc}{section}{List of Algorithms}
\listofalgorithms
\newpage
\addcontentsline{toc}{section}{Acronyms}
\chapter*{Acronyms}
\begin{table}
  \label{tab:acronymns}
\begin{tabular}{ll}
\acrshort{am} & \acrlong{am}  \\
\acrshort{asr} & \acrlong{asr}\\
\acrshort{birnn} & \acrlong{birnn}\\
\acrshort{bleu} & \acrlong{bleu}\\
\acrshort{cfg} & \acrlong{cfg} \\
\acrshort{cmu} & \acrlong{cmu} \\
\acrshort{cmvn} & acrlong{cmvn} \\
\acrshort{cmn} & \acrlong{cmn} \\
\acrshort{cnn} & \acrlong{cnn} \\
\acrshort{ctc} & \acrlong{ctc} \\{ctc}{CTC}{Connectionist Temporal Classification}
\acrshort{dbn} & \acrlong{dbn} \\{dbn}{DBN}{Deep Belief Network}
\acrshort{dct} & \acrlong{dct} \\{dct}{DCT}{Discrete Cosine Transform}
\acrshort{dnn} & \acrlong{dnn} \\{dnn}{DNN}{Deep Neural Network}
\acrshort{dnns} & \acrlong{dnns} \\{dnns}{DNNs}{Deep Neural Networks}
\acrshort{dsn} & \acrlong{dsn} \\{dsn}{DSN}{Deep Scattering Network}
\acrshort{dtw} & \acrlong{dtw} \\{dtw}{DTW}{Dynamic Time Warping}
\acrshort{fsg} & \acrlong{fsg} \\{fsg}{FSG}{Finite-State Grammar}
\acrshort{fsgs} & \acrlong{fsgs} \\{fsgs}{FSGs}{Finite-State Grammars}
\acrshort{fst} & \acrlong{fst} \\{fst}{FST}{Finite-State Transducer}
\acrshort{cnn} & \acrlong{cnn} \\{gmm}{GMM}{Gaussian Mixture Model}
\acrshort{cnn} & \acrlong{cnn} \\{gru}{GRU}{Gated Recurrent Unit}
\acrshort{cnn} & \acrlong{cnn} \\{hlda}{HLDA}{Heteroscedastic Linear Discriminant Analysis}
\acrshort{cnn} & \acrlong{cnn} \\{hmm}{HMM}{Hidden Markov Model}
\acrshort{cnn} & \acrlong{cnn} \\{idft}{IDFT}{Inverse Discrete Cosine Transform}
\acrshort{cnn} & \acrlong{cnn} \\{lda}{LDA}{Linear Discriminant Analysis}
\acrshort{cnn} & \acrlong{cnn} \\{lm}{LM}{Language Model}
\acrshort{cnn} & \acrlong{cnn} \\{lpc}{LPC}{Linear Predictive Coding}
\acrshort{cnn} & \acrlong{cnn} \\{lstm}{LSTM}{Long Short-Term Memory}
\acrshort{cnn} & \acrlong{cnn} \\{mfcc}{MFCC}{Mel Frequency Cepstral Coefficients}
\acrshort{cnn} & \acrlong{cnn} \\{mfsc}{MFSC}{Mel Frequency Spectral Coefficients}
\acrshort{cnn} & \acrlong{cnn} \\{ml}{ML}{Machine Learning}
\acrshort{cnn} & \acrlong{cnn} \\{mllt}{MLLT}{Maximum Likelihood Linear Transformation}
\acrshort{cnn} & \acrlong{cnn} \\{mlp}{MLP}{Multi Layer Perceptron}
\acrshort{cnn} & \acrlong{cnn} \\{mlps}{MLPs}{Multi Layer Perceptrons}
\acrshort{cnn} & \acrlong{cnn} \\{oov}{OOV}{"out Of vocabulary"}
\acrshort{cnn} & \acrlong{cnn} \\{plp}{PLP}{Perceptual Linear Prediction}
\acrshort{cnn} & \acrlong{cnn} \\{rasta}{RASTA}{RelAtive SpecTrAl}
\acrshort{cnn} & \acrlong{cnn} \\{rbm}{RBM}{Restricted Boltzmann Machine}
\acrshort{cnn} & \acrlong{cnn} \\{rnn}{RNN}{Recurrent Neural Network}
\acrshort{cnn} & \acrlong{cnn} \\{rnns}{RNNs}{Recurrent Neural Networks}
\acrshort{cnn} & \acrlong{cnn} \\{sgmm}{SGMM}{Sub-space Gaussian Mixture Model}
\acrshort{cnn} & \acrlong{cnn} \\{sgd}{SGD}{Stochastic Gradient Descent}
\acrshort{cnn} & \acrlong{cnn} \\{stc}{STC}{Semi-Tied Co-variance matrix}
\acrshort{cnn} & \acrlong{cnn} \\{vtln}{VTLN}{Vocal Tract Length Normalisation}
\acrshort{cnn} & \acrlong{cnn} \\{wfst}{WFST}{Weighted  Finite State Transducer}
\end{tabular}
\end{table}

  
\chapter{\href{https://docs.google.com/document/d/1h8ZEcfEUpjJM6wYkgYYH-ryuiBFYVGSQA-Sf1StQtiY/edit#heading=h.i9tlo6ovvcpr}{Introduction}}\label{ch1_intro}
\input{chapters/ch01}

%\chapter{Low Resource Speech Models, End-to-end models and the scattering
\chapter{\href{https://docs.google.com/document/d/1h8ZEcfEUpjJM6wYkgYYH-ryuiBFYVGSQA-Sf1StQtiY/edit#heading=h.i9tlo6ovvcpr}{Literature Review}}\label{c02}\label{ch2litrev}
\input{chapters/ch02}

%\chapter{Speech processing systems method}
\chapter{Methodology}\label{ch3Method}
\input{chapters/ch03}

%\chapter{Recurrent Neural Networks in Speech Recognition}
\chapter{Background 1: Recurrent Neural Networks in Speech Recognition}\label{ch3RNN}
\input{chapters/ch04}

\chapter{Background 2: Deep Scattering network}\label{ch4DSN}
\input{chapters/ch05}

\chapter{Empirical Analysis 1: Wakirike Language Model}\label{ch6_wlm}
\input{chapters/ch06}

\chapter{Empirical Analysis 2: Deep Recurrent Speech Recognition models}\label{ch6_speech}
\input{chapters/ch07}

\chapter{Conclusion and Future Work}\label{ch8_future}
\input{chapters/ch08}
\spacing{1.0}
\addcontentsline{toc}{chapter}{Appendix I - Haar wavelet}
\chapter*{\appendix}
\input{chapters/ch08a}\label{app_haar}
\addcontentsline{toc}{chapter}{Appendix II - Gabor and Morlet wavelet filters}
\chapter*{\appendix}
\input{chapters/ch08aa}
\addcontentsline{toc}{chapter}{Appendix III - Scatter Transform implementation}
\chapter*{\appendix}
\input{chapters/ch08b}
\addcontentsline{toc}{chapter}{Appendix IV - Sample TensorFlow Client code}
\chapter*{\appendix}
\input{chapters/ch08c}\label{app4_tfcode}
\bibliographystyle{plainnat}

\bibliography{bib}

\end{document}
