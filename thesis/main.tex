\documentclass[12pt,twoside]{report}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage[a4paper,width=150mm,top=25mm,bottom=25mm,bindingoffset=6mm]{geometry}

% package used by \citep and \citet
\usepackage[sort&compress,comma,authoryear]{natbib}
\usepackage[options ]{algorithm2e}

\usepackage{setspace}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[RO,LE]{Deep Scattering End-to-End Architectures towards Low Resource Speech Recognition}
\fancyfoot{}
\fancyfoot[LE,RO]{\thepage}
\fancyfoot[LO,CE]{Chapter \thechapter}
\fancyfoot[CO,RE]{I. J. Alamina}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\title{
    {An End-to-End Low Resource Speech Recognition Model}\\
    {\large University of Huddersfield}\\
    {\includegraphics{university.png}}
}
\author{Iyalla John Alamina}
\date{31 December, 2018}

\begin{document}

\maketitle
\spacing{1.5}

\chapter*{Abstract}
This thesis investigates and acknowledges the various limitations of Deep Neural Network (DNN) techniques when applied to low resource speech recognition.   Various aspects of developing corpora for speech recognitions systems are explored.  In particular various recurrent neural network (RNN) techniques were explored to implement end-to-end speech models, language model (LM) and the phonetic dictionary aspects of speech recognition. Gated Recurrent Units (GRU) RNNs were used employed for the language model and the phonetic dictionary for the Wakirike language while bidirectional recurrent neural networks (bi-RNNs) were used to create end-to-end speech recognition model for English language.

Previous systems employed for low resource speech recognition involving deep networks included various knowledge transfer mechanisms including hybrid hidden markov models (HMM) to deep neural networks (HMM-DNN) models and those that are HMM alone-based include subspace Gaussian Mixture Models (GMMs).   These models are based on the HMM generative model and N-gram language models.  However, the model developed in this thesis makes use of an end-to-end discriminative model using the Bi-RNN acoustic/speech model augmented using speech features from a specialised light weight convolution network-the deep scattering network (DSN).  The advantage of using a light weight DSN is to reduce the training time required by  bi-RNNs at the same time focusing on end-to-end speech units as a one step process rather than a three-step process requiring an AM, LM and phonetic dictionary. The research therefore shows it is possible build speech recognition systems with less resources, that is, with only aligned acoustic data.  At the same time the inherent problems of speech recognition, that is, determining the relevant speech features required for accurate speech pattern recognition can be addressed by making use of deep scattering network features as opposed to traditional speech features. 

\chapter*{Dedication}
Glory to God

\chapter*{Declaration}
I declare that..

\chapter*{Acknowledgement}
I want to thank...

\tableofcontents

\chapter{Introduction}
\input{chapters/ch01}

\chapter{Low Resource Speech Models, End-to-end models and the scattering network}\label{c02}
\input{chapters/ch02}

\chapter{Recurrent Neural Networks in Speech Recognition}\label{ch3RNN}
\input{chapters/ch03}

\chapter{Deep Scattering network}
\input{chapters/ch04}

\chapter{Wakirike Language Model}
\input{chapters/ch05}

\chapter{Deep Learning Speech Model}
\input{chapters/ch06}

\spacing{1.0}
\bibliographystyle{plainnat}

\bibliography{bib}

\end{document}
