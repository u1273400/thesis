{\rtf1\ansi\ansicpg1252\cocoartf2509
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Roman;\f1\fnil\fcharset0 AppleSymbols;\f2\ftech\fcharset77 Symbol;
\f3\fnil\fcharset0 LucidaGrande;\f4\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc2\levelnfcn2\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{lower-roman\}.}{\leveltext\leveltemplateid1\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid101\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat3\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid201\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid301\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid302\'01.;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat4\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid401\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid402\'01.;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat3\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid501\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid601\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid701\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid801\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid9}
{\list\listtemplateid10\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid901\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid10}
{\list\listtemplateid11\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid1001\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid11}
{\list\listtemplateid12\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid1101\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid12}
{\list\listtemplateid13\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid13}
{\list\listtemplateid14\listhybrid{\listlevel\levelnfc2\levelnfcn2\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{lower-roman\}.}{\leveltext\leveltemplateid1301\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid14}
{\list\listtemplateid15\listhybrid{\listlevel\levelnfc2\levelnfcn2\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{lower-roman\}.}{\leveltext\leveltemplateid1401\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid15}
{\list\listtemplateid16\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid16}
{\list\listtemplateid17\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid17}
{\list\listtemplateid18\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1701\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid18}
{\list\listtemplateid19\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid1801\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid19}
{\list\listtemplateid20\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat15\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid1901\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid20}
{\list\listtemplateid21\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat18\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid2001\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid21}
{\list\listtemplateid22\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat24\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid2101\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid22}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}{\listoverride\listid10\listoverridecount0\ls10}{\listoverride\listid11\listoverridecount0\ls11}{\listoverride\listid12\listoverridecount0\ls12}{\listoverride\listid13\listoverridecount0\ls13}{\listoverride\listid14\listoverridecount0\ls14}{\listoverride\listid15\listoverridecount0\ls15}{\listoverride\listid16\listoverridecount0\ls16}{\listoverride\listid17\listoverridecount0\ls17}{\listoverride\listid18\listoverridecount0\ls18}{\listoverride\listid19\listoverridecount0\ls19}{\listoverride\listid20\listoverridecount0\ls20}{\listoverride\listid21\listoverridecount0\ls21}{\listoverride\listid22\listoverridecount0\ls22}}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl640\sa240\partightenfactor0

\f0\fs56 \cf2 \expnd0\expndtw0\kerning0
Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs56 A thesis submitted to the University of 
\fs24 \

\fs56 Huddersfield in partial fulfilment of the requirements for the degree of Doctor of Philosophy 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 Iyalla John Alamina January 8, 2020 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page1image6989360.png \width4500 \height2490 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 Abstract 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 This thesis investigates and acknowledges the various limitations of Deep Neural Net- work (DNN) techniques when applied to low resource speech recognition. Various aspects of developing corpora for speech recognition systems are explored. In partic- ular, various Recurrent Neural Network (RNN) techniques were explored to imple- ment end-to-end speech and Language Model (LM). Gated Recurrent Unit (GRU) RNNs were used employed for the language model for a low resourced Wakirike lan- guage while bidirectional recurrent neural networks (bi-RNNs) were used to create end-to-end speech recognition model for English language. 
\fs24 \

\fs32 Previous systems employed for low resource speech recognition involving deep networks included various knowledge transfer mechanisms including hybrid hidden markov models (HMM) to deep neural networks (HMM-DNN) models and those that are HMM alone-based include subspace Gaussian Mixture Models (GMMs). These models are based on the HMM generative model and N-gram language models. How- ever, the model developed in this thesis makes use of an end-to-end discriminative model using the bi-RNN acoustic/speech model augmented using speech features from a specialised light weight convolution network-the Deep Scattering Network (DSN). While the light weight DSN helped to reduce the training complexity, at the same time by focusing on end-to-end with Connectionist Temporal Classification (CTC) decoding, the speech model was compressed into a one step process rather than a three-step process requiring an Acoustic Model (AM), Language Model (LM) and phonetic dictionary. The research therefore shows that it is possible to use this compacting strategy in addition to augmented speech features required for speech pattern recognition by deploying deep scattering network features with higher di- mensional vectors when compared to traditional speech features. 
\fs24 \

\fs32 2 
\fs24 \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 Dedication 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 To the praise and glory of our God and of His Christ. 
\fs24 \

\fs32 3 
\fs24 \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 Acknowledgements 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 I thank the members supervisory team including Dr David Wilson and Dr Simon Parkinson for the invaluable guidance and keen interest throughout my research. 
\fs24 \

\fs32 I also acknowledge my parents (Prof. Mrs. Jane Alamina and Dr. P. T. Alam- ina) for immense support shown. My wife, children (Topaz and Jade) and family members have also stood by given and given all the encouragement I could ever need. Thank you. Finally, to all who have said a prayer and have contributed towards my studies or well being, I am grateful to you all. 
\fs24 \

\fs32 4 
\fs24 \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 Copyright statement 
\fs24 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa320\partightenfactor0
\ls1\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	i.	}\expnd0\expndtw0\kerning0
The author of this thesis (including any appendices and/or schedules to this thesis) owns any copyright in it (the \'93Copyright\'94) and s/he has given The Uni- versity of Huddersfield the right to use such copyright for any administrative, promotional, educational and/or teaching purposes. \uc0\u8232 \
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	ii.	}\expnd0\expndtw0\kerning0
Copies of this thesis, either in full or in extracts, may be made only in accor- dance with the regulations of the University Library. Details of these regula- tions may be obtained from the Librarian. This page must form part of any such copies made. \uc0\u8232 \
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	iii.	}\expnd0\expndtw0\kerning0
The ownership of any patents, designs, trademarks and any and all other in- tellectual property rights except for the Copyright (the \'93Intellectual Property Rights\'94) and any reproductions of copyright works, for example graphs and tables (\'93Reproductions\'94), which may be described in this thesis, may not be owned by the author and may be owned by third parties. Such Intellectual Property Rights and Reproductions cannot and must not be made available for use without the prior written permission of the owner(s) of the relevant Intellectual Property Rights and/or Reproductions \uc0\u8232 \
\pard\pardeftab720\sl360\sa240\partightenfactor0
\cf2 5 
\fs24 \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 Contents 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Abstract..................................... 2 Dedication.................................... 2 Acknowledgements ............................... 3 CopyrightStatement .............................. 4 ListofFigures.................................. 9 ListofTables.................................. 11 ListofAlgorithms ............................... 12 Acronyms.................................... 13 
\fs24 \

\fs32 1 Introduction 14 
\fs24 \

\fs32 1.1 ASRAsaMachineLearningproblem.................. 15 1.2 Generative-Discriminative Speech Models disambiguation . . . . . . . 16 1.3 LowResourceLanguages......................... 18 1.4 TheWakirikeLanguage ......................... 19 1.5 Researchaimandobjectives....................... 19 1.6 MainContributiontoknowledge..................... 21 1.7 Thesisoutline............................... 21 1.8 ChapterSummary ............................ 22 
\fs24 \

\fs32 2 Literature Review 24 
\fs24 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa240\partightenfactor0
\ls2\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
2.1 \'a0SpeechRecognitionOverview ...................... 24 2.1.1 HMM-basedGenerativespeechmodel. . . . . . . . . . . . . . 26 2.1.2 ChallengesofSpeechRecognition ................ 27 2.1.3 Challengesoflowspeechrecognition . . . . . . . . . . . . . . 27 
\fs24 \uc0\u8232 \
\ls2\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
2.2 \'a0LowResourceSpeechRecognition.................... 28 2.2.1 LowResourcelanguagemodelling................ 29 
\fs24 \uc0\u8232 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 6 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 2.2.2 Low Resource Acoustic and speech modelling . . . . . . . . . 32 
\fs24 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa240\partightenfactor0
\ls3\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
2.3 \'a0Groundwork for low resource end-to-end speech modelling . . . . . . 33 2.3.1 Deepspeech............................ 33 2.3.2 SpeechRecognitiononalowbudget . . . . . . . . . . . . . . 34 2.3.3 AddingaScatteringlayer .................... 35 
\fs24 \uc0\u8232 \
\ls3\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
2.4 \'a0ChapterSummary ............................ 36 
\fs24 \uc0\u8232 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 3 Methodology 38 
\fs24 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa240\partightenfactor0
\ls4\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.1 \'a0Assumptions................................ 39 
\fs24 \uc0\u8232 \
\ls4\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.2 \'a0SpeechProcessingsoftwareandtools .................. 39 3.2.1 CMUSphinx............................ 40 3.2.2 Kaldi................................ 45 3.2.3 MozillaDeepSpeech........................ 47 3.2.4 MatlabandScatNettoolbox................... 50 3.2.5 TensorFlow ............................ 57 3.2.6 Choregraphe............................ 62 3.2.7 Alisa................................ 63 
\fs24 \uc0\u8232 \
\ls4\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.3 \'a0InitialExperiments............................ 65 
\fs24 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sl360\sa240\partightenfactor0
\ls4\ilvl1
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.3.1 \'a0Auto-correlationExperiments .................. 65 
\fs24 \uc0\u8232 \
\ls4\ilvl1
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.3.2 \'a0ExperimentswithNaorobot................... 68 
\fs24 \uc0\u8232 \
\ls4\ilvl1
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.3.3 \'a0Digit Speech Recognition and Alignment Experiments . . . . . 68 
\fs24 \uc0\u8232 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa240\partightenfactor0
\ls4\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.4 \'a0End-to-endResearchExperiments.................... 69 
\fs24 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sl360\sa240\partightenfactor0
\ls4\ilvl1
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.4.1 \'a0Tensor flow sequence-to-sequence character-to-diacritically-labelled- charactermodel.......................... 69 
\fs24 \uc0\u8232 \
\ls4\ilvl1
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.4.2 \'a0Sequence-to-sequence Grapheme-to-Phoneme (G2P) model . . 70 
\fs24 \uc0\u8232 \
\ls4\ilvl1
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.4.3 \'a0GRU language model for Wakirike language based on Tensor- Flow................................ 70 
\fs24 \uc0\u8232 \
\ls4\ilvl1
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.4.4 \'a0Bi-Directional LSTM-based end-to-end speech model . . . . . 71 
\fs24 \uc0\u8232 \
\ls4\ilvl1
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.4.5 \'a0ESP-NetExperiments ...................... 71 
\fs24 \uc0\u8232 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa240\partightenfactor0
\ls4\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.5 \'a0Methodofevaluation........................... 72 
\fs24 \uc0\u8232 \
\ls4\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.6 \'a0ChapterSummary ............................ 73 
\fs24 \uc0\u8232 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 0 I. J. Alamina 7 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa240\partightenfactor0
\ls5\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
4 \'a0Background 1: Recurrent Neural Networks in Speech Recognition 74 
\fs24 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\sl360\sa240\partightenfactor0
\ls5\ilvl1
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
4.1 \'a0Neuralnetworkarchitecture ....................... 74 4.1.1 Multi-layerPerceptron(MLP).................. 75 4.1.2 Sigmoid and soft-max Activation Function . . . . . . . . . . . 76 4.1.3 Backpropagationalgorithm(backprop). . . . . . . . . . . . . 77 4.1.4 GradientDescent ......................... 78 
\fs24 \uc0\u8232 \
\ls5\ilvl1
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
4.2 \'a0RNN,LSTMandGRUNetworks .................... 79 4.2.1 DeepNeuralNetworks(DNNs) ................. 79 4.2.2 RecurrentNeuralNetworks ................... 81 4.2.3 Back propagation through time (BPTT) algorithm . . . . . . 82 4.2.4 LSTMsandGRUs ........................ 86 
\fs24 \uc0\u8232 \
\ls5\ilvl1
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
4.3 \'a0Deepspeecharchitecture......................... 88 4.3.1 Connectionist Temporal Classification (CTC) . . . . . . . . . 89 4.3.2 Forward-backwardalgorithm................... 92 4.3.3 CTCLossfunction ........................ 95 
\fs24 \uc0\u8232 \
\ls5\ilvl1
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
4.4 \'a0ChapterSummary ............................ 96 
\fs24 \uc0\u8232 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa240\partightenfactor0
\ls5\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
5 \'a0Background 2: Deep Scattering network 98 
\fs24 \uc0\u8232 
\fs32 5.1 Fouriertransform............................. 99 5.2 Wavelettransform ............................100 5.3 DiscreteandFastwavelettransform...................101 5.4 Melfilterbanks..............................103 5.5 Deepscatteringspectrum ........................106 5.6 ChapterSummary ............................107 
\fs24 \uc0\u8232 \
\ls5\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
6 \'a0Empirical Analysis 1: Wakirike Language Model 109 
\fs24 \uc0\u8232 
\fs32 6.1 DataPreparation .............................110 6.2 GRUTraining...............................110 6.3 OutputLanguageGeneration ......................111 6.4 ChapterSummary ............................112 
\fs24 \uc0\u8232 \
\ls5\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
7 \'a0Empirical Analysis 2: Deep Recurrent Speech Recognition models114 
\fs24 \uc0\u8232 
\fs32 7.1 DeepScatteringFeatures.........................115 7.2 CTC-BiRNNArchitecture ........................115 
\fs24 \uc0\u8232 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 8 Chapter 0 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa240\partightenfactor0
\ls6\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
7.3 \'a0CTCDecoding ..............................117 
\fs24 \uc0\u8232 \
\ls6\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
7.4 \'a0ModelHyperparameters.........................120 
\fs24 \uc0\u8232 \
\ls6\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
7.5 \'a0ModelBaseline ..............................120 
\fs24 \uc0\u8232 \
\ls6\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
7.6 \'a0Results...................................121 
\fs24 \uc0\u8232 \
\ls6\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
7.7 \'a0PreliminaryESPNetExperiment ....................122 
\fs24 \uc0\u8232 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 7.7.1 ESPNet Speech model architecture, parameters and results . . 122 7.8 ChapterSummary ............................123 
\fs24 \

\fs32 8 Conclusion and Future Work 126 
\fs24 \

\fs32 8.1 Generativeadversarialnetworks(GAN). . . . . . . . . . . . . . . . .126 8.2 Attention-basedModels .........................127 8.3 JointTrainingwithESPNet .......................127 8.4 ModelPretraining.............................128 8.5 Conclusion.................................130 
\fs24 \

\fs32 Appendix I - Haar wavelet 130 Appendix II - Gabor and Morlet wavelet filters 131 Appendix III - Scatter Transform implementation 135 Appendix IV - Sample TensorFlow Client code 138 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 0 I. J. Alamina 9 
\fs24 \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 List of Figures 
\fs24 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa240\partightenfactor0
\ls7\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
2.1 \'a0HMMGenerativeModel ......................... 26 
\fs24 \uc0\u8232 \
\ls7\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
2.2 \'a0AutomaticSpeechRecognitionPipeline. . . . . . . . . . . . . . . . . 28 
\fs24 \uc0\u8232 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa240\partightenfactor0
\ls8\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.1 \'a0CMUSphinx4recognisersystem .................... 41 
\fs24 \uc0\u8232 \
\ls8\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.2 \'a0KaldiArchitecture(Poveyetal.,2011b)................. 46 
\fs24 \uc0\u8232 \
\ls8\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.3 \'a0Scattertransformwaveletfilters..................... 53 
\fs24 \uc0\u8232 \
\ls8\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.4 \'a0Unnormalisedscattergram ........................ 55 
\fs24 \uc0\u8232 \
\ls8\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.5 \'a0Lognormalisedscattergram ....................... 55 
\fs24 \uc0\u8232 \
\ls8\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.6 \'a0Sample TensorFlow computation graphs(Goldsborough, 2016) . . . . 58 
\fs24 \uc0\u8232 \
\ls8\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.7 \'a0Tensorflow graph with backprop nodes (Goldsborough, 2016) . . . . . 59 
\fs24 \uc0\u8232 \
\ls8\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.8 \'a0AlisaArchitecture(Stanetal.,2016) .................. 64 
\fs24 \uc0\u8232 \
\ls8\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.9 \'a0Originalwaveforminputforauto-correlation . . . . . . . . . . . . . . 66 
\fs24 \uc0\u8232 \
\ls8\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3.10 \'a0Originalwaveforminputforauto-correlation . . . . . . . . . . . . . . 67 
\fs24 \uc0\u8232 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa240\partightenfactor0
\ls9\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
4.1 \'a0Perceptron................................. 75 
\fs24 \uc0\u8232 \
\ls9\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
4.2 \'a0Neuralnetwork .............................. 75 
\fs24 \uc0\u8232 \
\ls9\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
4.3 \'a0AnLSTMCell(Gravesetal.,2013)................... 87 
\fs24 \uc0\u8232 \
\ls9\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
4.4 \'a0Beam Search Lattice Structure (Graves et al., 2006) . . . . . . . . . . 94 
\fs24 \uc0\u8232 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa240\partightenfactor0
\ls10\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
5.1 \'a0FourierEquation ............................. 99 
\fs24 \uc0\u8232 \
\ls10\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
5.2 \'a0SampleSpectrogram ...........................100 
\fs24 \uc0\u8232 \
\ls10\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
5.3 \'a0Time frequency tiling for (a) Fourier Transform (b) Short-time Fourier 
\fs24 \uc0\u8232 
\fs32 Transform(STFT)(c)Wavelettransform. . . . . . . . . . . . . . . .101 
\fs24 \uc0\u8232 \
\ls10\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
5.4 \'a0Melfilterplot(Lyons,2012) .......................105 
\fs24 \uc0\u8232 \
\ls10\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
5.5 \'a0Scatteringnetwork-2layersdeep....................108 
\fs24 \uc0\u8232 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 7.1 DeepscatteringBi-RNNModel .....................117 10 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa240\partightenfactor0
\ls11\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
7.2 \'a0Prefixbeamsearchalgorithm ......................124 
\fs24 \uc0\u8232 \
\ls11\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
7.3 \'a0Training Loss, where w < x < y < z are taken arbitrarily across the 
\fs24 \uc0\u8232 
\fs32 total number of epochs . . . . . . . . . . . . . . . . . . . . . . . . . . 125 
\fs24 \uc0\u8232 \
\ls11\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
7.4 \'a0WER, where wxyz are taken arbitrarily across the total number of 
\fs24 \uc0\u8232 
\fs32 epochs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 
\fs24 \uc0\u8232 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 1 Haarwavelet ...............................132 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa240\partightenfactor0
\ls12\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
2 \'a0MultiresolutionanalysisofHaarwavelets . . . . . . . . . . . . . . 
\fs24 \uc0\u8232 \
\ls12\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3 \'a0Fourier transform of adjacent scale Gabor wavelet. \uc0\u964  has been set to 
\fs24 \uc0\u8232 
\fs32 0.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
\fs24 \uc0\u8232 \
\ls12\ilvl0
\fs32 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
4 \'a0Fourier transform of adjacent scale Gabor wavelet. \uc0\u964  has been set to 
\fs24 \uc0\u8232 
\fs32 0.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
\fs24 \uc0\u8232 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 . 132 
\fs24 \

\fs32 . 134 
\fs24 \

\fs32 . 136 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 0 I. J. Alamina 
\fs24 \

\fs32 11 
\fs24 \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 List of Tables 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 6.1 PerplexityCalculationresults ......................112 
\fs24 \

\fs32 7.1 GPUExperiments ............................121 7.2 SummaryofGPUExperiments .....................122 
\fs24 \

\fs32 12 
\fs24 \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 List of Algorithms 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 1 DNNtrainingalgorithm.......................... 81 2 RNNtrainingalgorithm.......................... 85 
\fs24 \

\fs32 13 
\fs24 \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 Chapter 1 Introduction 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Automatic Speech Recognition (ASR) is a subset of Machine Translation that takes a sequence of raw audio information and translates or matches it against the most likely sequence of text as would be interpreted by a human language expert. In this thesis, Automatic Speech Recognition will also be referred to as ASR or speech recognition for short. 
\fs24 \

\fs32 It can be argued that while ASR has achieved excellent performance in specific applications, much is left to be desired for general purpose speech recognition (Yu and Deng, 2016). While commercial applications like Google voice search and Apple Siri give evidence that this gap is closing, there still are yet other areas within this research space that speech recognition task is very much an unsolved problem. 
\fs24 \

\fs32 It is estimated that there are close to 7,000 human languages in the world (Be- sacier et al., 2014a) and yet for only a fraction of this number have there been efforts made towards practical ASR systems. The level of ASR accuracy that has been so far achieved are based on large quantities of speech data and other lin- guistic resources used to train models for ASR. These models which depend largely on pattern recognition techniques degrade tremendously when applied to different languages other than the languages that they were trained or designed for (Besacier et al., 2014b, Rosenberg et al., 2017). More specifically, the collection of sufficient amounts of linguistic resources required to create accurate models for ASR are par- ticularly laborious and time consuming sometimes extending to decades (Goldman, 2011, Stan et al., 2016). Research, therefore, geared towards alternative approaches towards developing is ASR systems that are reproducible across languages lacking 
\fs24 \

\fs32 14 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 the resources required to build robust systems is apt. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 1.1 ASR As a Machine Learning problem 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Automatic Speech Recognition can be put into a class of Machine Learning problems described as sequence pattern recognition because an ASR attempts to discriminate a pattern from the sequence of speech utterances. 
\fs24 \

\fs32 One immediate problem realised with this definition leads us to discuss statistical speech models that address how to handle the problem described in the following paragraph. 
\fs24 \

\fs32 Speech is a complex phenomena that begins as a cognitive process and ends up as a physical process (Becchetti and Ricotti, 1998). The process of automatic speech recognition attempts to reverse engineer steps back from the physical process to the cognitive process giving rise to latent variables or mismatched data or loss of information from interpreting speech information from one physiological layer to the next. 
\fs24 \

\fs32 It has been acknowledged in the research community (Deng and Li, 2013, Watan- abe and Chien, 2015) that work being done in Machine Learning has enhanced the research of automatic speech recognition. Similarly any progress made in ASR usually constitute contributions to enhancements made in the Machine Learning field. This also may be attributed to the fact that speech recognition in itself is a sequence pattern recognition problem subclass of machine learning. Therefore techniques within speech recognition could be applied generally to sequence pattern recognition problems at large. 
\fs24 \

\fs32 The two main approaches to Machine Learning problems historically involve two methods rooted in statistical science. These approaches are generative and discrim- inative models. From a computing science perspective, the generative approach is a brute-force approach while the discriminative model uses a rather heuristic ap- proach to Machine Learning. This chapter presents the introductory ideas behind these two approaches and establishes the motivation for the proposed models used in this research for low resource speech recognition, as well as introducing the Wakirike language as the motivating language case study. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 1 I. J. Alamina 15 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 1.2 Generative-Discriminative Speech Models dis- ambiguation 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 In chapter 2, the Hidden Markov Model (HMM) is examined as a powerful and major driver behind generative modelling of sequential data like speech. Generative models are data-sensitive models because they are derived from the data by accu- mulating as many different features which can be seen and make generalisations based on observed parameters. The discriminative model, on the other hand, has a heuristic approach to form a classification. Rather than using features of the data directly, the discriminative method attempts to parameterise the data based on ini- tial constraints(Lasserre et al., 2006). It is therefore concluded that the generative approach uses a bottom-to-top strategy starting with the fundamental structures to determine the overall structure, while the discriminative method uses a top-to- bottom approach starting with the big picture and then drilling down to determine the fundamental structures. 
\fs24 \

\fs32 Ultimately, generative models for Machine Learning learning can be interpreted mathematically as a joint distribution that produces the highest likelihood of out- puts and inputs based on a predefined decision function. The outputs for speech recognition being the sequence of words and the inputs for speech being the audio waveform or equivalent speech sequence. More specifically, 
\fs24 \

\fs32 d
\fs21\fsmilli10667 \dn6 y 
\fs32 \up0 (x; \uc0\u955 ) = p(x, y; \u955 ) = p(x|y; \u955 )p(y; \u955 ) (1.1) 
\fs24 \

\fs32 where d
\fs21\fsmilli10667 \dn6 y
\fs32 \up0 (x;\uc0\u955 ) is the decision function of y for data labels x. This joint probabil- ity expression given as p(x|y; \u955 ) can also be expressed as the conditional probability product in equation (1.1). In this equation, \u955  predefines the nature of the distribu- tion referred to as model parameters (Deng and Li, 2013). 
\fs24 \

\fs32 Similarly, Machine Learning discriminative models are described mathematically as the conditional probability defined by the generic decision function below: 
\fs24 \

\fs32 d
\fs21\fsmilli10667 \dn6 y 
\fs32 \up0 (x; \uc0\u955 ) = p(y|x; \u955 ) (1.2) 
\fs24 \

\fs32 It is clearly seen that the discriminative paradigm follows a much more direct 16 Chapter 1 I. J. Alamina 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 approach to pattern recognition. Although this approach appears cumbersome to model, this research leans towards this direct approach. However, what the discrim- inative model gains in discriminative modularity, it loses in the model parameter estimation of (\uc0\u955 ) in equation (1.1) and (1.2) (Gales et al., 2012). As this research investigates, although the generative process is able to generate arbitrary outputs from learned inputs, its major drawback is the direct dependence on the training data from which the model parameters are learned. Specific characteristics of var- ious Machine Learning models are reserved for later chapters, albeit the heuristic nature of the discriminative approach, which means not directly dependent on the training data, gains over the generative approach as discriminative models are able to better compensate for latent variables. 
\fs24 \

\fs32 In the case of speech signals, the original signal is corrupt and the intended infor- mation message attenuated when the signal undergoes physiologic transformations of the speaking and hearing process and moves from one speech production mecha- nism mentioned in section 1.1 to the next. The theme of pattern recognition through arbitrary layers of complexity is reinforced in the notion of deep learning Deng et al. (2014) as an attempt to learn patterns from data at multiple levels of abstraction. Thus while shallow Machine Learning models like Hidden Markov Models (HMMs) define latent variables for fixed layers of abstraction, deep Machine Learning mod- els handle hidden/latent information for arbitrary layers of abstraction determined heuristically. As deep learning mechanisms are typically implemented using Deep Neural Networks, this work applies deep Recurrent Neural Networks as an end-to- end discriminative classifier for speech recognition. This is a so called \'94end-to-end model\'94 because it adopts the top-to-bottom Machine Learning approaches. Unlike the typical generative classifiers that require sub-word acoustic models, the end-to- end models develop algorithms at higher levels of abstraction as well as the lower levels of abstraction. In the case of the model utilised in this research, the levels of abstraction include sentence/phrase, words and character discrimination. A second advantage of the end-to-end model is that because the traditional generative mod- els require various stages of modeling including an acoustic, language and lexicon, the end-to-end discriminating multiple levels of abstractions simultaneously only requires a single stage process, greatly reducing the quantity of resources required 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 1 I. J. Alamina 17 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 for speech recognition. From a low resource language perspective this is a desir- able behaviour meaning that the model can be learned from an acoustic only source without the need of an acoustic model or a phonetic dictionary. Thus techniques involving deep learning and end-to-end modelling are proposed and have been found to be self-sufficient (Hannun et al., 2014a) with modest results without a language model. However, applying a language model was observed to serve as a correction factor further improving recognition results (Hannun et al., 2014a). 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 1.3 Low Resource Languages 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Another challenge observed in complex Machine Learning models for both generative as well as discriminative learning models is the data intensive nature of the work required for robust classification models. Saon et al. (2015) recommends around 2000 hours of transcribed speech data for robust speech recognition system. As is covered in the next chapter, for new languages, which are low in training data such as transcribed speech, there are various strategies devised for low resource speech recognition. Besacier et al. (2014a) outlines various matrices for bench-marking low resource languages. From the generative speech model interest perspective, reference is made to languages having less than ideal data in transcribed speech, phonetic dictionary and a text corpus for language modelling. For end-to-end speech recognition models interests, the data relevant for low resource evaluation is the transcribed speech and a text corpus for language modelling. It is worth noting that it was observed in Besacier et al. (2014a) that speaker-base often does not affect a language resource status of a language and was often observed that large speaker bases could in fact lack language/speech recognition resources and that some languages having small speaker bases did in fact have sufficient language/ speech recognition resources. 
\fs24 \

\fs32 Speech recognition methods investigated in this work are motivated by the Wakirike language discussed in the next section, which is a low resource language by definition. Thus this research looked at low research language modelling for the Wakirike language from a corpus of Wakirike text available for analysis. However, due to the insufficiency of transcribed speech for the Wakirike language, English lan- 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 18 Chapter 1 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 guage was substituted and used as a control variable to study low resource effects of a language when exposed to speech models developed in this work. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 1.4 The Wakirike Language 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The Wakirike municipality is a fishing community comprising 13 districts in the Niger Delta area of the country of Nigeria in the West African region of the conti- nent of Africa. The first set of migrants to Wakirike settled at the mainland town of Okrika between AD860 and AD1515 at the earliest. These early settlers had migrated from Central and Western regions of the Niger Delta region of Nigeria. As the next set of migrants also migrated from a similar region, when the second set of migrants met with the first settlers they exclaimed \'93we are not different\'94 or \'93Wakirike\'94 (S., 2008). 
\fs24 \

\fs32 Although the population of the Wakirike community from a 1995 report (Simons and Fennig, 2018) is about 248,000, the speaker base is significantly less than stip- ulated. The language is classified as Niger-Congo and Ijoid languages. The writing orthography is Latin and the language status is 5 (developing) (Simons and Fennig, 2018). This means that although the language is not yet an endangered language, it still isn\'92t thriving and it is being passed on to the next generation at a limited rate. 
\fs24 \

\fs32 The Wakirike language was the focus for this research. And End-to-end deep neural network language model was built for the Wakirike language based on the availability of the new testament bible printed edition that was available for pro- cessing. The corpus utilized for this thesis work is approximately 9,000 words. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 1.5 Research aim and objectives 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 In this research, we develop speech processing models and language models which deliver robust deep and recurrent neural network implementations towards low re- source speech recognition. In particular, we develop a language model based on Gated Recurrent Unit (GRU) for the Wakirike language and a bi-directional Recur- rent Neural Network (bi-RNN) speech model for the English language. The aim of 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 1 I. J. Alamina 19 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 this research is therefore to build competitive and cross-lingual ASR systems in a well-rounded resource conservative manner 
\fs24 \

\fs32 The research objectives were as follows: 
\fs24 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa320\partightenfactor0
\ls13\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Discover fundamental tasks relating to Language learning; \uc0\u8232 \
\ls13\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Discover criteria for creating ASR platforms for new languages; \uc0\u8232 \
\ls13\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Build robust ASR systems using methods that also system resource robust; and \uc0\u8232 \
\ls13\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Build robust ASR systems using fewer resources, that is reduce the amount and/or number of resources required to build ASR systems. \uc0\u8232 Within this framework, our focus on language learning tasks was on Automatic Speech recognition while the intention was to achieve the last two objectives through one or more of the following means: \u8232 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa320\partightenfactor0
\ls14\ilvl0\cf2 \kerning1\expnd0\expndtw0 {\listtext	i.	}\expnd0\expndtw0\kerning0
Reduction of time to train speech models; \uc0\u8232 \
\ls14\ilvl0\kerning1\expnd0\expndtw0 {\listtext	ii.	}\expnd0\expndtw0\kerning0
Optimisation of sub-tasks and training architecture within the ASR pipeline; \uc0\u8232 \
\ls14\ilvl0\kerning1\expnd0\expndtw0 {\listtext	iii.	}\expnd0\expndtw0\kerning0
Reduction in the overall time taken to develop speech models; \uc0\u8232 \
\ls14\ilvl0\kerning1\expnd0\expndtw0 {\listtext	iv.	}\expnd0\expndtw0\kerning0
Make efficient use of training parallelism; \uc0\u8232 \
\ls14\ilvl0\kerning1\expnd0\expndtw0 {\listtext	v.	}\expnd0\expndtw0\kerning0
Obtain better or state of the art performance; and \uc0\u8232 \
\ls14\ilvl0\kerning1\expnd0\expndtw0 {\listtext	vi.	}\expnd0\expndtw0\kerning0
Induce model simplicity thereby reducing training and development time with- out compromising performance. \uc0\u8232 \
\pard\pardeftab720\sl360\sa240\partightenfactor0
\cf2 Furthermore, following the Interspeech 2015 Zero Resource Speech Challenge (Versteegh et al., 2015), this research also fulfilled the objectives of modelling speech at subword, word and syntax level. The Zero Resource Speech Challenge is inspired from infants ability to construct acoustic and language models of speech in an end-to- end manner. At the word and syntax level this research develops a character-based language model that reinforces subword, word and syntax level speech model based on Character-Temporal-Classification CTC. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 20 Chapter 1 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 1.6 Main Contribution to knowledge 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 This work uses a well-established neural language model for the low resourced lan- guage of Wakirike. At the same time this work implements a unique combination of end-to-end deep recurrent neural network models with a robust and state of the art audio signal processing mechanism involving a hierarchical Deep Scattering Network (DSN) to engineer high-dimensional features to compete with current acoustic and deep architectures for speech recognition. While the base-line model gave state of the art performance from a multi-feature input on a Medium Vocabulary Continuous Speech Recognition corpus, with a Character Error Rate of 9.5%; the DSN-feature network was shown to be heading towards model saturation. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 1.7 Thesis outline 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The engineered systems, methods and supporting literature contained in this thesis report follows the following outline and describe the research outputs of an end-to- end speech recogniser and develops the theory based on the building blocks of the research outputs. 
\fs24 \

\fs32 Chapter two introduces the speech recognition pipeline and the generative speech model. Chapter two also outlines the weaknesses in the generative model and de- scribes some of the Machine Learning techniques applied to improve speech recog- nition performance. The methods and techniques and description of the various tools and metrics for analysis of the research outputs are described and examined in Chapter three. 
\fs24 \

\fs32 Various Low speech recognition methods are reviewed and the relevance of this study is also highlighted. Chapter four describes Recurrent Neural Networks (RNNs). Starting with Multi Layer Perceptrons (MLPs), we go on to specialised recurrent neural networks including Long Short-Term Memory (LSTM) networks and the Gated Recurrent Unit (GRU) are detailed. These recurrent neural network units form building blocks of the language model for Wakirike language implemented in this work. 
\fs24 \

\fs32 Chapter five explains the wavelet theorem as well as the deep scattering spec- trum. The chapter develops the theory from Fourier transform and details the 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 1 I. J. Alamina 21 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 significance of using the scattering transform as a feature selection mechanism for low resource recognition. 
\fs24 \

\fs32 Chapters six and seven give descriptions of the models developed by this thesis and details the experimental setup along with the results obtained. Chapter eight is the conclusion of the work and recommendations for further study. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 1.8 Chapter Summary 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Amidst seeming large success of speech-to-text technology referred to as Automatic Speech Recognition (ASR), there are still areas in which ASR technology struggle to perform up to the minimum acceptable level. Situations such as very noisy environments and far field speech recognition constitute common physical scenarios where ASR performance degrades significantly. Another non-physical area in which ASR falls short of acceptable performance and chosen as the focus of this research is the area of low-resource speech recognition. This is the scenario where languages not rich in linguistic resources are unable to use existing resources and algorithms used in languages rich in linguistic and ASR resources, to perform automatic speech recognition. 
\fs24 \

\fs32 As this chapter identifies, the ASR problem is traditionally a () problem that models where speech models are trained from language-specific data. While these speech models may perform well for the languages the models were trained for, when introduced to a different language, having a different set of learning features, these pre-trained models fall short of expected performances for these new languages. Moreover, if the new languages do not possess a rich set of linguistic features, in- cluding resources such as aligned speech and an online text corpus amongst others (Besacier et al., 2014b), it becomes time-consuming and extremely laborious to de- velop new ASR models for speech recognition for these so-called ASR \'93low-resource\'94 languages. 
\fs24 \

\fs32 This chapter also introduces the Wakirike language as a low resource language and the motivating language for this research. In addition, the various machine learning architectures used in this research for low resource speech recognition for the Wakirike and for English language are reviewed. In particular, Deep Neural 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 22 Chapter 1 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Networks (DNNs) are highlighted as choice algorithms in speech recognition, and then, the Chapter goes on to describe the research novelty and the outline of this thesis. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 1 I. J. Alamina 23 
\fs24 \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 Chapter 2 Literature Review 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The speech recogniser developed in this thesis is based on an end-to-end discrimina- tive deep recurrent neural network. Two models were developed. The first model, a Gated Recurrent Unit Recurrent Neural Network (GRU-RNN), was used to develop a character-based (). The second model is a bi-directional Recurrent Neural Net- work (bi-RNN) is an end-to-end speech model capable of generating word sequences based on learned character sequence outputs. This chapter describes the transition from generative speech models to these discriminative end-to-end recurrent neu- ral network models. Low speech recognition strategies are also discussed and the contribution to knowledge gained by using character-based discrimination as well as introducing deep scattering features to the bi-RNN speech model is brought to light. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 2.1 Speech Recognition Overview 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Computer speech recognition takes raw audio speech and converts it into a sequence of symbols. This can be considered as an analog to digital conversion as a continuous signal becomes discretised. The way this conversion is done is by breaking up the audio sequence into very small packets referred to as frames and developing discriminating parameters or features for each frame. Then, using the vector of features as input to the speech recogniser. 
\fs24 \

\fs32 A statistical formulation (Young et al., 2002) for the speech recogniser follows given that each discretised output word in the audio speech signal is represented as 
\fs24 \

\fs32 24 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 a vector sequence of frame observations defined in the set O such that\uc0\u8232 O = o
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 ,o
\fs21\fsmilli10667 \dn6 2
\fs32 \up0 ,...,o
\fs21\fsmilli10667 \dn6 T
\fs32 \up0 . (2.1) 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Equation 2.1 says that, at each discrete time t, we have an observation o
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 , which 
\fs21\fsmilli10667 D 
\fs24 \

\fs32 is, in itself is a vector in R . From the conditional probability, it can be formulated that certain word sequences from a finite dictionary are most probable given a sequence of observations. That is: 
\fs24 \

\fs32 arg max\{P (w
\fs21\fsmilli10667 \dn6 i
\fs32 \up0 |O)\} (2.2) 
\fs21\fsmilli10667 t 
\fs24 \

\fs32 Section 2.1.2 outline some challenges of speech recognition which result in the analysis of P(w
\fs21\fsmilli10667 \dn6 i
\fs32 \up0 |O) being no trivial task. The divide and conquer strategy there- fore employed uses Bayes formulation to simplify the problem. Accordingly, the argument that maximises the probability of an audio sequence given a particular word multiplied by the prior probability of that word is equivalent to the original posterior probability required to solve the original speech recognition problem. This is summarised by the following equation 
\fs24 \

\fs32 P (w
\fs21\fsmilli10667 \dn6 i
\fs32 \up0 |O) = \up21 P (O|w
\fs21\fsmilli10667 \up16 i
\fs32 \up21 )P (w
\fs21\fsmilli10667 \up16 i
\fs32 \up21 ) \up0 (2.3) P (O) 
\fs24 \

\fs32 According to Bayes\'92 rule, the posterior probability is obtained by multiplying a certain likelihood probability by a prior probability. The likelihood in this case, P(O|w
\fs21\fsmilli10667 \dn6 i
\fs32 \up0 ), is obtained from a Hidden Markov Model (HMM) parametric model such that rather than estimating the observation densities in the likelihood probability, these are obtained by estimating the parameters of the HMM model. The HMM model explained in the next section gives a statistical representation of the latent variables of speech at a mostly acoustic level. 
\fs24 \

\fs32 The second parameter in the speech model, interpreted from Bayes\'92 formula, is the prior probability of a given word. This aspect of the model is the language model which is reviewed in section 2.2.1. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page25image9099648.png \width1022 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 2 I. J. Alamina 25 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 2.1: HMM Generative Model Young et al. (2002) 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 2.1.1 HMM-based Generative speech model 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 A HMM represents a finite state machine where a process transits a sequence of states from a set of fixed states (Gales et al., 2008, Young et al., 2002). The overall sequence of transitions will have a start state, an end state and a finite number of intermediate states all within the set of finite states. Each state transition emits an output observation that represents the current internal state of the system. 
\fs24 \

\fs32 In an HMM represented in Figure 2.1 there are two important probabilities. The first is the state transition probability given by a
\fs21\fsmilli10667 \dn6 ij 
\fs32 \up0 this is the probability to move from state i to state j. The second probability b
\fs21\fsmilli10667 \dn6 j 
\fs32 \up0 is the probability that an output observation is emitted when in a particular state. 
\fs24 \

\fs32 Where O, are the output observations and M is the HMM. Given that X rep- resents the sequence of states transitioned by a process, a HMM defines the joint probability of X and the output probabilities given the HMM in the following rep- resentation: 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 T 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f1\fs32 \cf2 \up29 \uc0\u56319 \u56336 
\f0 \up0 b (o)a (2.4) 
\fs21\fsmilli10667 x(t) t x(t)x(t+1) 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page26image6930432.png \width5102 \height3381 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 P(O|M)=
\f1 \up29 \uc0\u56319 \u56335 
\f0 \up0 a 
\fs21\fsmilli10667 X 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 x(0)x(1) 
\fs24 \

\fs21\fsmilli10667 t=1\uc0\u8232 
\fs32 Generally speaking, the HMM formulation presents 3 distinct challenges. The 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 first is the likelihood of a sequence of observations given in equation 2.4 above. The next two, described later, is the inference and the learning problem. While the inference problem determines the sequence of steps given the emission probabilities, the learning problem determines the HMM parameters, that is the initial transition and emission probabilities of the HMM model. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 26 Chapter 2 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 For the case of the inference problem, the sequence of states can be obtained by determining the sequence of states that maximises the probability of the output sequences. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 2.1.2 Challenges of Speech Recognition 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The realised symbol is assumed to have a one to one mapping with the segmented raw audio speech. However, the difficulty in computer speech recognition is the fact that there is a significant amount of variation in speech that would make it practically intractable to establish a direct mapping from segmented raw speech audio to a sequence of static symbols. The phenomena known as co articulation has it that there are several different symbols having a mapping to a single waveform of speech in addition to several other varying factors including the speaker mood, gender, age, the medium of speech transduction, the room acoustics, et cetera. 
\fs24 \

\fs32 Another challenge faced by automated speech recognisers is the fact that the boundaries of the words are not apparent from the raw speech waveform. A third problem that immediately arises from the second is the fact that the words from the speech may not strictly follow the words in the selected vocabulary database. Such occurrence in speech recognition research is referred to as \'94out Of vocabulary\'94 (OOV) terms. It is reasonable to approach these challenges using a divide and conquer strategy. In this case, the first step would be to make provision for word boundaries. This first step in speech recognition is referred to as the isolated word recognition case (Young et al., 2002). 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 2.1.3 Challenges of low speech recognition 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Speech recognition for low resource languages poses another distinct set of chal- lenges. In chapter one, low resource languages were described to be languages lacking in resources required for adequate Machine Learning of models needed for genera- tive speech models. These resources are described basically as a text corpus for language modelling, a phonetic dictionary and transcribed audio speech for acous- tic modelling. Figure 2.2, illustrates how resources required for speech recognition are utilised. It is observed that in addition to the three resources identified other processes are required for the speech decoder to function normally. For example, 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 2 I. J. Alamina 27 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 2.2: Automatic Speech Recognition Pipeline Besacier et al. (2014a) 
\fs24 \

\fs32 aligned speech would also need to be segmented into speech utterances to ensure that the computer resources are used conservatively. 
\fs24 \

\fs32 In terms of data collection processing Besacier et al. (2014a) enumerate the chal- lenges for developing low resource ASR systems to include the fact that phonologies (or language sound systems) differ across languages, word segmentation problems, fuzzy grammatical structures, unwritten languages, lack of native speakers having technical skills and the multidisciplinary nature of ASR constitute impedance to ASR system building. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 2.2 Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 In this system building speech recognition research, the focus was on the develop- ment of a language model and an end-to-end speech model comparable in perfor- mance to state of the art speech recognition system consisting of an acoustic model and a language model. Low resource language and acoustic modelling are now re- viewed keeping in mind that little work has been done on low-resource end-to-end speech modelling when compared to general end-to-end speech modelling and gen- eral speech recognition as a whole. 
\fs24 \

\fs32 From an engineering perspective, a practical means of achieving low resource speech modelling from a language rich in resources is through various strategies of the Machine Learning sub-field of transfer learning. 
\fs24 \

\fs32 Transfer learning takes the inner representation of knowledge derived from train- ing algorithm used from one domain and applying this knowledge in a similar domain 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page28image6981456.jpg \width6231 \height3083 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 28 Chapter 2 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 having different set of system parameters(Ramachandran et al., 2016). Early work of this nature for speech recognition is demonstrated in (Vu and Schultz, 2013) where multi-layer perceptrons were used to train multiple languages rich in linguis- tic resources. In a later section entitled \'93speech recognition on a budget\'94, a transfer learning mechanism involving deep neural networks from (Kunze et al., 2017) is described. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 2.2.1 Low Resource language modelling 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 General language modelling is reviewed and then Low resource language modelling is discussed in this section. In section 2.1, recall from equation 2.3, the general speech model influenced by Bayes\'92 theorem. 
\fs24 \

\fs32 P (w
\fs21\fsmilli10667 \dn6 i
\fs32 \up0 |O) = \up21 P (O|w
\fs21\fsmilli10667 \up16 i
\fs32 \up21 )P (w
\fs21\fsmilli10667 \up16 i
\fs32 \up21 ) \up0 (2.5) P (O) 
\fs24 \

\fs32 The speech recognition model is a product of an acoustic model (likelihood probability),P(O|w
\fs21\fsmilli10667 \dn6 i
\fs32 \up0 ) and the language model (prior probability),P(w
\fs21\fsmilli10667 \dn6 i
\fs32 \up0 ). The devel- opment of language models for speech recognition is discussed in Juang and Furui (2000) and Young (1996). 
\fs24 \

\fs32 Language modelling formulate rules that predict linguistic events and can be modelled in terms of discrete density P (W ), where W = (w
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 , w
\fs21\fsmilli10667 \dn6 2
\fs32 \up0 , ..., w
\fs21\fsmilli10667 \dn6 L
\fs32 \up0 ) is a word sequence. The density function P(W) assigns a probability to a particular word sequence W . This value determines how likely the word is to appear in an utterance. A sentence with words appearing in a grammatically correct manner is more likely to be spoken than a sentence with words mixed up in an ungrammatical manner, and, therefore, is assigned a higher probability. The order of words therefore reflect the language structure, rules, and conventions in a probabilistic way. Statistical language modeling therefore, is an estimate for P(W) from a given set of sentences, or corpus. 
\fs24 \

\fs32 The prior probability of a word sequence w = w
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 , . . . , w
\fs21\fsmilli10667 \dn6 k 
\fs32 \up0 required in equation (2.2) is given by: 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 K\uc0\u8232 
\fs32 P(w) = 
\f1 \up29 \uc0\u56319 \u56336 
\f0 \up0 P(w
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 |w
\fs21\fsmilli10667 \dn6 k\uc0\u8722 1
\fs32 \up0 ,...,w
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 ) (2.6) 
\fs24 \

\fs21\fsmilli10667 k=1\uc0\u8232 
\fs32 The N-gram model is formed by the conditioning of the word history in equation 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page25image9099648.png \width1022 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 2 I. J. Alamina 29 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 2.6. This therefore becomes: 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 K\uc0\u8232 
\fs32 P(w) = 
\f1 \up29 \uc0\u56319 \u56336 
\f0 \up0 P(w
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 |w
\fs21\fsmilli10667 \dn6 k\uc0\u8722 1
\fs32 \up0 ,w
\fs21\fsmilli10667 \dn6 k\uc0\u8722 2
\fs32 \up0 ,...,w
\fs21\fsmilli10667 \dn6 k\uc0\u8722 N+1
\fs32 \up0 ) 
\fs24 \

\fs21\fsmilli10667 k=1 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 N is typically in the range of 2-4. 
\fs24 \

\fs32 (2.7) 
\fs24 \

\fs32 N-gram probabilities are estimated from training corpus by counting N-gram oc- currences. This is plugged into maximum likelihood (ML) parameter estimate. For example, Given that N=3 then the probability that three words occurred is assum- ing C(w
\fs21\fsmilli10667 \dn6 k\uc0\u8722 2
\fs32 \up0 w
\fs21\fsmilli10667 \dn6 k\uc0\u8722 1
\fs32 \up0 w
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 ) is the number of occurrences of the three words C(w
\fs21\fsmilli10667 \dn6 k\uc0\u8722 2
\fs32 \up0 w
\fs21\fsmilli10667 \dn6 k\uc0\u8722 1
\fs32 \up0 ) is the count for w
\fs21\fsmilli10667 \dn6 k\uc0\u8722 2
\fs32 \up0 w
\fs21\fsmilli10667 \dn6 k\uc0\u8722 1
\fs32 \up0 w
\fs21\fsmilli10667 \dn6 k 
\fs32 \up0 then 
\fs24 \

\fs32 P(w
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 |w
\fs21\fsmilli10667 \dn6 k\uc0\u8722 1
\fs32 \up0 ,w
\fs21\fsmilli10667 \dn6 k\uc0\u8722 2
\fs32 \up0 ) \uc0\u8776  \up21 C(w
\fs21\fsmilli10667 \up16 k\uc0\u8722 2
\fs32 \up21 w
\fs21\fsmilli10667 \up16 k\uc0\u8722 1
\fs32 \up21 w
\fs21\fsmilli10667 \up16 k
\fs32 \up21 ) \up0 (2.8) \up5 C(w
\fs21\fsmilli10667 \up0 k\uc0\u8722 2
\fs32 \up5 w
\fs21\fsmilli10667 \up0 k\uc0\u8722 1
\fs32 \up5 ) 
\fs24 \up0 \

\fs32 The major problem with maximum likelihood estimation scheme is data sparsity. This can be tackled by a combination of smoothing techniques involving discounting and backing-off. The alternative approach to robust language modelling is the so- called class based models (Brown et al., 1992, ?) in which data sparsity is not so much an issue. Given that for every word w
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 , there is a corresponding class c
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 , then, 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 K\uc0\u8232 
\fs32 P(w)
\f1 \up29 \uc0\u56319 \u56336 
\f0 \up0 P(w
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 |c
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 )p(c
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 |c
\fs21\fsmilli10667 \dn6 k\uc0\u8722 1
\fs32 \up0 ,...,c
\fs21\fsmilli10667 \dn6 k\uc0\u8722 N+1
\fs32 \up0 ) (2.9) 
\fs24 \

\fs21\fsmilli10667 k=1 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 In 2003, Bengio et al. (2003) proposed a language model based on neural Multi- Layer Perceptrons (MLPs). These MLP language models resort to a distributed representation of all the words in the vocabulary such that the probability function of the word sequences is expressed in terms of these word-level vector representations. The performance of the MLP-based language models was found to be, in cases for models with large parameters, better than the traditional n-gram models. 
\fs24 \

\fs32 Improvements over the MLPs still using neural networks over the next decade include works of Luong et al. (2013), Mikolov et al. (2011), Sutskever et al. (2014), involved the utilisation of deep neural networks for estimating word probabilities in a language model. While a Multi-Layer Perceptron consists of a single hidden layer, in addition to the input and output layers, a deep network, in addition to having several hidden layers, is characterised by complex structures that render the architecture 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page30image9049536.png \width1120 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 30 Chapter 2 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 beyond the basic feed forward nature. Particularly, for Recurrent Neural Network (RNN) architectures, we also have some feedback neurons in addition to the forward neurons where data flows in the reverse direction, from output to input. 
\fs24 \

\fs32 Furthermore, the probability distributions in these deep neural networks were either based upon word or sub-word models, this time having representations which also conveyed some level of syntactic or morphological weights to aid in establishing word relationships. These learned weights are referred to as token or unit embedding (Pennington et al., 2014). 
\fs24 \

\fs32 For the neural network implementations so far seen, a large amount of data is required due to the nature of words to have large vocabularies, even for medium- scale speech recognition applications. Kim et al. (2016) on the other hand took a different approach to language modelling taking advantage of the long-term sequence memory of long-short-term memory cell recurrent neural network (LSTM-RNN) to model a language based on characters rather than on words. This greatly reduced the number of parameters involved and therefore the complexity of implementation. This method is forms the basis of the Wakirike language model implementation in this work due to the low resource constraints gains made when using a character-level language model. 
\fs24 \

\fs32 Other low resource language modelling strategies employed for the purpose of speech recognition was demonstrated by Xu and Fung (2013). The language model developed in that work was based on phrase-level linguistic mapping from a high re- source language to a low resource language using a probabilistic model implemented using a Weighted Finite State Transducer (WFST). This method uses WFST rather than a neural network due to scarcity of training data required to develop a neural network. However, it did not gain from the high non linearity ability of a neural net- work model to discover hidden patterns in data, being a shallower Machine Learning architecture. 
\fs24 \

\fs32 The language model implemented in this thesis report uses a character-based Neural network language model that employs a recurrent neural network similar to that of Kim et al. (2016), however based on Gated Recurrent Unit (GRU) RNNs (Cho et al., 2014), for the Okrika language which is a low resource language, bearing in mind that the character level network will reduce the number of parameters 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 2 I. J. Alamina 31 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 required for training, just enough to develop a working language model for the purpose of speech recognition. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 2.2.2 Low Resource Acoustic and speech modelling 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Two transfer learning techniques for acoustic modelling investigated by Povey et al. (2011a) and Ghoshal et al. (2013) respectively include the Sub-space Gaussian Mix- ture Model (SGMM) and the use of pretrained hidden layers of a deep neural net- work trained multilingually as a means to initialise weights for an unknown language. This second method of low resource modelling has been informally referred to as the swap-hat method. 
\fs24 \

\fs32 Recall that one of the challenges associated with new languages is that phonetic systems differ from one language to another. Transfer learning approaches attempt however to recover patterns common to seemingly disparate systems and model these patterns. 
\fs24 \

\fs32 The physiologic speech production mechanism is based on the premise that sounds are produced by approximate movements and positions of articulators that comprise the human speech production system and that this mechanism is common to all humans. It is possible to model dynamic movement from between various phones as tied state mixture of Gaussians. These dynamic states modelled using Gaussian Mixture Model (GMM) are also called senones. Povey et al. (2011a) pos- tulated a method to factorize these Gaussian mixtures into a globally shared set of parameters that are non-dependent individual HMM states. These factorisations model senones that are not represented in original data and thought to be a rep- resentation of the overall acoustic space. While preserving individual HMM states, the decoupling of the shared space and its reuse makes SGMMs a viable candidate for transfer learning of acoustic models for new languages. 
\fs24 \

\fs32 The transfer learning procedure proposed in Ghoshal et al. (2013) employed the\uc0\u8232 use of Deep Neural Networks, in particular Deep Belief Network (DBN)s (Bengio\u8232 et al., 2007). Deep Belief Networks are pretrained, layer-wise stacked s (RBMs)(Smolensky, 1986). The output of this network trained on senones correspond to HMM context dependent states. However, by decoupling hidden layers from outer and output lay-\u8232 ers and fine-tuned to a new language, the network is shown to be insensitive to the 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 32 Chapter 2 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 choice of languages analogous to global parameters of SGMMs. The 7-layer, 2000 neuron per layer network used did not utilise a bottleneck layer corresponding to triphone states trained on MFCC features (Grezl and Fousek, 2008). 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 2.3 Groundwork for low resource end-to-end speech modelling 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The underpinning notion of this work is firstly a departure from the extra process- ing required for bottom-to-top that comes as a byproduct of the generative process sponsored by the HMM-based speech models. This has an advantage of simplifying the speech pipeline from acoustic, language and phonetic model to just a speech model that approximates the same process. Secondly, the model developed seeks to overcome the data intensity barrier and was seen to achieve measurable results for GRU RNN language models. Therefore adopting the same character-based strategy, this research performed experiments using the character-based bi-directional recur- rent neural networks (BiRNN). However, BiRNNs researchers have found them like other deep learning algorithms, too be quite data intensiveHannun et al. (2014a). The next paragraphs introduce Deep-speech BiRNNs and the two strategies for tack- ling the data intensity drawback as related with low resource speech recognition. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 2.3.1 Deep speech 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Up until recently, speech recognition research has been centred on improvements of the HMM-based acoustic models. This has included a departure from generative training of HMM to discriminative training (Woodland and Povey, 2000) and the use of neural network precursors to initialise the HMM parameters (Mohamed et al., 2012). Although these discriminative models brought improvements over generative models, being HMM dependent speech models they lacked the end-to-end nature. This means that they were subject to training of acoustic, language and phonetic models. With the introduction of the Connectionist Temporal Classification (CTC) loss function, Graves and Jaitly (2014) finally found a means to end-to-end speech recognition departing from HMM-based speech recognition. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 2 I. J. Alamina 33 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 The architecture of the Deep-speech end-to-end speech recognition model Han- nun et al. (2014b) follows an end-to-end Bi-directional Recurrent Neural Network (BiRNN) and CTC loss function (Graves et al., 2006). The CTC loss function uses a modified beam search to sum over all viable sequences of the input and output sequence space alignments so as to maximise the likelihood of the output sequence characters. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 2.3.2 Speech Recognition on a low budget 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 In this section, a recent transfer learning speech model (Kunze et al., 2017) that has some characteristics similar to the speech model developed in this thesis is re- viewed. The end-to-end speech model described by Kunze et al. (2017) is based on that developed by Collobert et al. (2016) and is based on deep convolutional neural networks rather than the Bi-RNN structure proposed by this work. In ad- dition it uses a loss function based on the AutoSegCriterion which is claimed to work competitively with raw audio waveform without any preprocessing. The main strategy for low resource management in their system was the freezing of some layers within the convolutional network layer. The low resource mechanisms used in this work includes the use of a unique scattering network being used as input features for the BiRNN model. The fascinating similarity between the end-to-end BiRNN speech model developed in this work and the transfer learning model in Kunze et al. (2017) is the fact that the scattering network input is equivalent to the output of a light-weight convolutional neural network Hannun et al. (2014b). Therefore the proposed system then approximates a combination of a recurrent neural network as well as a convolution neural network without the overhead of actually training a convolutional neural network (CNN)(Szegedy et al., 2015). 
\fs24 \

\fs32 Introduction of the unique scattering network is discussed in the next section. It is worthy to note however that Kunze et al. (2017) uses a CNN network only while (Amodei et al., 2016) uses both RNN and CNN network. The speech model in this thesis uses a BiRNN model and combines an RNN model with the scattering layer which represents a light-weight low resource friendly pseudo enhanced CNN backing. What is meant by pseudo enhanced CNN backing is reserved for the next section, however, therefore, the proposed speech model in this thesis stands to gain 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 34 Chapter 2 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 from an enhanced but lightweight CNN combined with RNN learning. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 2.3.3 Adding a Scattering layer 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 In Machine Learning, training accuracy is greatly improved through a process de- scribed as feature engineering. In feature engineering, discriminating characteristics of the data are enhanced at the same time non-distinguishing features constituting noise are removed or attenuated to a barest minimum. A lot of the components signal speech signal are due to noise in the environment as well as signal channel distortions such as losses due to conversion from audio signals to electrical signal in the recording system. 
\fs24 \

\fs32 In Figure 2.2, feature engineering is done at the feature extraction stage of the ASR pipeline. It has been shown that a common technique using Mel Frequency Cepstral Coefficients (MFCC) (Davis and Mermelstein, 1980) can represent speech in a stable fashion that approximate how the working of the human auditory speech processing and is able to filter useful components in the speech signal required for human speech hearing. Similar feature processing schemes have been developed include Perceptual Linear Prediction (PLP) (Hermansky, 1990) and RelAtive Spec- TrAl (RASTA) (Hermansky and Morgan, 1994). 
\fs24 \

\fs32 The scattering spectrum defines a locally translation invariant representation of a signal resistant to signal deformation over extended periods of time spanning seconds of the signal (And\uc0\u32 \u769 en and Mallat, 2014). While Mel-frequency cepstral coefficients (MFCCs) are cosine transforms of Mel-frequency spectral coefficients (MFSCs), the scattering operator consists of a composite wavelet and modulus operation on input signals. 
\fs24 \

\fs32 Over a fixed time, MFSCs measure signal energy having constant Q bandwidth Mel-frequency intervals. This procedure is susceptible to time-warping signal dis- tortions since these information often reside in the high frequency regions discarded by Mel-frequency intervals. As time-warping distortions is not explicit classifier ob- jective when developing these filters, there is no way to recover such information using current techniques. 
\fs24 \

\fs32 In addition, short time windows of about 20 ms are used in these feature extrac- tion techniques since at this resolution speech signal is mostly locally stationary. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 2 I. J. Alamina 35 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Again, this resolution adds to the loss of dynamic speech discriminating informa- tion on signal structures that are non-stationary at this time interval. To minimize this loss Delta-MFCC and Delta-Delta-MFCCs (Furui, 1986) are some of the means developed to capture dynamic audio signal characterisation over larger time scales. 
\fs24 \

\fs32 By computing multi-scale co-occurrence coefficients from a wavelet-modulus op- eration, And\uc0\u32 \u769 en and Mallat (2011) show that non-stationary attributes of a signal lost by MFSC coefficients is regained in multi scale co-occurrence coefficients. The scat- tering transform therefore, derives a scattering representation with an interpretation similar to MFSC-like measurements. Together with higher-order co-occurrence co- efficients, deep scattering spectrum coefficients represent audio signals similar to models based on cascades of constant-Q filter banks and rectifiers. In particular, second-order co-occurrence coefficients contain relevant signal information capable of discriminating dynamic information lost to the MFCC analog over several sec- onds and therefore a more efficient discriminant than the MFCC representation. Second-order co-occurrence coefficients calculated by cascading wavelet filter banks and rectified using modulus operations have been evaluated as equivalent to a light- weight convolutional neural networks whose output posteriors are computed at each layer instead of only at the output layer (Mallat, 2016). 
\fs24 \

\fs32 The premise for this work is that low speech recognition can be achieved by having higher resolution features for discrimination as well as using an end-to-end framework to replace some of the cumbersome and time-consuming hand-engineered domain knowledge required in the standard ASR pipeline. In addition, this research work makes contributions to the requirements for the two tracks specified in the Zero Resource challenge of 2015 (Versteegh et al., 2015). The first requirement is sub-word modelling satisfied by using deep scattering network and the second that of spoken term discovery criteria being satisfied by the end-to-end speech model supplemented with a language model. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 2.4 Chapter Summary 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 1 introduces the key terms Discriminative and Generative classification. In this Chapter, these two different classification mechanisms are compared and con- 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 36 Chapter 2 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 trasted as they relate to speech recognition. The Hidden Markov Model (HMM) is considered as the key Generative algorithm used in speech recognition. This chap- ter discusses the HMM algorithm and outlines its limitations in speech recognition. Other challenges associated with speech recognition and low speech recognition are discussed. 
\fs24 \

\fs32 The method taken by this research towards low resource recognition is described as well as current related research in speech recognition involving low resource dis- criminative strategies. In addition, transfer learning approaches in low speech speech recognition are previewed. This chapter also outlines the addition of a scattering layer towards increasing discriminating feature tangibility for speech recognition. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 2 I. J. Alamina 37 
\fs24 \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 Chapter 3 Methodology 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 This chapter describes the system building methodology (Nunamaker Jr et al., 1990) as applied to deep recurrent architectures for speech recognition. As this approach involves theory building, system development, experimentation and observation, this chapter describes the procedures which were incorporated in order to achieve the aims and objectives of this research. 
\fs24 \

\fs32 In order to arrive at the initial research questions and hypothesis a literature survey of speech processing advances was carried out. 
\fs24 \

\fs32 The initial research topic was centred around a language learning companion. Thus, a mini survey was conducted on recipients\'92 use of technology in general learn- ing. However, after the literature survey, the research was narrowed down to core language technology assistive features and speech recognition for low resource lan- guages was the chosen area of research focus. 
\fs24 \

\fs32 This research develops several software systems based on knowledge acquired from the literature survey in order to gain deeper understanding into the state of the art research results as well as building upon baseline systems in order to achieve the research aims and objectives. It was through this methodology that the final systems developed in chapters six and seven were designed and developed as a unique combination of existing research systems. While the system built in chapter seven is a combination of systems in order to generate knowledge in the field of speech recognition, the value added from the system built in chapter six relates to using already successful methods in speech recognition on a new language having linguistic data challenges. 
\fs24 \

\fs32 38 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 3.1 Assumptions 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 This research makes the following assumptions and claims. 
\fs24 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa320\partightenfactor0
\ls15\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	i.	}\expnd0\expndtw0\kerning0
The first assumption is that Software engineering systems are successfully developed using an incremental and iterative manner of increasing complexity. \uc0\u8232 \
\ls15\ilvl0\kerning1\expnd0\expndtw0 {\listtext	ii.	}\expnd0\expndtw0\kerning0
End-to-end speech models are more conservative on actual software engineer- ing complexity and in that respect are said to be utilised towards low resource speech recognition. \uc0\u8232 \
\ls15\ilvl0\kerning1\expnd0\expndtw0 {\listtext	iii.	}\expnd0\expndtw0\kerning0
End-to-end speech recognition has been made possible using recurrent neural networks (RNNs) and connectionist temporal classification (CTC) algorithm. \uc0\u8232 \
\ls15\ilvl0\kerning1\expnd0\expndtw0 {\listtext	iv.	}\expnd0\expndtw0\kerning0
By having a higher number of features, Deep scattering networks (DSNs) can better detect speech than state of the art Mel Frequency Cepstral Coefficients (MFCCs). \uc0\u8232 \
\ls15\ilvl0\kerning1\expnd0\expndtw0 {\listtext	v.	}\expnd0\expndtw0\kerning0
There is knowledge to be gained in the application of speech models to new languages. \uc0\u8232 \
\pard\pardeftab720\sl360\sa240\partightenfactor0
\cf2 Based on the above assumptions this research proposes that there is much knowl- edge to be gained from combining the use of Scatter transform features with RNNs and application of current deep RNNs in the modelling of the Wakirike language. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 3.2 Speech Processing software and tools 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 This research set out to build and evaluate several speech processing systems. Some of the systems were built by hand from scratch; however, the end products were adaptations of already existing open source speech recognition research projects. The systems and platforms adapted for this research include the following: 
\fs24 \

\fs32 \'95 CMUSphinx\uc0\u8232 \'95 Kaldi\u8232 \'95 Mozilla DeepSpeech \'95 Scatternet toolbox 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 39 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 \'95 Matlab\uc0\u8232 \'95 Tensorflow\u8232 \'95 Choregraphe 
\fs24 \

\fs32 While the research sought to focus on speech models for the Wakirike language, several other sub systems were required for but development of the baseline models in addition to the final model the following system development steps were taken to arrive at the final output models: 
\fs24 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa320\partightenfactor0
\ls16\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Auto-correlation experiments \uc0\u8232 \
\ls16\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Experiments with Nao robot \uc0\u8232 \
\ls16\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
CMUSphinx Digits speech recogniser \uc0\u8232 \
\ls16\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Digit speech recogniser using Kaldi \uc0\u8232 \
\ls16\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Python based speech alignment experiments \uc0\u8232 \
\ls16\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Sequence-to-sequence grapheme-to-phoneme (G2P) model \uc0\u8232 \
\ls16\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
TensorFlow sequence-to-sequence character-to-diacritically-labelled-character model \uc0\u8232 \
\ls16\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
GRU language model for Wakirike language based on TensorFlow \uc0\u8232 \
\ls16\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Bi-Directional LSTM-based end-to-end speech model \uc0\u8232 \
\ls16\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
ESP-Net experiments \uc0\u8232 In the following sections, the tools utilised for the systems developed and how they were utilised is discussed. Subsequently, the actual systems developed incre- mentally towards the final models are described. \u8232 
\fs37\fsmilli18667 3.2.1 CMUSphinx 
\fs32 \uc0\u8232 The CMU Sphinx recogniser system is illustrated in Figure 3.1. In a speech appli- cation or experiment, the recogniser is called within the user application and is fed with input and other control parameters that determines the recogniser behaviour. \u8232 \
\pard\pardeftab720\sl280\partightenfactor0

\fs24 \cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 40 Chapter 3 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 3.1: CMU Sphinx4 recogniser system 
\fs24 \

\fs32 From the illustration, it is observed how the components of feature extraction, acous- tic modelling, language modelling and decoding are linked within the CMU Sphinx system. Note that for identification and clarity classes/modules are capitalised in the following paragraphs. 
\fs24 \

\fs32 In the CMU Sphinx realisation, the FrontEnd module implements feature ex- traction. The Linguist module implements the acoustic modelling and the lan- guage model component. Finally, the Decoder module implements a decoder. The ConfigurationManager class is used to determine the behaviour of the recogniser by specifying the parameters of the other modules. 
\fs24 \

\fs32 From this implementation, the FrontEnd processor is the signal processing unit of Sphinx-4 parameterising signals using various implementations into a final sequence of Features. The Linguist is in charge of language and pronunciation modelling. This includes phonetic information from the Dictionary and structural information from one or more sets of LanguageModels and AcousticModels. The output of the Linguist is a SearchGraph. The Feature\'92s output from the FrontEnd and the SearchGraph output from the Linguist become the input for the SearchManager in the Decoder. The output of the decoder are Results objects. At any time prior to or during the recognition process, the researcher can via his application application issue Controls through the ConfigurationManager to each of the modules, and become a partner in the recognition process. The following subsections summarise 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page41image6986656.png \width5096 \height4353 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 41 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 the submodules (Walker et al., 2004). 
\fs24 \

\fs32 FrontEnd Module 
\fs24 \

\fs32 Being consistent with having a \'93pluggable\'94 framework, CMU Sphinx4 has the abil- ity that most of its components can be replaced and at run-time. This flexibility allows various implementations of the comprising components of the recogniser. Accordingly, the front end supports but is not limited to Mel Frequency Cepstral Coefficients (MFCC), Perceptual Linear Prediction (PLP) and Linear Predictive Coding (LPC) implementations. In addition, comprising modules within the vari- ous implementations include support for various signal processing utilities such as Hamming Windows, Discrete Cosine Transform (DCT), Bark Frequency Warping, Mel Frequency Filtering, Cepstral Mean Normalisation (CMN) etc. All the tasks therefore required by the feature extraction process are implemented in this module. 
\fs24 \

\fs32 Linguist 
\fs24 \

\fs32 The job of the Linguist is to model the higher order and lower order grammar con- tent of the audio input. This particular module caters for the acoustic model and the language model. The various Linguist implementations allow CMU Sphinx- 4 to support different tasks such as traditional Context Free Grammar (CFG), Finite-State Grammars (FSGs), finite-state transducers and small N-gram language models. This module has three pluggable modules representing the Dictionary, LanguageModel and AcousticModel. The Dictionary comprises the pronunciation of all the words to be used in the Decoder. Sphinx-4 Linguist provides primary support for the CMU Pronouncing Dictionary (Carnegie Mellon University, 2016). The SearchGraph produced by the Linguist is capable of sharing parameters such as Gaussian mixtures, transition matrices and mixture weights and Sphinx-4 pro- vides a single Acoustic model supporting acoustic models generated by the Sphinx- 3 trainer. Depending on the memory architecture various implementations of the Linguist include the FlatLinguist, DynamicFlatLinguist and LexTreeLinguist. These will either create the SearchGraph entirely in memory or on demand. Finally, the LanguageModel supports a variety of formats such as SimpleWorldListGramar which as the name implies supports a simple word list. The JSGFGramar is a BNF- 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 42 Chapter 3 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 style platform-independent realisation of the Java Speech API Grammar format. LMGrammar produces a bigram model. FSTGrammar supports finite-state transducer ARPA FST grammar format. The SimpleNGramModel support N-gram model and the LargeTriGramModel is suited to optimise memory storage. 
\fs24 \

\fs32 Decoder 
\fs24 \

\fs32 Provides a pluggable SearchManager to simplify decoding. Decoder tells SearchManager to recognise a set of Feature frames. This creates a Result object that contains\uc0\u8232 all the paths that have reached a final non-emitting state(i.e. Word endings). Ap- plications can modify the search space and Result object between steps, permitting 
\fs24 \

\fs32 the application to become a partner in the recognition process. The SearchManager is not restricted on any particular implementation, examples include Frame syn- chronous Viterbi, Bushderby, A*, bi-directional and parallel searches. 
\fs24 \

\fs32 Each SearchManager uses a token passing algorithm described by (Young, Rus- sel Thornton, 1989). A sphinx-4 token is an object that is associated with a SearchState and contains the overall acoustic and language scores of the path at a given point, a reference to the SearchState, a reference to an input Feature frame, and other relevant information. 
\fs24 \

\fs32 The SearchManager sub-framework generates ActiveLists from currently ac- tive tokens in the search trellis by pruning using a pluggable Pruner. These in turn can be modified by the application to perform both relative and absolute beam pruning. 
\fs24 \

\fs32 The SearchManager sub-framework also communicates with the Scorer, a plug- gable state probability estimation module that provides state output density values on demand. 
\fs24 \

\fs32 Other modules 
\fs24 \

\fs32 ConfigurationManager allows various module implementations to be combined in various ways. Finally, we illustrate how the ConfigurationManager creates Au- tomatic Speech Recognition (ASR) experiments using the CMU-sphinx4 objects described above in the sample code from (Lamere et al., 2003) 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 43 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl340\sa240\partightenfactor0

\fs29\fsmilli14667 \cf2 package com.example; 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl340\sa240\partightenfactor0

\fs29\fsmilli14667 \cf2 import import import 
\fs24 \

\fs29\fsmilli14667 import import import 
\fs24 \

\fs29\fsmilli14667 java . io . File ;\uc0\u8232 java . io . FileInputStream ; java . io . InputStream ; 
\fs24 \

\fs29\fsmilli14667 edu .cmu. sphinx . api . Configuration ;\uc0\u8232 edu .cmu. sphinx . api . SpeechResult ;\u8232 edu .cmu. sphinx . api . StreamSpeechRecognizer ; 
\fs24 \

\fs29\fsmilli14667 Configuration configuration = new Configuration (); configuration\uc0\u8232 . setAcousticModelPath(\'94resource :en\u8722 us\'94); configuration 
\fs24 \

\fs29\fsmilli14667 . s e t D i c t i o n a r y P a t h ( \'94 r e s o u r c e : cmudict\uc0\u8722 en\u8722 us . d i c t \'94 ) ; configuration 
\fs24 \

\fs29\fsmilli14667 . setLanguageModelPath ( \'94 r e s o u r c e : en\uc0\u8722 us . lm . bin \'94 ) ; 
\fs24 \

\fs29\fsmilli14667 StreamSpeechRecognizer recognizer = new StreamSpeechRecognizer ( configuration ); 
\fs24 \

\fs29\fsmilli14667 InputStream stream = new FileInputStream(new File(\'94test .wav\'94)); 
\fs24 \

\fs29\fsmilli14667 recognizer . startRecognition (stream );\uc0\u8232 SpeechResult result ;\u8232 while (( result = recognizer . getResult ()) != null) \{ 
\fs24 \

\fs29\fsmilli14667 System.out.format(\'94Hypothesis: %s\\n\'94, result.getHypothesis()); \} 
\fs24 \

\fs29\fsmilli14667 recognizer . stopRecognition (); 
\fs24 \

\fs29\fsmilli14667 class TranscriberDemo \{ 
\fs24 \

\fs29\fsmilli14667 public\uc0\u8232 public static void main( String [ ] args ) throws Exception \{ 
\fs24 \

\fs29\fsmilli14667 \} \} 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The above java code sample represents a user application. We see three classes be- ing imported. The Configuration, SpeechResult, and StreamSpeechRecognizer class. The Configuration object holds resources for the acoustic model, language 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 44 Chapter 3 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 model and phonetic dictionary. The SpeechRecognizer object has different im- plementations representing the source of the speech signal. In the above sam- ple the StreamSpeechRecogniser class is used to load the speech signal from a wave (.wav) file. However other speech signal sources are available such as the LiveSpeechRecogniser which implements loading the speech sound signal from a microphone device if available. In addition, Walker et al. (2004) 4 asserts that the Sphinx-4 system provides additional tools and utilities that contain helper classes for computing recognition statistics such as Word Error Rate (WER), phoneme error rates (PER) etc. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 3.2.2 Kaldi 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 CMU Sphinx provides an object-oriented approach to speech recognition. Kaldi Povey et al. (2011b) on the other hand is a highly modularised library written in C++. Kaldi is based on weighted finite state transducers (WFSTs) used for in- ference graphs and decoding. The Kaldi WFSTs utilises OpenFst, an open source library, at its core. Together with a collection of configuration scripts for building complete recognition systems, Kaldi supports modeling of a variety of speech model variations with vast support for linear and affine transforms of speech features of ar- bitrary phonetic-context sizes. Kaldi is specifically suited for acoustic modeling with subspace Gaussian Mixture Models (SGMM) in addition to the standard Gaussian Mixture Models (GMMs). 
\fs24 \

\fs32 Architecture 
\fs24 \

\fs32 The component architecture of Kaldi is illustrated in the figure below. Modules can be divided into those that utilise the linear algebra libraries and those that use OpenFST. The decodable class forms the link between these two scopes. The rest of the modules lower down the hierarchy are based on modules higher up hierarchy according to this divide. 
\fs24 \

\fs32 Feature Extraction 
\fs24 \

\fs32 Kaldi supports various speech feature outputs including the standard Mel Frequency Cepstral Coefficients (MFCC), Perceptual Linear Prediction (PLP), Vocal Tract 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 45 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 3.2: Kaldi Architecture(Povey et al., 2011b) 
\fs24 \

\fs32 Length Normalisation (VTLN), Cepstral Mean Variance Normalisation (CMVN), Linear Discriminant Analysis (LDA), Semi-Tied Co-variance matrix (STC),Maximum Likelihood Linear Transformation (MLLT), Heteroscedastic Linear Discriminant Analysis (HLDA). These systems are made complete with various configuration parameters for fine tuning their individual models. 
\fs24 \

\fs32 Acoustic Modelling 
\fs24 \

\fs32 Full co-variance as well as diagonal co-variance GMM modelling is implemented in Kaldi. Efficient log-likelihoods are computed using simple dot products of mean times in-variance and in-variance co-variance. The DiagGmm class is responsible for storing co-variances of Gaussian densities. The Acoustic Modelling (AM) class rep- resented by the AMDiagGmm class comprise a set of DiagGmm objects. These objects which represent Gaussian Mixture Models (GMMs) are in turn represented Prob- ability Density Function (PDF) indices which are then mapped to Hidden Markov Model (HMM) states. There are classes to represent HMM topology as well as the overarching topology representing transition modelling. These two sets of classes provide information required for developing decoding graphs. Rather than using the conventional approach for HMM modelling using hand-made decision tree for left and right phones in a mono-phone model, tree-clustering algorithms automati- cally generate the decision tree. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page46image6971936.png \width5102 \height4182 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 46 Chapter 3 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Language Modelling and Decoding Graphs 
\fs24 \

\fs32 Using the Finite-State Transducer (FST) back in addition to third party language modelling software, Kaldi is able infer sentence estimations using n-gram language models. During decoding, transition-ids are created and attached to corresponding pdf-IDs as a result of tied-state nature of phones where different phones are al- lowed to have share the same distribution. The transition-id therefore encapsulates the shared pdf-ID and the arc (transition) of phone-specific topology. This way transitions are fine-grained without adding complexity to the decoding graph 
\fs24 \

\fs32 Core decoding algorithms are implemented using C++ classes one per decoder. Decoders implement an interface which accepts an acoustic model score for a partic- ular input-symbol and frame. While single-pass decoding is achieved through C++ classes, multi-pass decoding is realised using the supporting configuration scripts. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 3.2.3 Mozilla DeepSpeech 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The DeepSpeech speech-to-text engine is an ASR speech model and model generator built by Mozilla is based on Baidu\'92s Deep Speech research paper (Hannun et al., 2014a). The system comes in two forms; an installable speech-to-text engine based on the English language and the model trainer. These components were created and run effectively on Unix based systems and to a limited extent on Microsoft Windows systems. Various options for installing the speech to text engine includes either command line based or as an application programming interface (API) using python or NodeJS. In addition, the speech-to-text (STT) engine API also supports bindings for the Rust language, GoLang and GStreamer. This thesis however, did not rely on the STT engine nor API, but rather on the model trainer which was adapted in this research for scattering transform feature-based end-to-end speech recognition. 
\fs24 \

\fs32 Runtime library dependencies of both the STT engine and the model trainer in- clude libsox, 2 for sound processing of audio; libstdc++6, libgomp1 and libpthread are used to compile the Connectionist Temporal Classification (CTC) decoder im- plementation which incorporates the KenLM trained language model Heafield et al. (2013). 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 47 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Graphics Processor Unit (GPU)-Enabled Speech Model Training 
\fs24 \

\fs32 The model trainer of the Mozilla DeepSpeech platform is facilitated by the ability to train models on a highly parallel processing Graphics Processing Unit (GPU). This enables model training-time speed-ups over traditional CPU machines. The Mozilla DeepSpeech platform recommends Nvidia Graphics 10 series processor with a system requirement of 8GB of Random Access Memory (RAM). In section 3.2.5 we introduce TensorFlow python library. Mozilla DeepSpeech platform is able to utilise the GPU using the Nvidia GPU library, CUDA. This is achieved through the python TensorFlow library created by Google as discussed in section 3.2.5. 
\fs24 \

\fs32 Common Voice training 
\fs24 \

\fs32 The speech corpus used for training in this research was obtained from the Mozilla Common Voice Initiative speech corpora. This consists of over 250 hours of speech data that is subdivided into test, development and training data sets. In addition, the data was subdivided into clean data, that is, clean audio recording with accurate translation and a small subset containing skewed data, that is, audio recording which was either noisy or lacking accurate transcriptions. The skewed data subset consisted 15-25% of the training corpus and was incorporated so that the neural network speech model could simulate and learn real world noisy audio speech-to- text translation. The Mozilla DeepSpeech model trainer provided bash scripts for importing the Common Voice speech corpora as well as converting the files into the appropriate formats and provision of mapping files for the model trainer. 
\fs24 \

\fs32 Mozilla DeepSpeech model parameters 
\fs24 \

\fs32 The model trainer consists of a root python script \'93DeepSpeech.py\'94 with various calls to other python scripts responsible for things like audio processing, distributed training, GPU configuration, training coordination. Other accessory bash scripts also present are responsible for downloading and training for different kinds of speech corpora including Mozilla Common Voice(moz, 2019a) and the Wall Street Journal (WSJ)(Paul and Baker, 1992). These sets of scripts are referred to as speech corpus importers. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 48 Chapter 3 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 In order to supply the model trainer with a set of hyper parameters for tun- ing various aspects of the Mozilla DeepSpeech platform, the following categories arguments passed to the root script ensue: 
\fs24 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa320\partightenfactor0
\ls17\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Geometry - Defines the number of neurons in the hidden layers of the neural network. \uc0\u8232 \
\ls17\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Cluster configuration - Parameters responsible for distributed training of the speech model across various nodes. \uc0\u8232 \
\ls17\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Global constants - These include all other parameters to gain fine control of the training process. These parameters include how much of the training corpus will be used and which subset should be included; early stopping for pre-trained models that have already been trained to saturation, that is to a stopping condition; the dropout rate for neural network regularisation. This is a strategy to overcome over-fitting where instead of learning inference features the data, the neural network memorizes the training data. \uc0\u8232 \
\ls17\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Global constants - These include all other parameters to gain fine control of the training process. These parameters include how much of the training corpus will be used and which subset should be included; early stopping for pre-trained models that have already been trained to saturation, that is to a stopping condition; the dropout rate for neural network regularisation. This is a strategy to overcome over-fitting where instead of learning inference features the data, the neural network memorizes the training data. \uc0\u8232 \
\ls17\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Adam optimiser - parameters for the Adam optimiser \uc0\u8232 \
\ls17\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Batching - set the number of batches during training. \uc0\u8232 \
\ls17\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Weight Initialisation - standard deviation coefficients for initialising weights. \uc0\u8232 \
\ls17\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Checkpointing - this includes the number of seconds before saving the current model parameter values to the disk. This enables resumption of training in instances where the training was interrupted. For training to resume success- fully, the resuming training geometry parameter must be exactly the same as the interrupted geometry training parameter. \uc0\u8232 \
\pard\pardeftab720\sl280\partightenfactor0

\fs24 \cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 49 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl360\sa320\partightenfactor0
\ls18\ilvl0
\fs32 \cf2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Exporting - Includes parameters for saving a saturated model for inference. \uc0\u8232 \
\ls18\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Reporting - Includes options for setting the log-level however reports are only \uc0\u8232 sent to the standard console output. \u8232 \
\ls18\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Decoder - These parameters include the path to the alphabet symbols and that of the custom CTC decoder used during decoding of the neural network output. \uc0\u8232 \
\ls18\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Inference - It is possible to use a model trainer to either perform a one-shot inference or resume training from an already exported model. The parameters used for inference are responsible for performing these stated tasks. \uc0\u8232 In addition to the above configuration there are other accessory scripts that can be used for TensorFlow specific tasks such as conversion of the output model graph to several exportable formats. \u8232 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 3.2.4 Matlab and ScatNet toolbox 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 In this research, feature processing of audio files to obtain their deep scattering transforms was achieved using a MATLAB toolbox known as ScatNet (And\uc0\u32 \u769 en et al., 2014). The ScatNet toolbox in general analyses time-series sampled analog signals and has been used successfully for music genre classification, texture and image classification (And\u32 \u769 en and Mallat, 2011, Sifre and Mallat, 2013, 2014). In particular, the scattering transforms produced are signal processing layers of increasing width where each layer constitutes the convolution of a linear filter bank wavelet operator (Wop) with a non linear complex modulus. 
\fs24 \

\fs32 ||complex signal| 
\f1 \uc0\u8902 
\f0  Wop| 
\f1 \uc0\u8902 
\f0  (low-pass filter) (3.1) 
\fs24 \

\fs32 It is the scattering transforms of the audio files that were fed into the DeepSpeech model trainer discussed in Section 3.2.3. The architecture of a scattering networks resembles a deep convolutional network in the sense that each subsequent layer is a mapping of all possible paths from the previous layer. 
\fs24 \

\fs32 ScatNet provides default options for most of the parameters that require tuning in order to derive the scattering coefficients for an input signal. In particular, for 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 50 Chapter 3 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 audio signals, the most important hyper-parameters set by the library is the number of scattering layers that captures the entire audio spectrum which is set at 2. In addition to this default, the only other parameter to set is the window period of the signal to be analysed per time. A suitable value for the window can be derived from the sampling rate of the input signal. The toolbox function S=scat(x,Wop) takes a an input signal, x, and an array of linear wavelet operators, Wop, in order to compute the scattering coefficients of the input signal. The resulting network, S is a cell array whose length M + 1 is equivalent to that of the linear filter operator. 
\fs24 \

\fs32 Wavelet Factories 
\fs24 \

\fs32 By providing optimal defaults for linear operators, ScatNet provides wavelet factories especially suited for efficient signal processing of images and sounds. Therefore, linear wavelet operators are built in a single command function through built-in \'93factories\'94, which perform wavelet analysis tasks. 
\fs24 \

\fs32 Further, the maximum number of wavelets J is automatically derived from the sampling from the sampling period T. The filter banks are formed by dilating the mother wavelet (\uc0\u968 ) by the dyadic factor (2
\fs21\fsmilli10667 \up10 1/Q
\fs32 \up0 ). In the Fourier domain this is expressed as 
\fs24 \

\fs32 \'88\'88\uc0\u8232 \u968 
\fs21\fsmilli10667 \dn6 j
\fs32 \up0 (\uc0\u969 ) = \u968 (2
\fs21\fsmilli10667 \up13 j/Q
\fs32 \up0 \uc0\u969 ) (3.2) 
\fs24 \

\fs32 For audio application, to ensure optimized frequency coverage without frequency- redundancy or overlapping, the mother wavelet (\uc0\u968 ) is chosen so that (Q
\fs21\fsmilli10667 \dn6 1 
\fs32 \up0 = 8) and (Q
\fs21\fsmilli10667 \dn6 2 
\fs32 \up0 = 1) by default. This also means that the first order filter will be of a higher frequency resolution when compared to the second order filter. 
\fs24 \

\fs32 Filter banks 
\fs24 \

\fs32 In order to visualise the filters being used by the wavelet operations and referring to Sections (5.5 and 7.1) where it is shown that the first and second order scattering coefficients are respectively defined by the following forms 
\fs24 \

\fs32 S
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 x(t, j
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 ) = |x 
\f1 \uc0\u8902 
\f0  \uc0\u968 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 1 
\fs32 \up0 | 
\f1 \uc0\u8902 
\f0  \uc0\u966 (t) (3.3) Chapter 3 I. J. Alamina 51 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 S
\fs21\fsmilli10667 \dn6 2
\fs32 \up0 x(t, j
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 , j
\fs21\fsmilli10667 \dn6 2
\fs32 \up0 ) = ||x 
\f1 \uc0\u8902 
\f0  \uc0\u968 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 1 
\fs32 \up0 | 
\f1 \uc0\u8902 
\f0  \uc0\u968 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 2 
\fs32 \up0 | 
\f1 \uc0\u8902 
\f0  \uc0\u966 (t) (3.4) 
\fs24 \

\fs32 where (\uc0\u968 
\fs21\fsmilli10667 \dn6 j
\fs32 \up0 ) are band-pass filters and (\uc0\u966 ) is a low-pass filter.\u8232 Furthermore, the wavelet transform operators(Wop) created by the wavelet factory 1d 
\fs24 \

\fs32 function are only function handles and do not have any data in themselves. A sec- ond return value may be retrieved from the wavelet factory which contains the set of filters returned as a cell array by the wavelet factory. 
\fs24 \

\fs32 [Wop,filters] = wavelet_factory_1d(N, filt_opt); 
\fs24 \

\fs32 The filters return argument has a similar structure to the scattering network where each element in the cell array corresponds to the layer order in the scatter network hierarchy. Moreover, similar to the the scatter network, the filters cell array hierarchy has M + 1 elements, where only M elements are utilised and no filters exist at M = 0. The non-zero coefficients of the band pass filters expressed in the Fourier domain, are held in filtersm.psi.filter field. When plotting these filters, they are first padded with zeros to the length of N which is the entire spectrum. Below is the sample plot made against default filters obtained by the wavelet factory 1d filter. 
\fs24 \

\fs32 The script below calculates filter banks at orders (M = 1) and (M = 2). The resulting plot is displayed in Figure 3.3. 
\fs24 \

\fs32 figure ;\uc0\u8232 for m = 1:2 
\fs24 \

\fs32 subplot(1 ,2 ,m);\uc0\u8232 hold on;\u8232 for k = 1:length(filters\{m\}.psi.filter) 
\fs24 \

\fs32 plot(realize filter(filters\{m\}.psi.filter\{k\}, N)); end 
\fs24 \

\fs32 hold off; ylim([0 1.5]); xlim([1 5
\f2 \uc0\u8727 
\f0 N/8]); 
\fs24 \

\fs32 end 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 52 
\fs24 \

\fs32 Chapter 3 
\fs24 \

\fs32 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 3.3: Scatter transform wavelet filters 
\fs24 \

\fs32 Using the Matlab API 
\fs24 \

\fs32 In the previous sections it was seen how the ScatNet toolbox calculates scatter coefficients based on wavelet theory. In this section, the scattering spectrum of an audio signal is implemented using only three command calls to ScatNet library. 
\fs24 \

\fs32 The three steps taken in this section are as follows. First load the audio file and set the properties of the audio file required for audio processing of the signal within ScatNet. Note that to do this the sampling rate of the input signal is required. Here, a clip of Handel\'92s Messiah, implemented in Matlab as a function is loaded into a \'93y\'94 variable by default with the \'93load handel\'94 command. 
\fs24 \

\fs32 Given that the sampling rate of the loaded clip is , the parameters set are 
\fs24 \

\fs32 i. N - the number of samples in the signal, and 
\fs24 \

\fs32 ii. T - the window size. Here, T is set to 4096 which corresponds to about half a second. 
\fs24 \

\fs32 load handel; % loads the signal into y\uc0\u8232 N = length(y);\u8232 T = 2^12; % length of the averaging window 
\fs24 \

\fs32 The second step is to create the filter operators for which the type of filter signal length, and the window length are passed in as parameters. It has been shown in 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page53image6926320.png \width5102 \height4278 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 53 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 the preceding sections that two layers are sufficient to capture energy contained in an audio signal and by default the quality factors of the two layers are (Q
\fs21\fsmilli10667 \dn6 1 
\fs32 \up0 = 8) and (Q
\fs21\fsmilli10667 \dn6 2 
\fs32 \up0 = 1). These default filter options are automatically integrated with the \'92audio\'92, filter type option. 
\fs24 \

\fs32 filt_opt = default_filter_options(\'92audio\'92, T); Wop = wavelet_factory_1d(N, filt_opt); 
\fs24 \

\fs32 Note that the wavelet factory functions is an intensive operations because many filters are being built at once in batch processing of signals discussed in Section 3.2.4, we therefore only perform this as part of the initialisation and the returned wavelet operators (Wop) can be reused without having to recreate them. 
\fs24 \

\fs32 Having all the parameters required, in the third and final step , call the scattering transform of y generic function scat, to derive the scatter coefficients. 
\fs24 \
\pard\pardeftab720\sl360\partightenfactor0

\fs32 \cf2     S = scat(y, Wop);\
\pard\pardeftab720\sl360\sa240\partightenfactor0
\cf2 Scatter Feature Enhancements and Batch Processing 
\fs24 \

\fs32 Having obtained the scatter coefficients, further feature enhancement is achieved by taking the log of the normalised coefficients. This can be visualised using the built-in scattergram function which produces a translation-invariant, second-order, spectrogram-like visualization of the scattering transform a one-dimensional audio signal. 
\fs24 \

\fs32 In the code snippet below, j1 is the second-order coefficients arbitrarily chosen to equal 23. The first parameter to scattergram are the first-order coefficients and the second wildcard [] parameters gathers all paths from the first order. 
\fs24 \
\pard\pardeftab720\sl360\partightenfactor0

\fs32 \cf2     j1 = 23;\
    scattergram(S\{2\},[],S\{3\},j1);\
\pard\pardeftab720\sl360\sa240\partightenfactor0
\cf2 The following functions in the code snippet below are applied to realise the log of the normalised scattergram. 
\fs24 \

\fs32 S = renorm_scat(S);\uc0\u8232 S = log_scat(S); scattergram(S\{2\},[],S\{3\},j1); 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 54 Chapter 3 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page55image6972352.png \width5102 \height3947 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Figure 3.4: Unnormalised scattergram 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page55image6972144.png \width5100 \height3985 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Figure 3.5: Log normalised scattergram 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 55 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 With the corresponding scattergram illustrated in figure 3.5. 
\fs24 \

\fs32 Finally, to utilise the scattering coefficients as features for classification tasks, we extract the vector form using format scat function. 
\fs24 \

\fs32 [S_table, meta] = format_scat(S); 
\fs24 \

\fs32 The S table is a P-by-N table, where P is the flattened total of all the scattering combined coefficients within each layer of the Deep Scattering Network (DSN) and N is the number of time points. The network is now feature ready for classification tasks using an affine space classifier. 
\fs24 \

\fs32 For batch processing ScatNet provides a database feature which can accept a collection of input vectors rather than a single input signal. The following commands show ScatNet commands for performing batch processing on the GTZAN dataset used for musical genre classification 
\fs24 \

\fs32 First, specify the path to the audio target. 
\fs24 \

\fs32 src = gtzan_src(\'92/path/to/dataset\'92); 
\fs24 \

\fs32 Next all the defaults for ScatNet analysis and processing are set as explained in the previous Sections 3.2.4,3.2.4 and 3.2.4 above. 
\fs24 \

\fs32 N = 5*2^17;\uc0\u8232 T = 8192;\u8232 filt_opt.Q = [8 1];\u8232 filt_opt.J = T_to_J(T, filt_opt);\u8232 scat_opt.M = 2;\u8232 Wop = wavelet_factory_1d(N, filt_opt, scat_opt); feature_fun = @(x)(format_scat( ... log_scat(renorm_scat(scat(x, Wop))))); 
\fs24 \

\fs32 It is possible to optimise the training by sub-sampling each signal. The fea- ture sampling option is used to specify sub-sampling. 
\fs24 \

\fs32 database_options.feature_sampling = 8;\uc0\u8232 Finally, a call is made to prepare database function to compute all the scatter 
\fs24 \

\fs32 network features of the src database.\uc0\u8232 56 Chapter 3 I. J. Alamina 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 database = prepare_database(src, feature_fun, database_options); 
\fs24 \

\fs32 In this research, further speed up was achieved by utilising Matlab\'92s parallel processing on the for loop (see Appendix III) thus bypassing the batch processing utility of ScatNet. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 3.2.5 TensorFlow 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 TensorFlow is a state-of-the-art high performance library by Google for Deep learn- ing. Deep learning is a branch of artificial intelligence which acquires learning from deep neural network architectures. The paragraphs and subsections that follow un- der this topic give an overview of the TensorFlow library as outlined by the following authors Abadi et al. (2016), Goldsborough (2016) and Abadi et al. (2017). 
\fs24 \

\fs32 Deep learning has significantly advanced in various application domains and by far out-performed traditional approaches. TensorFlow offers researchers and enthusiasts an open source software library for use in defining, training and deploying deep learning models. 
\fs24 \

\fs32 TensorFlow works by defining data flow graphs with mutable state. A data flow graph represent complex functional node and edge architectures, where each node represents an operator instance applied to input values which constitutes the edges. The operators are implemented by abstract kernels for particular types of interchangeable devices (such as CPUs and GPUs)(Abadi et al., 2017). 
\fs24 \

\fs32 There are three main concepts at TensorFlow\'92s core. These concepts are ten- sors, operations and mutability. Tensors are arrays of arbitrary dimensions where the underlying data type is either specified or inferred at graph-construction time. Operations process data and constitute nodes within the compute graph. Basic operations invariably are mathematical functions such as vector dot products. How- ever, some of the operations indeed may be associated with a read or state update. Such tensor which permit run-time updates in TensorFlow are referred to as vari- ables. Finally, there may be edges for communicating and constrain the order of execution. These structures invariably affect the observable graph semantics and may also affect the computation performance. 
\fs24 \

\fs32 Once a TensorFlow program constructs a graph using a client interface such the Python API, the TensorFlow program can send messages to the graph, by \'93feeding\'94 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 57 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 3.6: Sample TensorFlow computation graphs(Goldsborough, 2016) 
\fs24 \

\fs32 it inputs and \'93fetching\'94 outputs from it. TensorFlow then propagates the input values through the execution graph performing operations called by the client code, until all nodes instructed to run returned with their outputs. 
\fs24 \

\fs32 Data dependencies and control edges, dictate the order of execution. Often, a graph is executed severally and tensors declared as placeholders or constants are used once. However, variable tensors have mutable state which allow persistence across multiple executions. The parameters of the model stored in variables are usually updated as part of running the graph. 
\fs24 \

\fs32 Programming Model 
\fs24 \

\fs32 In this section examples of execution data flow graphs are given; and in the following sections we highlight the other major special features of TensorFlow including auto- matic differentiation and back-prop algorithm implementation, control flow, check pointing, programming interface, sample implementation and graph visualisation. 
\fs24 \

\fs32 In a TensorFlow computational data flow graph, vertices or nodes of the directed graph represent operations, while edges signify flow of data between these vertices or operations. Thus labels on nodes are representative of the actions taken at that node. Similarly, labels representing values flow in the direction of the edges. The inputs to a labeled operation are therefore the labels which have edges directed towards the operation. A computation or data flow graph is illustrated in Figure 3.6. 
\fs24 \

\fs32 The left graph displays a basic compute graph consisting of an addition operator 58 Chapter 3 I. J. Alamina 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page58image6943744.png \width5102 \height3557 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 3.7: Tensorflow graph with backprop nodes (Goldsborough, 2016) 
\fs24 \

\fs32 having two input variables x and y. The result, z is the output of the + operation. The right graph gives an example logistic regression function. y\'88 is the final output of the function for some sample vector x, weight vector w and scalar bias b. As shown in the graph, y\'88 is the output of the sigmoid or logistic function \uc0\u963  
\fs24 \

\fs32 Backprop nodes 
\fs24 \

\fs32 The Backprop algorithm Goodfellow et al. (2016) is an efficient method to compute error values for weights within a multilayer or deep neural network. The algorithm is summarised as follows. Assuming a neural network with two hidden layers. The two layerswithinthenetworkrespectivelyhaveoutputfunctionsf(x;w)andg(x;w)such that f(x;w) = f
\fs21\fsmilli10667 \dn6 x
\fs32 \up0 (w) and g(x;w) = g
\fs21\fsmilli10667 \dn6 x
\fs32 \up0 (w). Where x is the input from the previous layer or from the input layer and are the weights. The error function , is an implicit function of all the previous layers. In the case of the 2-layer network e = (f
\fs21\fsmilli10667 \dn6 x 
\f3\fs32 \up0 \uc0\u9702 
\f0  g
\fs21\fsmilli10667 \dn6 x
\fs32 \up0 )(w) = f
\fs21\fsmilli10667 \dn6 x
\fs32 \up0 (g
\fs21\fsmilli10667 \dn6 x
\fs32 \up0 (w)). The back prop algorithm uses the chain rule to correctly assign appropriate updates to each weight at every layer within the network. The updates which are the gradient or the error function with respect to the weights are de
\fs21\fsmilli10667 \dn6 x
\fs32 \up0 /dw. The backprop algorithm therefore uses the formula [f
\fs21\fsmilli10667 \dn6 x
\fs32 \up0 (g
\fs21\fsmilli10667 \dn6 x
\fs32 \up0 (w))]
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0  
\fs32 \up0 = f
\fs21\fsmilli10667 \dn8 x
\f2 \up10 \uc0\u8242 
\f0  
\fs32 \up0 (g
\fs21\fsmilli10667 \dn6 x
\fs32 \up0 (w)) \'b7 g
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0\fs32 \up0 x(w) as it traverses the graph in reverse to compute the updates. 
\fs24 \

\fs32 Figure 3.7 illustrates how tensor implements backprop by adding two extra nodes at the appropriate layers within the network to satisfy the chain rule. For each 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page59image6927360.png \width5102 \height4528 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 59 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 operation encountered, there is a corresponding gradient function that reverses the output as a function of the inputs. The output of this gradient function can then be propagated backwards to a previous operation which would represent a previous layer within a neural network. The gradient function propagated to the previous layer is then used to complete the parameters of the chain rule by multiplying with that previous layer\'92s gradient. This output is ready to be propagated down the network to the subsequent layer to perform a similar function. This process continues until all the weights within the network have appropriate update values. 
\fs24 \

\fs32 Control flow 
\fs24 \

\fs32 TensorFlow also supports control-flow operations. For this reason TensorFlow is not a directed acyclic graph (DAG) but can support cyclic structures. If the number of loops required by the computation graph is known at graph construction. It is easy to maintain a DAG structure simply by unrolling the number of loops speci- fied. However, this is not always the case. There are instances in which a variable number of loops is required at runtime. Hence, the computation graph becomes in- creasingly complex. This is particularly the case for back gradient descent and back propagation of errors (see section 3.2.5 for a walk through). The process of stepping back through a loop in reverse to compute gradients is known as back-propagation through time (Al-Rfou et al., 2016). 
\fs24 \

\fs32 Checkpoints 
\fs24 \

\fs32 One can add Save a node to a compute graph, connecting them to variables whose tensors can then be serialized. At another instance one may connect the same variable to a Restore operation. This operation deserializes the stored tensor at another point within the execution graph. This is especially useful over long periods of training to keep track of the model\'92s variable parameters. These elements form part of distributed TensorFlow\'92s fault tolerance ecosystem. 
\fs24 \

\fs32 Programming Interface 
\fs24 \

\fs32 TensorFlow implementation provides two developer interfaces which include the Python interface and the C++ interface. While the python interface offers a rich 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 60 Chapter 3 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 feature set for creation and execution of computation graphs, the C++ interface is primarily a back end implementation with a much more limited API primarily used for executing graphs built with Python and serialised to Google\'92s protocol buffer. 
\fs24 \

\fs32 It is worth noting that unlike PyTorch (Ketkar, 2017), the Python API hand- shakes very well with NumPy(Oliphant, 2006\'96) numeric and scientific open source programming library. As such, TensorFlow tensors can be naturally substituted with NumPy ndarrays without any need for type-conversion seen in PyTorch tensors. 
\fs24 \

\fs32 Tensorflow client model walk through 
\fs24 \

\fs32 In this section, a sample client tensorflow model is examined. The model consists of a 
\fs24 \

\fs32 simple multi-layer perceptron (MLP) with one input and one output layer to classify 
\fs24 \

\fs32 hand-writtin digits in the MNIST(Krizhevsky et al., 2012) dataset. In this dataset, 
\fs24 \

\fs32 the examples are small images 28 \'d7 28 pixels depicting handwritten digits from 0 to 
\fs24 \

\fs32 9. The examples form a matrix having the shape X 
\f2 \uc0\u8712 
\f0  R
\fs21\fsmilli10667 \up10 n\'d7784 
\fs32 \up0 where represents the 
\fs24 \

\fs32 number of images, and 784 represents the flattened 28 x 28 pixel image. The client 
\fs24 \

\fs32 code performs an affine transform operation, X \'b7 W + b, where W is the matrix 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 n\'d710 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 sof tmax(x)
\fs21\fsmilli10667 \dn6 i 
\fs32 \up0 = \up21 exp(x
\fs21\fsmilli10667 \up16 i 
\fs32 \up21 ) \up0 (3.5) \uc0\u931 
\fs21\fsmilli10667 \dn6 j 
\fs32 \up0 exp(x
\fs21\fsmilli10667 \dn6 j
\fs32 \up0 ) 
\fs24 \

\fs32 Error loss values are then computed using an objective function and the model\'92s current training parameters W and . This is obtained from the cross entropy cal- culation given by 
\fs24 \

\fs32 H(L,Y)
\fs21\fsmilli10667 \dn6 i 
\fs32 \up0 =\uc0\u931 
\fs21\fsmilli10667 \dn6 j
\fs32 \up0 L
\fs21\fsmilli10667 \dn6 i,j 
\fs32 \up0 \'b7log(Y
\fs21\fsmilli10667 \dn6 i,j
\fs32 \up0 ) (3.6) 
\fs24 \

\fs32 Where Y = softmax(x) and L are the correct one-hot-encoded labels. More precisely, the batch-mean loss over all inputs x. 
\fs24 \

\fs32 Next, the Stochastic Gradient Descent (SGD) is run to update the weights of our model. A TensorFlow class is provided and will be initialised with a learning rate. The minimise function of this class takes the loss tensor as parameter used for 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 784\'d710 10 
\fs32 ,andbisavectorofbiases
\f2 \uc0\u8712 
\f0 R . Theresultoftheaffine 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 ofweights
\f2 \uc0\u8712 
\f0 R\uc0\u8232 transform operator is the matrix Y 
\f2 \uc0\u8712 
\f0  R\uc0\u8232 gives an unnormalised distribution of digits.In order to obtain the valid probability distribution Pr[x = i] where x-th example is classified as the digit i, the soft-max method is utilised. 
\fs24 \

\fs32 . The resulting non-probabilistic logits 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page61image9049344.png \width745 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 61 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 minimisation.\uc0\u8232 The operations run repeatedly within a tf.Session context manager. Refer to 
\fs24 \

\fs32 Appendix IV for the complete code listing. 
\fs24 \

\fs32 Visualisation 
\fs24 \

\fs32 TensorFlow interface offers the option of visualising computation graphs. Complex topologies consisting of various sub-layers can be presented in a lucid form, offering the user a congruent, organised picture of exactly how data is consumed in a compute graph. Sub-graphs may be grouped into visual blocks and referred to in name scopes. For example a single neural network layer may take up such a named scope. The name scopes are then interactively expanded on to give the detailed group visualisation. 
\fs24 \

\fs32 Two types of metrics are obtainable from the TensorBoard. These are summary operations, when attached as nodes in the graph, permit the user to monitor in- dividual tensor values over time. The first is the scalar summaries which capture tensor values and can be sampled at certain points within training epochs. One can now, for example, observe the trend of the accuracy loss of the training model over time. 
\fs24 \

\fs32 The other summary operation offers the user the ability to track distributions, such as final soft-max densities or the distribution of neural network weights. 
\fs24 \

\fs32 Lastly, sample images can be visualised on the TensorBoard graph. This way kernel filters of a convolutional neural network can also be visualised. In addition to all of these, one can perform zooming and panning actions directly on TensorBoard\'92s web interface including expansion and collapsing of individual name scopes 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 3.2.6 Choregraphe 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The Choregraphe software tool is a high-level language used for programming of Nao humanoid robots. This is built on top of the Naoqi/Gentoo Unix/Robot Operating System(ROS) ??. Speech recognition and processing modules of the Choregraphe tool were explored and expanded at the initial stages of the research. However the Choregraphe software tool for the Nao robot was found to be unsuitable in speech recognition at the level of research that aligned with the research objectives and 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 62 Chapter 3 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 therefore was not utilised in this work. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 3.2.7 Alisa 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Alisa tool is a lightly supervised sentence segmentation tool based on Voice Activity Detection (VAD) algorithms (Stan et al., 2016). It is so called lightly supervised because it requires small amounts of training data. Generally the tool was asserted to be optimised for sentence segmentation and offered assistance in the creation of new speech corpora in a language-independent fashion. 
\fs24 \

\fs32 The Alisa tool researchers deploy a two-step method for aligning speech, and claim performance up to 70% imperfect transcriptions often found in online resources can be successfully aligned with a word error rate of less than 0.5%. This tool is therefore said to be suitable for development multilingual and under-resourced language aligned speech-corpora. 
\fs24 \

\fs32 The motivation behind Alisa was to reduce the time and effort used to gather large amounts of quality data as well as actively eliminate the domain knowledge required to phonetically transcribe speech data. In addition, and as a bonus to achieving the first objective, is the ability to migrate speech technology fairly seam- lessly from one language to another and therefore realise the rather tedious task of automatic transcription of a new language. 
\fs24 \

\fs32 Alisa Architecture 
\fs24 \

\fs32 The goal of automatic transcription of new language with low resource constraint is particularly valuable to this research and as such, it would be relevant to review the enhancements introduced to Alisa. The two step-method consists of a GMM-based sentence level segmenter and also an iterative grapheme acoustic model used for alignment. The sentence level GMM-based speech segmenter is used to automati- cally segment speech into utterances which as discussed earlier forms the basic unit of processing within any ASR system. This attempts to relieve the researcher off the manual process of segmenting the continuous audio file manually. This process included a GMM-based voice activity detector trained from about 10 minutes of manually labeled data. The second step grapheme based acoustic model is supple- mented with a highly restricted word network they referred to as a skip network. To- 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 63 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 3.8: Alisa Architecture(Stan et al., 2016) 
\fs24 \

\fs32 gether an iterative acoustic modelling training procedure is formulated. The method described required the initial training data and a minimal labelling procedure that involved simple letter to sound rules and inter-sentence silence segments to provide an orthographic transcript of the initial 10 minute recording data. Therefore, this process is resource-effective because non-experts can also provide this data. The actual alignment process made use of a grapheme level Viterbi decoder to drive the iteratively self-trained grapheme models. The model architecture is shown in the figure below. 
\fs24 \

\fs32 Figure 3.8 shows a block diagram of the steps involved in the alignment. The method can be applied to any language with an alphabetic writing system, given the availability of speech resource and its corresponding approximate script. 
\fs24 \

\fs32 There is an option of using a grapheme based acoustic model. This however increases the margin for error. Several steps were introduced in the Alisa tool to minimise this error margin. The chief being the introduction of a tri-grapheme acoustic model which is modeled after using context dependent triphones in tra- ditional acoustic modelling. Other techniques deployed to crash the error margin 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page64image6943952.png \width5108 \height6186 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 64 Chapter 3 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 include the use of discriminative training with the Maximum Mutual Information (MMI) criterion (Schluter and Ney, 2001) and methods described in (Novotney and Schwartz, 2009). It was observed that Alisa provided good alignment but was not fully featured. For instance it had no way of adding insertions and substitutions in the audio data not provided in the transcription. Finally, Alisa was found to be restricted to only languages that can utilise the English alphabet. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 3.3 Initial Experiments 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The experiments in the following sections describe initial experiments based on the initial study of a language learning companion before the research was narrowed down to a low resource speech recognition. These preliminary experiments in addi- tion to a preliminary Language Learning Survey helped to narrow down the Research to the specific speech processing task of Low Resource Automatic Speech Recogni- tion (LR-ASR). 
\fs24 \

\fs32 The following sections describe analysis of raw wave-forms using auto-correlation signal processing in Matlab and experiments made with the Nao robot speech pro- cessing engine and experiments with speech recognition toolkit and speech processing tasks. These tasks include digit recognition systems using CMUSphinx and Kaldi speech recognition toolkits and speech alignment tasks using Alisa tool. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 3.3.1 Auto-correlation Experiments 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Preliminary experiments were carried out on raw speech signals in an attempt to quickly segment individual phonemes based on a basic threshold algorithm. Fur- ther experiments designed an auto-correlation algorithm to attempt to discover a phoneme alphabet in a particular data set in a semi-supervised fashion. 
\fs24 \

\fs32 This method had the goal of simulating posterior distributions of phonemes from auto-correlation estimate. This presents an unnormalised posterior distribution measurement of phoneme segments over the entire signal. 
\fs24 \

\fs32 The correlation theory is based on the idea that when a signals is superimposed on itself in a time-shifted manner, the convolution over itself is highest when the two signals have zero time lag that is, perfectly overlapped in sync and the better 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 65 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 3.9: Original waveform input for auto-correlation 
\fs24 \

\fs32 the overlapping the higher the value of the correlation and the lesser the signals are matched they tend to cancel out each other and hence a very low value of the correlation. The normalised auto-correlation value is obtained in Picone (1996) from a signal x(n) in the following equation: 
\fs24 \

\fs32 Based on experimental procedure, estimated locations of similar wave-forms rep- resenting segmented phonemes are calculated. Although the procedure is subject to degrade in the face of most of the difficulties associated with dealing with raw audio waveform, it further emphasises the need for accurate speech features and pre-processing highlighted in the previous chapter. 
\fs24 \

\fs32 This two stage procedure performs segmentation of phonemes and then discovery of phoneme clusters using a statistical auto-correlation algorithm. The process is described in the following sections. 
\fs24 \

\fs32 Segmentation 
\fs24 \

\fs32 Figure 3.10 describes the various steps of the segmentation phase while Figure 3.9 shows the original audio file. At the segmentation phase, we first of all adjust the scale of the original raw audio file to have only positive values rather than having it centred on zero (Figure 3.10a). At the next step a smoothing filter based on experimentation is used to perform both smoothing as well as determining the peaks and trough (Figure 3.10b). Then a threshold is applied to segment the waveform 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page66image6943120.png \width5102 \height3106 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f1\fs32 \cf2 \up24 \uc0\u56319 \u56333 
\f0\fs21\fsmilli10667 \up16 N \uc0\u8722 1 
\fs32 \up0 x(n)x(n \uc0\u8722  i) 
\fs21\fsmilli10667 n=0 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \up18 \uc0\u936 (i)= 
\f1 \uc0\u56319 \u56323 \up8 \uc0\u56319 \u56333 
\f0\fs21\fsmilli10667 \up0 N\uc0\u8722 1
\fs32 \dn16 x(n)
\fs21\fsmilli10667 \dn6 2
\f1\fs32 \up18 \uc0\u56319 \u56324 \u56319 \u56323 \up8 \uc0\u56319 \u56333 
\f0\fs21\fsmilli10667 \up0 N\uc0\u8722 1
\fs32 \dn16 x(n\uc0\u8722 i)
\fs21\fsmilli10667 \dn6 2
\f1\fs32 \up18 \uc0\u56319 \u56324 
\f0  (3.7) 
\fs21\fsmilli10667 \up0 n=0 n=0 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page66image9057024.png \width2375 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 66 Chapter 3 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 3.10: Original waveform input for auto-correlation based on discovered inflection points (Figure 3.10c). 
\fs24 \

\fs32 Auto-correlation 
\fs24 \

\fs32 At the auto-correlation stage estimated phoneme segment boundaries are stored in an array and cross-correlated with the original signal. Even though at a top- level view, the entire signal is auto-correlated, at the individual segment level, the signals are cross correlated against one another. Furthermore, to achieve a \'91fair\'92 correlation estimate, individual segments representing estimated phonemes need to be re-sampled to eliminate mismatching of contour representations of the individual phonemes. 
\fs24 \

\fs32 The proposed auto-correlation algorithm performs both top-down and bottom- top processing. In the first stage it does bottom-top segmentation, while in the sec- ond phase top-bottom auto-correlation. The major weakness is this auto-correlation method the segmentation algorithm, data filtering and the feature representation. The Bayesian method of segmentation (Kamper et al., 2016) which is related to this method also improved on these weaknesses was able to improve on these weaknesses by using ASR feature preprocessing and a combination of acoustic embedding Dy- namic Time Warping (DTW) for clustering rather than auto-correlation. In effect using the extracted features for clustering is in theory a better speech estimate with less intrinsic noise for classification than using an only smoothed audio data. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page67image6915504.png \width5096 \height4020 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 67 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 3.3.2 Experiments with Nao robot 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Nao is a humanoid robot developed mainly for deployment in environments for robotics education and development purposes. Nao comes with a speech recogni- tion software that offers features such as language settings and recognition sensi- tivity. However it was understandably found to be limited because the Nao robot itself does not possess the processing power to perform CPU intensive training of acoustic models. The Nao robot did however offer a level of support for using the pocketsphinx system. The pocketsphinx system is the C-language equivalent of CMUSphinx speech recognizer system also by Carnegie Mellon University. Using the pocketsphinx method, acoustic models trained high performance systems can then be deployed to Nao for fast decoding within the Nao. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 3.3.3 Digit Speech Recognition and Alignment Experiments 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 These experiments were performed using CMU Sphinx4 recognition system and Kaldi speech recognition software. While CMU Sphinx and pocketsphinx deliv- ered standard interface for speech recognition using generative hybrid models, Kaldi speech in addition also offered advanced methods such as subspace Gaussian mix- ture model used to develop cross-lingual acoustic models and deep architectures for hybrid generative-discriminative models for speech recognition. The main challenge with Kaldi was that it was CPU intensive and required a reasonable amount of parallel processing to achieve good results within a reasonable time period. 
\fs24 \

\fs32 Speech alignment experiments were performed using the Alisa Stan et al. (2016) tool which is a python based tool with calls made to the HMM toolkit Young et al. (2002). The Alisa tool alignment process undergoes a semi-supervised process and requires an error prone time-intensive manual pre-alignment procedure. The tool itself was found to be quite unstable and the output results were not very easily reproducible for further tests to be carried out on different data sets. In addition, the time-intensive pre-alignment procedure made the tool not very useful for this research. Had the tool been more successful, the tool, which utilises Voice Activity Detection (VAD) algorithms, would have been especially useful for sentence segmen- tation of long sequences of transcribed audio speech. This tool however still lacked in alignment at either a word-level or sub-word level of alignment required in ASR 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 68 Chapter 3 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 pipelines. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 3.4 End-to-end Research Experiments 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 A significant issue arises when using HMM-based toolkits such as Kaldi. This is the requirement for aligned speech. In more recent endeavours, there have been efforts towards automatic alignment of transcribed audio speech recordings through successive Baum-Welch estimation techniques Gales et al. (2014), Ragni and Gales (2018), Ragni et al. (2014). However, this technique is not particularly compatible with end-to-end goals adopted for this research as it would require preprocessing and successive pre-training of the data set. 
\fs24 \

\fs32 The following section describes the post-alignment experiments and in a later Chapter, how these methods deal with the problem of automatic speech alignment in a fashion which was compatible with end-to-end speech processing. The end-to- end requirements were desirable for low-resource speech recognition as it introduces a simpler speech model design. The downside however to the end-to-end approach is the dependency on very deep recurrent neural network structures which require large volumes of data for successful training. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 3.4.1 Tensor flow sequence-to-sequence character-to-diacritically- labelled-character model 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Experiments performed in this and the next three sections are all based on sequence- to-sequence modelling using recurrent neural networks. While the this section and the next section represent precursor experiments centred around speech recognition tasks, the later two sections represent the final experiments reported in this work. 
\fs24 \

\fs32 The character-to-diacritically labelled character model was a sequence-to-sequence diacritically labeled experiment to automatically infer diacritic transcriptions of the Wakirike language given the plain unmarked Wakirike language text as input. This is a task, when achieved successfully, then becomes a sub task towards developing a phonetic dictionary for the Wakirike Language and the phonetic dictionary in turn can be used in HMM speech recognition or equivalent end-to-end models. This ex- periment was a precursor experiment, the results of which were reserved for further 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 69 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 study. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 3.4.2 Sequence-to-sequence Grapheme-to-Phoneme (G2P) model 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 This follow up experiment to the previous experiment in section 3.4.1, attempts to automatically generate a phonetic dictionary from graphemes in a text corpus. Grapheme-to-phoneme experiments come in two flavours, the first being a contin- uation of the previous experiment, that is, using diacritically marked symbols, and the second flavour using non-marked graphemes as input. The experiments we per- formed used the latter non-marked graphemes as input. As this experiment was also a subtask in HMM speech model building, the results of these experiments were reserved for further study. 
\fs24 \

\fs32 What follows in the next three sections are sequence-to-sequence experiments actively developed in this research and are detailed in chapters (6,7 and 8). A brief summary of the experiments are highlighted in the following sections (3.4.3, 3.4.4 and 3.4.5). Note that these models all utilise TensorFlow deep learning library including the Bi-directional speech model (section 3.4.4) which is built on top of Mozilla DeepSpeech with the exception of section 3.4.5 which is based on PyTorch; a similar deep learning library. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 3.4.3 GRU language model for Wakirike language based on TensorFlow 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The language model developed in this research is a character-based sequence-to- sequence deep recurrent neural network that maps a sequence of characters to a sequence of words found in the training data set. This model met the objective of reducing the vocabulary size required for language models as well as the text corpus required as inferences could be made over the smaller-fixed character vocabulary rather than orders or magnitude larger word corpus with the possibility of out of vocabulary terms found in the training data. Though this may occur in the character sequence-model at the inference stage, it would not normally happen during training. The neural network model developed is described in Chapters 4 and 7, and consists 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 70 Chapter 3 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 of Gated Recurrent Unit (GRU) Recurrent Neural Network (RNN). The GRU is a specialised type of Long Short-Term Memory (LSTM) cell RNN. The emphasis here is on the ability to model over particularly long sequences of the training data. In this case, over long character sequences. Thus, the network is able to learn long term dependencies as would be naturally required to construct grammatically correct sentences. In essence, the RNN is able to learn grammar rules inherently from the training data. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 3.4.4 Bi-Directional LSTM-based end-to-end speech model 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 A similar LSTM sequence-to-sequence network based on Baidu Research\'92s original research design (Hannun et al., 2014a) is developed in this research for end-to-end speech recognition. This model, as its name implies, attempts to establish long term relationships by adding a reinforcing LSTM layer learning information but this time from the opposite direction, hence the bi-directional architecture. 
\fs24 \

\fs32 In addition, the model incorporates the Connectionist Temporal Classifier (CTC) decoder. This enables the model to make run-time inferences on both the character as well as estimate audio wave to character label alignment simultaneously. This makes this design accommodate end-to-end goals and ultimately simplifies the over- all design and completely eliminates the need for either manual or semi-supervised alignments mentioned previously in sections (3.2.7, 3.3.3 and 3.4). 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 3.4.5 ESP-Net Experiments 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The ESP-Net (End-to-end Speech Network) toolkit (Watanabe et al., 2018), is a speech processing toolkit that was of interest to this research because it offers end- to-end capabilities not only in Automatic Speech Recognition (ASR) but also in Text-To-Speech (TTS) or speech synthesis and other speech-sequence-processing related tasks. In addition, the toolkit offers multi-modal training combining both Attention networks Vaswani et al. (2017) with CTC Transformer networks as well as multi-channel feature representation that is, the fusing together of multiple feature representations of an audio signal. Only preliminary experiments were carried out using ESPNet and is discussed in Chapter 8 of this work. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 71 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 3.5 Method of evaluation 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 System building methodology (Nunamaker Jr et al., 1990) for speech recognition sys- tems requires models to be evaluated against speech recognition Machine Learning metrics. For language models, perplexity metric was used for evaluation. BiLin- gual Evaluation Understudy (BLEU)(Papineni et al., 2002) has also been used as a metric for evaluating language models. 
\fs24 \

\fs32 Perplexity measures the complexity of a language that the language model is designed to represent (Jelinek, 1976). In practice, the entropy of a language with an N-gram language model P
\fs21\fsmilli10667 \dn6 N
\fs32 \up0 (W) is measured from a set of sentences and is defined as 
\fs24 \

\fs32 H = 
\f1 \up29 \uc0\u56319 \u56335 
\f0  \up0 P
\fs21\fsmilli10667 \dn6 N
\fs32 \up0 (W) (3.8) 
\fs21\fsmilli10667 W
\f2 \uc0\u8712 
\f0 \uc0\u937  
\fs24 \

\fs32 where \uc0\u937  is a set of sentences of the language. The perplexity, which is interpreted as the average word-branching factor, is defined as 
\fs24 \

\fs32 PP(W) = 2
\fs21\fsmilli10667 \up13 H 
\fs32 \up0 (3.9) where H is the average entropy of the system or the average log probability defined 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 as 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \dn8 1 
\f1 \up0 \uc0\u56319 \u56335 
\f0\fs21\fsmilli10667 \up10 N 
\fs24 \up0 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 H = \uc0\u8722 \u8232 For a bi gram model therefore, equation (3.10) becomes 
\fs24 \

\fs32 (3.10) 
\fs24 \

\fs32 (3.11) 
\fs24 \

\fs32 (3.12) 
\fs24 \

\fs32 After simplifying we have 
\fs24 \

\fs32 [log
\fs21\fsmilli10667 \dn6 2
\fs32 \up0 P(w
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 ,w
\fs21\fsmilli10667 \dn6 2 
\fs32 \up0 ...w
\fs21\fsmilli10667 \dn6 N
\fs32 \up0 )] 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \up16 N 
\fs21\fsmilli10667 \up0 i=1\uc0\u8232 
\fs32 \dn14 PP(W) = 2
\fs21\fsmilli10667 \up0 H 
\fs32 \dn14 = 2
\fs21\fsmilli10667 \up0 \uc0\u8722  
\fs16 \up8 1 
\f1\fs21\fsmilli10667 \up16 \uc0\u56319 \u56333 
\f0\fs16 \up10 N 
\fs21\fsmilli10667 \up0 [log
\fs16 \dn3 2
\fs21\fsmilli10667 \up0 P(w
\fs16 \dn3 1
\fs21\fsmilli10667 \up0 ,w
\fs16 \dn3 2
\fs21\fsmilli10667 \up0 ...w )] 
\fs24 \
\pard\pardeftab720\sl180\sa240\partightenfactor0

\fs16 \cf2 \dn3 N \up0 i=1 \up2 N 
\fs24 \up0 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page72image9092864.png \width1058 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 P P (W ) = 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f1\fs32 \cf2 \up37 \uc0\u56319 \u56342 \up18 \uc0\u56319 \u56341 \up0 \uc0\u56319 \u56341 \dn19 \uc0\u56319 \u56340 
\f0\fs16 \dn16 N 
\f1\fs32 \up0 \uc0\u56319 \u56336 
\f0\fs21\fsmilli10667 \up10 N 
\fs32 \dn8 1 
\fs24 \up0 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page72image9092672.png \width783 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \up2 P (w
\fs21\fsmilli10667 \dn3 i
\fs32 \up2 |w
\fs21\fsmilli10667 \dn3 i\uc0\u8722 1
\fs32 \up2 )\up0 \uc0\u8232 Full speech recognition pipelines are usually evaluated against the Word Error 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Rate (WER). WER is computed as follows:\uc0\u8232 WER = \up21 I + D + R \up0 \'d7 100 (3.13) 
\fs24 \

\fs32 WC 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 i=1 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page72image9072448.png \width756 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 72 Chapter 3 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Here I,D, and R are wrong insertions, deletions and replacements respectively and WC is the word count. 
\fs24 \

\fs32 Metrics used for low speech recognition in the zero speech challenge (Versteegh et al., 2015) include the ABX metric. Other common speech recognition error met- rics following a similar definition as the Word Error Rate (WER) are Character Error Rate (CER), Phoneme Error Rate (PER) and Syllabic Error Rate (SyER) and sentence error rate (SER). 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 3.6 Chapter Summary 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 In this chapter we outline how this research set out to achieve its objectives. The main claim of this research is that by building a speech model that combines knowl- edge of end-to-end processing along with state of the art signal processing, the overall training complexity and build time for new ASR systems can be improved. This re- search aims to deliver this through by the unique combination of a CTC-based deep recurrent bi-directional neural network with high performance feature processing of Deep Scattering Networks (DSNs). 
\fs24 \

\fs32 This chapter also reviews the technologies utilised by this research in order to ar- rive at the research outputs and briefly describes the experiments performed. Within this space we describe CMUSphinx, Kaldi, Mozilla DeepSpeech, TensorFlow, Mat- lab and ScatNet as major libraries used. The first two of these are Hidden Markov Model (HMM)-based libraries and the rest are signal processing systems used to build Deep Recurrent Neural Network (RNN) models. Finally, we mention metrics for the evaluation of the models built in this research. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 3 I. J. Alamina 73 
\fs24 \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 Chapter 4 
\fs24 \

\fs66\fsmilli33333 Background 1: Recurrent Neural Networks in Speech Recognition 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The HMM model described in Chapter 2 uses a divide and conquer strategy which has also been described as a generative Machine Learning algorithm in which we use the smaller components\'92 representations as modelled by the HMM to learn the entire speech process. In previous chapters, this was referred to as the bottom-top strategy. The discriminative method however uses the opposite mechanism. Instead of using the building blocks of speech to determine speech parameters of a HMM, the discriminative strategy determines the posterior probability directly using the joint probability distribution of the parameters involved in the discriminative process. The discriminative approach, discussed in this chapter focuses in on Neural network architectures. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 4.1 Neural network architecture 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The building block of a neural network simulates a combination of two consecutive linear and non-linear operations having many inputs interconnected with the linear portion of the network. This rudimentary structure is described by McCullough and Pitts (1942) and in Cowan (1990) as the Perceptron in Figure 4.1 
\fs24 \

\fs32 The linear operation is the sum of the products of the input feature and a weight vector set. This vector sum of products is referred to as an affine transformation or operation. The non linear operation is given by any one of a selection of nonlinear 
\fs24 \

\fs32 74 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 4.1: Perceptron 
\fs24 \

\fs32 Figure 4.2: Neural network 
\fs24 \

\fs32 functions. In Figure 4.2 this is shown as a step function. The step function is activated (becomes 1) whenever the output of the linear function is above a certain threshold, otherwise remains at 0. A simple neural network of perceptrons is formed by stacking the perceptrons into an interconnected layer as shown in the Figure 4.2 : 
\fs24 \

\fs32 From the preceding paragraph, each combination of linear operation followed by a non linear operation is called a neuron and the total number of neurons in the layer formed is termed as M-number of neurons in the layer. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 4.1.1 Multi-layer Perceptron (MLP) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The multilayer Perceptron or MLP extends the basic Perceptron structure by adding one or more hidden layers. These hidden layers comprise the outputs of one layer becoming the input of the next layer. In the simplest case having one hidden layer, the output of layer 1 becomes the input of the final output layer. In comparison, the Perceptron is a one dimensional structure having one or more linear and non linear combination outputs, while the multilayer Perceptron is a 2-dimensional structure having one or more hidden layers of N linear and non-linear combination outputs. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page75image6944784.png \width3964 \height1748 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page75image6944992.png \width3968 \height3129 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 4 I. J. Alamina 75 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Mathematically speaking the output of each layer of an MLP having N inputs and M neurons is given by 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 z
\fs21\fsmilli10667 \dn6 j 
\fs32 \up0 =h(b
\fs21\fsmilli10667 \dn6 j
\fs32 \up0 )= \up21 1 \up0 1+e
\fs21\fsmilli10667 \up10 \uc0\u8722 b
\fs16 \up5 j 
\fs24 \up0 \

\fs32 is the non-linear function while is the linear function given by: 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 N 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 b
\fs21\fsmilli10667 \dn6 j 
\fs32 \up0 = 
\f1 \up29 \uc0\u56319 \u56335 
\f0 \up0 w
\fs21\fsmilli10667 \up16 (1) 
\fs32 \up0 j = 1,2,...,M 
\fs21\fsmilli10667 ji 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 i=0 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 (4.1) 
\fs24 \

\fs32 (4.2) 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page76image8942528.png \width565 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 For each layer in the MLP, the zeroth input value x
\fs21\fsmilli10667 \dn6 0 
\fs32 \up0 is 1 indicating a bias term. This bias term is used in the neural network to ensure regularised and expected behaviour of the neural network. In this example the non-linear step function is given by a more complex exponential. In the next section the nonlinear functions for a multilayer Perceptron is derived. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 4.1.2 Sigmoid and soft-max Activation Function 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The combination of the linear function and the non linear function in the neural network could be said to be transformation of an algebraic problem to a probabilistic function. In this case the \'94step\'94 function is a squashing sigmoid-shaped function that converts the inputs into a Naive Bayes function evaluating the probability that an output belongs to any of the output classes (C
\fs21\fsmilli10667 \dn6 y
\fs32 \up0 ) given the data (x). 
\fs24 \

\fs32 p(C
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 |x) = f(a) = f(w
\f1\fs21\fsmilli10667 \up13 \uc0\u8868 
\f0\fs32 \up0 x + w
\fs21\fsmilli10667 \dn6 0
\fs32 \up0 ) (4.3) In a two class problem with classes C
\fs21\fsmilli10667 \dn6 1 
\fs32 \up0 and C
\fs21\fsmilli10667 \dn6 2
\fs32 \up0 , the posterior probability of class C
\fs21\fsmilli10667 \dn6 1 
\fs24 \up0 \

\fs32 is expressed using Bayes\'92s theorem\uc0\u8232 \dn16 p(C
\fs21\fsmilli10667 \dn22 1
\fs32 \dn16 |x) = \up5 p(x|C
\fs21\fsmilli10667 \up0 1
\fs32 \up5 )p(C
\fs21\fsmilli10667 \up0 1
\fs32 \up5 ) 
\fs24 \up0 \

\fs32 p(x|C
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 )p(C
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 ) + p(x|C
\fs21\fsmilli10667 \dn6 2
\fs32 \up0 )p(C
\fs21\fsmilli10667 \dn6 2
\fs32 \up0 ) Dividing through by p(x|C
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 )p(C
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 ) gives us 
\fs24 \

\fs32 p(C
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 |(x) = \up21 1\up0 \uc0\u8232 \dn16 1 + 
\fs21\fsmilli10667 \up0 p(x|C
\fs16 \dn3 1
\fs21\fsmilli10667 \up0 )p(C
\fs16 \dn3 1
\fs21\fsmilli10667 \up0 ) 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 p(x|C
\fs16 \dn3 2 
\fs21\fsmilli10667 \up0 )p(C
\fs16 \dn3 2 
\fs21\fsmilli10667 \up0 ) 
\fs32 76 Chapter 4 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 (4.4) 
\fs24 \

\fs32 (4.5) I. J. Alamina 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page76image8998272.png \width2043 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page76image9007488.png \width1007 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page76image9006912.png \width698 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 If we define the ratio of the log posterior probabilities as \dn22 a = ln \up0 p(x|C
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 )p(C
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 ) 
\fs24 \

\fs32 p(x|C
\fs21\fsmilli10667 \dn6 2 
\fs32 \up0 )p(C
\fs21\fsmilli10667 \dn6 2 
\fs32 \up0 ) If we substitute back into (4) we have: 
\fs24 \

\fs32 p(C
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 |x) = f(a) = \up21 1 \dn8 1+e
\fs21\fsmilli10667 \up0 \uc0\u8722 a 
\fs24 \

\fs32 (4.6) 
\fs24 \

\fs32 (4.7) 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page77image9126272.png \width933 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page77image9126464.png \width523 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Here a = w
\f1\fs21\fsmilli10667 \up10 \uc0\u8868 
\f0\fs32 \up0 x = w
\fs21\fsmilli10667 \dn6 0
\fs32 \up0 . Thus the activation for the non-linear function is driven by the probability of the data to give the output class. The probabilistic function here is called a sigmoid function due to the s-shaped graph that is plotted by the function. 
\fs24 \

\fs32 Rather than using the sigmoid function for multi-class classification a simi- lar soft max function is derived by using the log probability of classes. If a
\fs21\fsmilli10667 \dn6 k 
\fs32 \up0 = ln(p(x|C
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 )p(C
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 )) then: 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \dn8 e
\fs21\fsmilli10667 \up2 a
\fs16 \up0 k 
\fs32 \up5 y
\fs21\fsmilli10667 \up0 k 
\fs32 \up5 = p(C
\fs21\fsmilli10667 \up0 k
\fs32 \up5 |x) = \dn16 \uc0\u931 
\fs21\fsmilli10667 \dn6 K\dn27 l=1
\fs32 \dn16 e
\fs21\fsmilli10667 \dn8 a
\fs16 \dn11 l 
\fs24 \up0 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 d\uc0\u8232 
\fs32 \up2 a
\fs21\fsmilli10667 \dn3 k 
\fs32 \up2 =
\f1 \up32 \uc0\u56319 \u56335 
\f0 \up2 w
\fs21\fsmilli10667 \dn3 ki
\fs32 \up2 x
\fs21\fsmilli10667 \dn3 i 
\fs24 \up0 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 (4.8) 
\fs24 \

\fs32 (4.9) 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page77image9126656.png \width523 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 i=0\uc0\u8232 
\fs32 Recall that in the generative classification method the problem is divided into sub 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 problems by using the conditional probability, while in the discriminative approach the joint probability is determined by looking at the data directly. This is what p(C
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 |x) represents. However also, recall that we still need to determine the correct probability distribution represented by the data. This is achieved by determining the values of the weights of the linear operation. In the next section a method known as back propagation is discussed. Back propagation is the training algorithm used to determine the weight vector of all the layers in the neural network. Back propagation is an extension of the Gradient descent algorithm. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 4.1.3 Back propagation algorithm (backprop) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 In the previous section, the neural network architecture has been described as having N inputs M neurons and L layers. Each layer comprises M neurons of a maximum of N inputs times M neurons interconnections which embodies the inner product 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 4 I. J. Alamina 77 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 of the inputs and unknown set of weights. The output of this inner product is then passed to a logistic squashing function that results in the output probabilities. The discriminative process is used here to determine the correct combination of weight vectors that accurately describe the training data. For neural networks, the weight vectors at each layer are determined through propagating the errors back through each preceding layer and adjusting the weights according to the errors propagated each time a batch of the data is processed. This process of continuously adjusting weights from back propagation continues until all the data is processed and a steady state has been reached. The steady state refers to the fact that the error has reached a steady and/or acceptable negligible value. This is often referred to in Machine Learning as convergence (Boden, 2002). 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 4.1.4 Gradient Descent 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The last section ended stating that the back-propagation algorithm is an extension of the gradient descent algorithm. It has also been seen that back propagation works by propagating the error and making adjustments on the weights. In this section, the Gradient Descent algorithm is reviewed and how it is used in back propagation is examined. 
\fs24 \

\fs32 The concept behind the Gradient descent algorithm is the fact that a function is optimized when the gradient of the function is equal to 0. Gradient descent algorithm is significant in Machine Learning applications because a cost function is easily defined for a particular Machine Learning application that is able to determine the error between the predicted value and the actual value. Then, the parameters of the problem can be adjusted until the derivative of the cost function using gradient descent is zero. Thus the Machine Learning algorithm adjusts its parameters until the error is minimised or removed. 
\fs24 \

\fs32 A common error function or cost function for neural networks is the sum-of- squares error cost function. This is obtained by summing the difference between the actual value and the Machine Learning model value over the training set N. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \dn8 1 
\f1 \up0 \uc0\u56319 \u56335 
\f0\fs21\fsmilli10667 \up10 K \up0 nnn2 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 2 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 k=1 
\fs32 Chapter 4 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 E = 
\fs24 \

\fs32 (y \uc0\u8722 t) 
\fs21\fsmilli10667 kk 
\fs24 \

\fs32 (4.10) I. J. Alamina 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 78 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 In a neural network having a weight matrix W of M neurons times N inputs, the resulting gradient is a vector of partial derivatives of E with respect to each element. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f1\fs32 \cf2 \up24 \uc0\u56319 \u56325 
\f0  \up0 \uc0\u8706 E \u8706 E \u8706 E 
\f1 \up24 \uc0\u56319 \u56326 
\f0  
\f2 \up0 \uc0\u8711 
\f0\fs21\fsmilli10667 \dn6 W
\fs32 \up0 E= \dn22 \uc0\u8706 w \up0 ,...,\dn22 \uc0\u8706 w \up0 ,...,\dn22 \uc0\u8706 w 
\fs24 \up0 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 10 ki Kd\uc0\u8232 
\fs32 The adjustment on each weight therefore on each iteration is: 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \uc0\u8706 E \up5 \uc0\u8706 w
\fs21\fsmilli10667 \up0 kj 
\fs24 \

\fs32 (4.11) 
\fs24 \

\fs32 (4.12) 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 \uc0\u964 +1 \u964 \u8232 
\fs32 w =w \uc0\u8722 \u951  
\fs24 \

\fs21\fsmilli10667 kj kj 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Where \uc0\u964  is the iteration and \u951  is a constant learning rate which is a factor to speed up or slow down the rate of learning of the Machine Learning algorithm which in this case is the neural network. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 4.2 RNN, LSTM and GRU Networks 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Neural networks have become increasingly popular due to their ability to model non- linear system dynamics. Since their inception, there have been many modifications made to the original design of having linear affine transformations terminated with a nonlinear functions as the means to capture both linear and non-linear features of the target system. In particular, one of such neural network modifications, namely the recurrent neural network, has been shown to overcome the limitation of varying lengths in the inputs and outputs of the classic feed-forward neural network. In addition the RNN is not only able to learn non-linear features of a system but has also been shown to be effective at capturing the patterns in sequential data. This section develops recurrent neural networks (RNNs) from a specialised multi-layer Perceptron (MLP) or the deep neural network (DNN). 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 4.2.1 Deep Neural Networks (DNNs) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Deep neural networks have been accepted to be networks having multiple layers and capable of hierarchical knowledge representation (Yu and Deng, 2016). This will therefore include multi-layer Perceptrons (MLPs) having more than one hidden layer (Dahl et al., 2012) as well as deep belief networks (DBNs)(Mohamed et al., 2009, Yu et al., 2010) having a similar structure. Therefore, following the MLP architecture, 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 4 I. J. Alamina 79 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 A DNN uses multiple hidden layers and generates distribution function, p(c|x
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 ) on the output layer when an input vector x
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 is applied. At the first hidden layer, activations are vectors evaluated using 
\fs24 \

\fs32 h
\fs21\fsmilli10667 \up13 (1) 
\fs32 \up0 = \uc0\u963 (W
\fs21\fsmilli10667 \up13 (1)T 
\fs32 \up0 x
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 + b
\fs21\fsmilli10667 \up13 (1)
\fs32 \up0 ) (4.13) 
\fs24 \

\fs32 The matrix W
\fs21\fsmilli10667 \up10 (1) 
\fs32 \up0 is the weight matrix and vector b
\fs21\fsmilli10667 \up10 (1)
\fs32 \up0 , the bias vector for the layer. The function \uc0\u963 (\'b7) is the point-wise non-linear function. DNNs activations h
\fs21\fsmilli10667 \up10 (i) 
\fs32 \up0 at layer i, at arbitrarily many hidden layers after the first hidden layer, are subsequently hidden activations are determined from 
\fs24 \

\fs32 h
\fs21\fsmilli10667 \up13 (1) 
\fs32 \up0 = \uc0\u963 (W
\fs21\fsmilli10667 \up13 (1)T 
\fs32 \up0 h
\fs21\fsmilli10667 \up13 (i\uc0\u8722 1) 
\fs32 \up0 + b
\fs21\fsmilli10667 \up13 (1)
\fs32 \up0 ) (4.14) 
\fs24 \

\fs32 The distribution over all the possible set of characters c is obtained in the final layer of the network in the exact way of a multi-layer Perceptron, that is, using soft max activation at the output layer of the form, 
\fs24 \

\fs32 p(c = c |x ) = 
\fs21\fsmilli10667 \up13 k k 
\fs32 \up0 (4.15) 
\fs21\fsmilli10667 \up16 k t 
\f1\fs32 \uc0\u56319 \u56333 
\f0  \dn8 exp(\uc0\u8722 (W
\fs21\fsmilli10667 \up8 (s)T 
\fs32 \dn8 h
\fs21\fsmilli10667 \up0 (i\uc0\u8722 1) 
\fs32 \dn8 + b
\fs21\fsmilli10667 \up8 (1)
\fs32 \dn8 )) 
\fs24 \up0 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 jkk 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 W
\fs21\fsmilli10667 \up16 (s) 
\fs32 \up0 and b
\fs21\fsmilli10667 \up16 (k) 
\fs32 \up0 respectively are the output weight matrix and the scalar bias term 
\fs21\fsmilli10667 kk 
\fs24 \

\fs32 of the k-th neuron. Accordingly, sub gradients for all parameters in the DNN are utilised to back propagate errors in weights during training for gradient-based op- timisation techniques. In DNN-HMM speech models, DNNs are trained to pre- dict probability distributions over senones. However, in the model neural network described in section 4.3.1, of this thesis, predicts per character conditional distri- butions. Combining equations (4.12, 4.13, 4.14 and 4.15) the following simplified 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \dn6 exp(\uc0\u8722 (W
\fs21\fsmilli10667 \up10 (s)T 
\fs32 \dn6 h
\fs21\fsmilli10667 \up5 (i\uc0\u8722 1) 
\fs32 \dn6 + b
\fs21\fsmilli10667 \up10 (1)
\fs32 \dn6 )) 
\fs24 \up0 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page80image8957760.png \width2138 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 80 Chapter 4 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 algorithm ensues 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 1 2 3 4 5 6 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Result: Optimal weights\uc0\u8232 initialise weights randomly;\u8232 while error is significant or epochs less than maximum do 
\fs24 \

\fs32 forward computation in equation (4.13 and 4.14 );\uc0\u8232 determine layer wise error for weights and biases \u8710 
\fs21\fsmilli10667 \dn6 W
\fs32 \up0 E and \uc0\u8710 
\fs21\fsmilli10667 \dn6 b
\fs32 \up0 E ; update weights and biases according to gradient descent. Equation (4.12); 
\fs24 \

\fs32 end 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page81image8973760.png \width12 \height828 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Algorithm 1: DNN training algorithm 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 4.2.2 Recurrent Neural Networks 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 One of the two advantages RNNs have over regular DNNs is the ability to capture varying lengths of outputs to inputs. That is for tasks such as language translation where there is no one to one correspondence of number of words in a sentence for example from the source language to the output destination language. At the same time the sentence length appearing at the input and that appearing at the output differ for different sentences. This is the first problem of varying lengths for input and output sequences. 
\fs24 \

\fs32 The second issue that RNNs effectively contain as opposed to DNNs is capturing temporal relationships between the input sequences. As was realised for hidden Markov models, it was seen that the HMM modeled not just observation likelihoods but also transition state likelihoods which were latent or hidden variables. By tying the output of previous neuron activations to present neuron activations, a DNN inherits a cyclic architecture becoming a recurrent neural network (RNN). As a result, an RNN is to able capture previous hidden states and in the process derive memory-like capabilities (Yu and Deng, 2016). 
\fs24 \

\fs32 In speech processing, it is observed that for a given utterance, there are vari- ous temporal dependencies which may not be sufficiently captured by DNN-based systems because DNN systems ignore previous hidden representations and output distributions at each time step t. The DNN derives its output using only the fea- ture inputs x
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 . The architecture of RNN to enable better modelling of temporal dependencies present in a speech is given in (Hannun et al., 2014b, Yu and Deng, 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 4 I. J. Alamina 81 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 2016). 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \dn14 h
\fs21\fsmilli10667 \up2 (j) 
\fs32 \dn14 = \uc0\u963 (W
\fs21\fsmilli10667 \up0 (j)T 
\fs32 \dn14 h
\fs21\fsmilli10667 \up2 (i\uc0\u8722 1) 
\fs32 \dn14 + W
\fs21\fsmilli10667 \up2 (j)T 
\fs32 \dn14 h
\fs21\fsmilli10667 \up2 (j) 
\fs32 \dn14 + b
\fs21\fsmilli10667 \up0 (j)
\fs32 \dn14 )) (4.16) 
\fs21\fsmilli10667 \up0 t tkt\uc0\u8722 1 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 It can be seen in equation (4.16) above that given a selected RNN hidden layer j, a temporally recurrent weight matrix W
\fs21\fsmilli10667 \up10 (f) 
\fs32 \up0 is computed for output activations h
\fs21\fsmilli10667 \up16 (j) 
\fs24 \up0 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 t\uc0\u8722 1 
\fs32 for the hidden activation vector of layer j at time step t \uc0\u8722  1 such that the output 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 contributes to the standard DNN output of W\uc0\u8232 equation (4.16) that the temporal recurrent weight matrix computation is a modified version of the standard DNN weight matrix computation and that the overall output is a superposition of the two. 
\fs24 \

\fs32 Since computations for a RNN are the same as those described in standard DNN evaluations, it is possible to compute the sub gradient for RNN architecture using the back propagation algorithm. The modified algorithm appropriately called back propagation through time (BPTT) (Boden, 2002, Jaeger, 2002) is derived in section 4.2.3 below. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 4.2.3 Back propagation through time (BPTT) algorithm 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 First we define an arbitrary but carefully chosen number of time steps t = 1, 2, . . . , T such that at each time step the states of the neuron activations j = 1, 2, . . . , J are captured. Using the sum-squared error as the cost function 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 TTL\uc0\u8232 
\fs32 E=c
\f1 \up29 \uc0\u56319 \u56335 
\f0 \up0 ||l
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 \uc0\u8722 y
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 ||
\fs21\fsmilli10667 \up13 2 
\fs32 \up0 =c
\f1 \up29 \uc0\u56319 \u56335 \u56319 \u56335 
\f0 \up0 (l
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 (j)\uc0\u8722 y
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 (j))
\fs21\fsmilli10667 \up13 2 
\fs32 \up0 (4.17) 
\fs24 \

\fs21\fsmilli10667 t=1 t=1 j=1 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Where c is a gradient descent convenience factor in Equation (4.17). ||l
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 \uc0\u8722  y
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 || is the modulus of the difference between the actual output y
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 and the label vector y
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 at time t. The two-step BPTT algorithm described in Yu and Deng (2016) is involves the recursive computation of the cost function and updating of the network weights. 
\fs24 \

\fs32 For each of these steps recall from equation (4.16) the activation of a hidden layer is a result of the composition of the regular DNN activation and an activation generated from weights from the previous time step. 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 \dn3 (j )T \up2 (i\uc0\u8722 1)\up0 \uc0\u8232 
\fs32 h
\fs21\fsmilli10667 \dn8 t 
\fs32 \up0 . It can also be seen from 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 82 Chapter 4 I. J. Alamina 
\fs24 \

\fs32 or 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The error term at final time t=T is 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 \dn6 y 
\fs32 \up0 \uc0\u948 E \u948 y
\fs21\fsmilli10667 \dn6 T
\fs32 \up0 (j) 
\f2\fs21\fsmilli10667 \dn8 \uc0\u8242 
\f0 \up0 \uc0\u8232 
\fs32 \uc0\u948 
\fs21\fsmilli10667 \dn8 T 
\fs32 \up0 (j) = \uc0\u8722  = (l
\fs21\fsmilli10667 \dn6 T 
\fs32 \up0 (j) \uc0\u8722  y
\fs21\fsmilli10667 \dn6 T 
\fs32 \up0 (j))g (v
\fs21\fsmilli10667 \dn6 T 
\fs32 \up0 (j)) for j = 1, 2, . . . , L 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \uc0\u948 
\fs21\fsmilli10667 \dn8 T\up16 y 
\fs32 \up0 =(l
\fs21\fsmilli10667 \dn6 T 
\fs32 \up0 \uc0\u8722 y
\fs21\fsmilli10667 \dn6 T
\fs32 \up0 )\'95g
\f2\fs21\fsmilli10667 \up13 \uc0\u8242 
\f0\fs32 \up0 (v
\fs21\fsmilli10667 \dn6 T
\fs32 \up0 ) The error at the hidden layer is given as 
\fs24 \

\fs32 (4.18) 
\fs24 \

\fs32 (4.19) 
\fs24 \

\fs32 (4.20) 
\fs24 \

\fs32 The recursive component for other time frames, t = T \uc0\u8722 1,T \u8722 2,...,1, the error term is determined as 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page83image9159808.png \width463 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page83image9159808.png \width463 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \uc0\u948 y
\fs21\fsmilli10667 \dn6 T 
\fs32 \up0 (j) \uc0\u948 v
\fs21\fsmilli10667 \dn6 T 
\fs32 \up0 (j) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f1\fs32 \cf2 \up32 \uc0\u56319 \u56327 
\f0\fs21\fsmilli10667 \up18 L 
\fs32 \up0 \uc0\u8706 E\u8706 v(i)\u8706 h(j)
\f1 \up32 \uc0\u56319 \u56328 
\f0  
\fs21\fsmilli10667 \up18 L \dn3 h
\f1\fs32 \up13 \uc0\u56319 \u56335 
\f0\fs21\fsmilli10667 \up0 TT
\f1\fs32 \up13 \uc0\u56319 \u56335 
\f0\fs21\fsmilli10667 \up0 y
\f2 \dn3 \uc0\u8242 
\f0  
\fs24 \up0 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \uc0\u948 
\fs21\fsmilli10667 \dn8 T
\fs32 \up0 (j) = \uc0\u8722 \u8232 or \u948 
\fs21\fsmilli10667 \up10 h 
\fs32 \up0 = W
\fs21\fsmilli10667 \up10 T 
\fs32 \up0 \uc0\u948 
\fs21\fsmilli10667 \up16 y 
\fs32 \up0 \'95 f
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0\fs32 \up0 (u
\fs21\fsmilli10667 \dn6 T 
\fs32 \up0 ) where \'95 is element-wise multiplication. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page83image9160192.png \width454 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page83image9160384.png \width492 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page83image9160384.png \width492 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 or 
\fs24 \

\fs32 \uc0\u948 
\fs21\fsmilli10667 \dn8 t\up16 y
\fs32 \up0 (j) = (l
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 (j) \uc0\u8722  y
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 (j))g
\f2\fs21\fsmilli10667 \up13 \uc0\u8242 
\f0\fs32 \up0 (v
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 (j)) for j = 1, 2, . . . , L \uc0\u948 
\fs21\fsmilli10667 \dn8 t\up16 y 
\fs32 \up0 = (l
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 \uc0\u8722  y
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 ) \'95 g
\f2\fs21\fsmilli10667 \up13 \uc0\u8242 
\f0\fs32 \up0 (v
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 ) 
\fs24 \

\fs32 (4.21) 
\fs24 \

\fs32 (4.22) 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 \dn16 i=1 
\fs32 \up0 \uc0\u8706 v
\fs21\fsmilli10667 \dn6 T
\fs32 \up0 (i)\uc0\u8706 h
\fs21\fsmilli10667 \dn6 T
\fs32 \up0 (j)\uc0\u8706 u
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 (j) 
\fs21\fsmilli10667 T hyT 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 = \uc0\u948 
\fs21\fsmilli10667 \dn8 T
\fs32 \up0 (i)w
\fs21\fsmilli10667 \dn6 hy
\fs32 \up0 (i,j)f (u
\fs21\fsmilli10667 \dn6 T
\fs32 \up0 (j)) for j = 1,2,...,N 
\fs21\fsmilli10667 i=1 
\fs24 \

\fs32 Therefore the output units are 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f1\fs32 \cf2 \up32 \uc0\u56319 \u56329 
\f0  
\fs21\fsmilli10667 \up18 N 
\fs32 \up0 \uc0\u8706 E \u8706 u (i) 
\fs21\fsmilli10667 \up18 L 
\fs32 \up0 \uc0\u8706 E \u8706 v (i) 
\f1 \up32 \uc0\u56319 \u56330 
\f0  \up0 \uc0\u8706 h (j) 
\fs21\fsmilli10667 \dn3 h 
\f1\fs32 \up13 \uc0\u56319 \u56335 
\f0  
\fs21\fsmilli10667 \up0 t+1 
\f1\fs32 \up13 \uc0\u56319 \u56335 
\f0  
\fs21\fsmilli10667 \up0 t t 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \uc0\u948 
\fs21\fsmilli10667 \dn8 t 
\fs32 \up0 (j) = \uc0\u8722  
\f1 \up16 \uc0\u56319 \u56329 
\f0\fs21\fsmilli10667 \up0 NL
\f1\fs32 \up16 \uc0\u56319 \u56330 
\f0  
\fs24 \up0 \

\fs32 +\uc0\u8232 = 
\f1 \up29 \uc0\u56319 \u56335 
\f0 \up0 \uc0\u948 
\fs21\fsmilli10667 \up13 h 
\fs32 \up0 (i)w (i,j)+
\f1 \up29 \uc0\u56319 \u56335 
\f0 \up0 \uc0\u948 
\fs21\fsmilli10667 \up16 y
\fs32 \up0 (i)w (i,j) f
\f2\fs21\fsmilli10667 \up13 \uc0\u8242 
\f0\fs32 \up0 (u(j))forj=1,...,N 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page83image9160768.png \width589 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page83image9160768.png \width589 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page83image9161152.png \width450 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page83image9161152.png \width450 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 i=1\uc0\u8232 
\fs32 or \uc0\u948 
\fs21\fsmilli10667 \up13 h 
\fs32 \up0 = 
\f1 \up26 \uc0\u56319 \u56321 
\f0 \up0 W
\f1\fs21\fsmilli10667 \up13 \uc0\u8868 
\f0  
\fs32 \up0 \uc0\u948 
\fs21\fsmilli10667 \up13 h 
\fs24 \up0 \

\fs21\fsmilli10667 i=1\uc0\u8232 
\fs32 + W
\f1\fs21\fsmilli10667 \up13 \uc0\u8868 
\f0  
\fs32 \up0 \uc0\u948 
\fs21\fsmilli10667 \up16 y
\f1\fs32 \up26 \uc0\u56319 \u56322 
\f0  \up0 \'95 f
\f2\fs21\fsmilli10667 \up13 \uc0\u8242 
\f0\fs32 \up0 (u ) 
\fs24 \

\fs21\fsmilli10667 t hh t+1 
\fs24 \

\fs21\fsmilli10667 hy t \up2 t 
\fs24 \up0 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 \dn16 i=1 
\fs32 \up0 \uc0\u8706 u
\fs21\fsmilli10667 \dn6 t+1
\fs32 \up0 (i) \uc0\u8706 h
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 (j)\uc0\u8232 
\fs21\fsmilli10667 \dn3 t+1 \up0 hh t hy t 
\fs24 \

\fs21\fsmilli10667 \dn16 i=1 
\fs32 \up0 \uc0\u8706 v
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 (i) \uc0\u8706 h
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 (j) \uc0\u8706 u
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 (j) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Note that the error terms are propagated back from hidden layer at time frame t + 1 to the output at time frame t. 
\fs24 \

\fs32 (4.23) 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 4 I. J. Alamina 83 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Update of RNN Weights 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The weights are updated using the error terms determined in the previous section. For the output weight matrices, we have 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f1\fs32 \cf2 \up8 \uc0\u56319 \u56335 
\f0\fs21\fsmilli10667 \up18 T 
\fs32 \up0 \uc0\u8706 E \u8706 v(i) 
\f1 \up8 \uc0\u56319 \u56335 
\f0\fs21\fsmilli10667 \up18 T 
\fs32 \up0 w
\fs21\fsmilli10667 \up13 new
\fs32 \up0 (i,j)=w (i,j)\uc0\u8722 \u947  
\fs21\fsmilli10667 \up16 t 
\fs32 \up0 =w (i,j)\uc0\u8722 \u947  
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \uc0\u948 
\fs21\fsmilli10667 \up16 y
\fs32 \up0 (i)h(j) 
\fs21\fsmilli10667 t t 
\fs24 \

\fs32 \uc0\u948 
\fs21\fsmilli10667 \up13 h
\fs32 \up0 (i)x(j) (4.25) 
\fs21\fsmilli10667 \dn3 t \up2 t 
\fs24 \up0 \

\fs32 (4.26) 
\fs24 \

\fs32 (4.27) 
\fs24 \

\fs32 \uc0\u8706 v
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 (i)\uc0\u8706 w
\fs21\fsmilli10667 \dn6 hy
\fs32 \up0 (i,j) 
\fs21\fsmilli10667 \up16 hy 
\fs32 \up0 orW
\fs21\fsmilli10667 \up13 new
\fs32 \up0 =W +\uc0\u947 
\f1 \up29 \uc0\u56319 \u56335 
\f0 \up0 \uc0\u948 
\fs21\fsmilli10667 \up13 t
\fs32 \up0 h
\f1\fs21\fsmilli10667 \up13 \uc0\u8868 
\f0  
\fs24 \up0 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 hy \up2 hy 
\fs24 \up0 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page84image9125632.png \width687 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 t=1 
\fs24 \

\fs21\fsmilli10667 t=1 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 For the input weight matrices, we get 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 i=1 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 (4.24) 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 T\uc0\u8232 hy \up2 hy \up0 yt 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 w
\fs21\fsmilli10667 \up13 new
\fs32 \up0 (i,j)=w (i,j)\uc0\u8722 \u947  
\fs21\fsmilli10667 xh \up2 xh 
\fs24 \up0 \

\fs32 or 
\fs24 \

\fs32 \uc0\u8706  E\u8232 \u8706 u
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 (i)\uc0\u8706 w
\fs21\fsmilli10667 \dn6 xh
\fs32 \up0 (i,j) 
\fs21\fsmilli10667 \up16 xh 
\fs24 \up0 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f1\fs32 \cf2 \dn6 \uc0\u56319 \u56335 
\f0\fs21\fsmilli10667 \up5 T \up0 t=1 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \uc0\u8706  u 
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 ( i ) 
\f1 \up8 \uc0\u56319 \u56335 
\f0\fs21\fsmilli10667 \up18 T 
\fs32 \up0 =w (i,j)\uc0\u8722 \u947  
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page84image9175040.png \width426 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page84image9175232.png \width689 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 For the recurrent weight matrices we have 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 t=1 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \uc0\u8706 u
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 (i) 
\fs21\fsmilli10667 \up16 hh 
\fs32 \up0 \uc0\u8706 u
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 (i) \uc0\u8706 w
\fs21\fsmilli10667 \dn6 hh
\fs32 \up0 (i, j) 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 t=1 T 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 = w
\fs21\fsmilli10667 \dn6 hh
\fs32 \up0 (i, j) \uc0\u8722  \u947  
\f1 \up29 \uc0\u56319 \u56335 
\f0  \up0 \uc0\u948 
\fs21\fsmilli10667 \dn8 h\up13 t 
\fs32 \up0 (i)h
\fs21\fsmilli10667 \dn6 t\uc0\u8722 1
\fs32 \up0 (j) 
\fs21\fsmilli10667 t=1 
\fs24 \

\fs32 W
\fs21\fsmilli10667 \up13 new 
\fs32 \up0 =W +\uc0\u947 
\f1 \up29 \uc0\u56319 \u56335 
\f0 \up0 \uc0\u948 
\fs21\fsmilli10667 \up13 t
\fs32 \up0 x
\f1\fs21\fsmilli10667 \up13 \uc0\u8868 
\f0  \up0 xh \up2 xh \up0 ht 
\fs24 \

\fs32 w
\fs21\fsmilli10667 \up13 new
\fs32 \up0 (i, j) = w (i, j) \uc0\u8722  \u947  
\f1 \up29 \uc0\u56319 \u56335 
\f0\fs21\fsmilli10667 \up40 T 
\fs32 \up21 \uc0\u8706 E 
\fs21\fsmilli10667 \up0 hh 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 T 
\fs24 \

\fs21\fsmilli10667 t=1 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page84image9175040.png \width426 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page84image9161536.png \width689 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 T 
\fs32 or=W
\fs21\fsmilli10667 \up13 new
\fs32 \up0 =W +\uc0\u947 
\f1 \up29 \uc0\u56319 \u56335 
\f0 \up0 \uc0\u948 
\fs21\fsmilli10667 \up13 t
\fs32 \up0 h
\f1\fs21\fsmilli10667 \up13 \uc0\u8868 
\f0  
\fs24 \up0 \

\fs21\fsmilli10667 hh \up2 hh \up0 ht\uc0\u8722 1 t=1 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 84 
\fs24 \

\fs32 Chapter 4 
\fs24 \

\fs32 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 In the BPTT algorithm the sub gradients are summed over all time frames. The algorithm is summarised below: 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Data: \{x
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 ,I
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 \}1\uc0\u8804 t\u8804 T 
\fs24 \

\fs32 Result: Optimal weights 
\fs24 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl260\sa240\partightenfactor0
\ls19\ilvl0
\fs21\fsmilli10667 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
1 \'a0
\fs32 //x
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 is the input feature sequence //I
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 is the label sequence; 
\fs24 \uc0\u8232 \
\ls19\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
2 \'a0
\fs32 initialise weights randomly; 
\fs24 \uc0\u8232 \
\ls19\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
3 \'a0
\fs32 for error is significant or epochs less than maximum do 
\fs24 \uc0\u8232 \
\ls19\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
4 \'a0
\fs32 for t 
\f2 \uc0\u8592 
\f0  1; t <= T ; t 
\f2 \uc0\u8592 
\f0  t + 1 do 
\fs24 \uc0\u8232 \
\ls19\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
5 \'a0
\fs32 //forward computation ; 
\fs24 \uc0\u8232 \
\ls19\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
6 \'a0
\fs32 u
\fs21\fsmilli10667 \dn6 t 
\f2\fs32 \up0 \uc0\u8592 
\f0  W
\fs21\fsmilli10667 \dn6 xh 
\fs32 \up0 + W
\fs21\fsmilli10667 \dn6 hh
\fs32 \up0 h
\fs21\fsmilli10667 \dn6 t\uc0\u8722 1
\fs32 \up0 ; 
\fs24 \uc0\u8232 \
\ls19\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
7 \'a0
\fs32 h
\fs21\fsmilli10667 \dn6 t 
\f2\fs32 \up0 \uc0\u8592 
\f0  f(u
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 ); 
\fs24 \uc0\u8232 \
\ls19\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
8 \'a0
\fs32 v
\fs21\fsmilli10667 \dn6 t 
\f2\fs32 \up0 \uc0\u8592 
\f0  W
\fs21\fsmilli10667 \dn6 hy
\fs32 \up0 h
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 ; 
\fs24 \uc0\u8232 \
\ls19\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
9 \'a0
\fs32 y
\fs21\fsmilli10667 \dn6 t 
\f2\fs32 \up0 \uc0\u8592 
\f0  g(v
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 ) 
\fs24 \uc0\u8232 \
\ls19\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
10 \'a0
\fs32 end 
\fs24 \uc0\u8232 \
\ls19\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
11 \'a0
\fs32 begin 
\fs24 \uc0\u8232 \
\ls19\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
12 \'a0
\fs32 //backprop through time ; 
\fs24 \uc0\u8232 \
\ls19\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
13 \'a0
\fs32 \uc0\u948 
\fs21\fsmilli10667 \dn8 T\up16 y 
\fs32 \up0 =(l
\fs21\fsmilli10667 \dn6 T 
\fs32 \up0 \uc0\u8722 y
\fs21\fsmilli10667 \dn6 T
\fs32 \up0 )\'95g
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0\fs32 \up0 (v
\fs21\fsmilli10667 \dn6 T
\fs32 \up0 ); 
\fs24 \uc0\u8232 \
\ls19\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
14 \'a0
\fs32 \uc0\u948 
\fs21\fsmilli10667 \up10 h 
\fs32 \up0 = W
\f1\fs21\fsmilli10667 \up10 \uc0\u8868 
\f0  
\fs32 \up0 \uc0\u948 
\fs21\fsmilli10667 \up10 h 
\fs32 \up0 + W
\f1\fs21\fsmilli10667 \up10 \uc0\u8868 
\f0  
\fs32 \up0 \uc0\u948 
\fs21\fsmilli10667 \up16 y 
\fs32 \up0 \'95 f
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0\fs32 \up0 (u ); 
\fs24 \uc0\u8232 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 T hh t+1 hy T \up2 T 
\fs24 \up0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl260\sa240\partightenfactor0
\ls20\ilvl0
\fs21\fsmilli10667 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
15 \'a0
\fs32 for t 
\f2 \uc0\u8592 
\f0  T \uc0\u8722  t; t >= 1; t 
\f2 \uc0\u8592 
\f0  t \uc0\u8722  1 do 
\fs24 \uc0\u8232 \
\ls20\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
16 \'a0
\fs32 \uc0\u948 
\fs21\fsmilli10667 \dn8 t\up16 y 
\fs32 \up0 = (l
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 \uc0\u8722  y
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 ) \'95 g
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0\fs32 \up0 (v
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 ); 
\fs24 \uc0\u8232 \
\ls20\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
17 \'a0
\fs32 \uc0\u948 
\fs21\fsmilli10667 \up10 h 
\fs32 \up0 = 
\f1 \up26 \uc0\u56319 \u56321 
\f0 \up0 W
\f1\fs21\fsmilli10667 \up10 \uc0\u8868 
\f0  
\fs32 \up0 \uc0\u948 
\fs21\fsmilli10667 \up10 h 
\fs32 \up0 + W
\f1\fs21\fsmilli10667 \up10 \uc0\u8868 
\f0  
\fs32 \up0 \uc0\u948 
\fs21\fsmilli10667 \up16 y
\f1\fs32 \up26 \uc0\u56319 \u56322 
\f0  \up0 \'95 f
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0\fs32 \up0 (u ); 
\fs24 \uc0\u8232 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 t hh t+1 hy t \up2 t 
\fs24 \up0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl260\sa240\partightenfactor0
\ls21\ilvl0
\fs21\fsmilli10667 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
18 \'a0
\fs32 end 
\fs24 \uc0\u8232 \
\ls21\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
19 \'a0
\fs32 end 
\fs24 \uc0\u8232 \
\ls21\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
20 \'a0
\fs32 update weights and biases according to gradient descent; 
\fs24 \uc0\u8232 \
\ls21\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
21 \'a0
\fs32 begin 
\fs24 \uc0\u8232 \
\ls21\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
22 \'a0
\fs32 W
\fs21\fsmilli10667 \up10 new 
\fs32 \up0 = W
\fs21\fsmilli10667 \dn6 hy 
\fs32 \up0 + \uc0\u947  
\f1 \up24 \uc0\u56319 \u56333 
\f0\fs21\fsmilli10667 \up16 T 
\fs32 \up0 \uc0\u948 
\fs21\fsmilli10667 \dn8 y\up10 t 
\fs32 \up0 h
\f1\fs21\fsmilli10667 \up10 \uc0\u8868 
\f0 \dn8 t 
\fs32 \up0 ; 
\fs24 \uc0\u8232 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 hy t=1 
\fs24 \

\fs21\fsmilli10667 23 
\fs32 W
\fs21\fsmilli10667 \up10 new
\fs32 \up0 =W
\fs21\fsmilli10667 \dn6 hh
\fs32 \up0 +\uc0\u947 
\f1 \up24 \uc0\u56319 \u56333 
\f0\fs21\fsmilli10667 \up16 T 
\fs32 \up0 \uc0\u948 
\fs21\fsmilli10667 \up10 t
\fs32 \up0 h
\f1\fs21\fsmilli10667 \up10 \uc0\u8868 
\f0 \dn8 t\uc0\u8722 1
\fs32 \up0 ; 
\fs21\fsmilli10667 hh t=1 h 
\fs24 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl260\sa240\partightenfactor0
\ls22\ilvl0
\fs21\fsmilli10667 \cf2 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
24 \'a0
\fs32 end 
\fs24 \uc0\u8232 \
\ls22\ilvl0
\fs21\fsmilli10667 \kerning1\expnd0\expndtw0 {\listtext	.	}\expnd0\expndtw0\kerning0
25 \'a0
\fs32 end 
\fs24 \uc0\u8232 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page85image9127808.png \width12 \height5800 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page85image9128000.png \width12 \height1373 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page85image9128192.png \width12 \height1937 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page85image9128384.png \width12 \height553 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page85image9128384.png \width12 \height553 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Algorithm 2: RNN training algorithm\uc0\u8232 Chapter 4 I. J. Alamina 85 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 4.2.4 LSTMs and GRUs 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 A special implementation of the RNN called the Long Short Term Memory (LSTM) has been designed to capture patterns over particularly long sequences of data and thus is an ideal candidate for generating character sequences while preserving syn- tactic language rules learned from the training data. 
\fs24 \

\fs32 The internal structure and working of the LSTM cell is documented by its cre- ators in Sak et al. (2014). The ability to recall information over extended sequences results from the internal gated structure which performs a series of element wise multiplications on the inputs and internal state of the LSTM cell at each time step. In addition to the output neurons which in this text we refer to as the write gate and denote as the current cell state, c
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 , three additional gates (comprising a neural network sub-layer) located within the LSTM cell are the input gate, the forget gate and the output gate. Together with the initial current state cell, these gates along with the current-state cell itself enable the LSTM cell architecture to store infor- mation, forward information, delete information and receive information. Generally however, the LSTM cell looks like a regular feed-forward network having a set of neurons capped with a nonlinear function. The recurrent nature of the network arises, however due to the fact that the internal state of the RNN cell is rerouted back as an input to the RNN cell or input to the next cell in the time-series giv- ing rise to sequence memory within the LSTM architecture. Mathematically, these gates are formulated as follows: 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 i
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 = \uc0\u963 (W
\fs21\fsmilli10667 \up13 (xi)
\fs32 \up0 x
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 + W
\fs21\fsmilli10667 \up13 (hi)
\fs32 \up0 h
\fs21\fsmilli10667 \dn6 t\uc0\u8722 1 
\fs32 \up0 + W
\fs21\fsmilli10667 \up13 (ci)
\fs32 \up0 c
\fs21\fsmilli10667 \dn6 t\uc0\u8722 1 
\fs32 \up0 + b
\fs21\fsmilli10667 \up13 (i)
\fs32 \up0 )\uc0\u8232 f
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 = \uc0\u963 (W
\fs21\fsmilli10667 \up13 (xf)
\fs32 \up0 x
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 + W
\fs21\fsmilli10667 \up13 (hf)
\fs32 \up0 h
\fs21\fsmilli10667 \dn6 t\uc0\u8722 1 
\fs32 \up0 + W
\fs21\fsmilli10667 \up13 (cf)
\fs32 \up0 c
\fs21\fsmilli10667 \dn6 t\uc0\u8722 1 
\fs32 \up0 + b
\fs21\fsmilli10667 \up13 (f)
\fs32 \up0 ) c
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 = f
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 \'95 c
\fs21\fsmilli10667 \dn6 t\uc0\u8722 1 
\fs32 \up0 + i
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 \'95 tanh(W
\fs21\fsmilli10667 \up13 (xc)
\fs32 \up0 x
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 + W
\fs21\fsmilli10667 \up13 (hc)
\fs32 \up0 h
\fs21\fsmilli10667 \dn6 t\uc0\u8722 1 
\fs32 \up0 + b
\fs21\fsmilli10667 \up13 (c)
\fs32 \up0 ) 
\fs24 \

\fs32 o
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 = \uc0\u963 (W
\fs21\fsmilli10667 \up13 (xo)
\fs32 \up0 x
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 + W
\fs21\fsmilli10667 \up13 (ho)
\fs32 \up0 h
\fs21\fsmilli10667 \dn6 t\uc0\u8722 1 
\fs32 \up0 + W
\fs21\fsmilli10667 \up13 (co)
\fs32 \up0 c
\fs21\fsmilli10667 \dn6 t\uc0\u8722 1 
\fs32 \up0 + b
\fs21\fsmilli10667 \up13 (o)
\fs32 \up0 ) 86 Chapter 4 
\fs24 \

\fs32 (4.28) 
\fs24 \

\fs32 (4.29) 
\fs24 \

\fs32 (4.30) 
\fs24 \

\fs32 (4.31) I. J. Alamina 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 4.3: An LSTM Cell (Graves et al., 2013) 
\fs24 \

\fs32 h
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 =o
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 \'95tanh(c
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 ) (4.32) 
\fs24 \

\fs32 The gates in the above formula are illustrated in Figure 4.3. i
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 represents the input gate, f
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 is the forget gate and o
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 represents the output gate. At each of these gates therefore, the inputs consisting of hidden states in addition to the regular inputs are multiplied by a set of weights and passed through a soft-max function. These weights during training learn whether the gate will, during inference, open or not. In summary, the input gate tells the LSTM whether or not to receive new information, the forget gate determines whether the current information it already has from the previous step should be kept or dropped and the output gate determines what should be forwarded to the next LSTM cell. Note also that the LSTM has two sigmoid (tanh) activation functions utilised at the input and output of the current cell c
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 . 
\fs24 \

\fs32 One particular variant of the original LSTM model is the GRU cell. Though simpler than an LSTM cell the GRU cell performs equally efficiently. The GRU cell is a subset implementation of the LSTM cell. Rather than using the output gate of the LSTM, this gate is omitted in the GRU and the output result of the other internal gates are always forwarded. The second simplification is a merge of the internal gate state vectors into a single vector h
\fs21\fsmilli10667 \dn6 (t)
\fs32 \up0 . This merged gate here referred to as z(t), controls both the forget gate an the input gate and acts as follows. Whenever a value is retained by the cell the previous value is erased first. That is, if the gate controller outputs a 1, in the LSTM this corresponds to the input gate is open and the forget gate is closed. Therefore if z(t) it outputs a 0, the reverse 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page87image6973184.png \width3972 \height2977 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 4 I. J. Alamina 87 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 happens for the input gate and the forget gate in the LSTM. There is, however, a new gate controller, r(t), which determines which portion of the previous state will be shown at the output (Cho et al., 2014). 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The architecture of a GRU is formulated as follows:\uc0\u8232 z =\u963 (W
\fs21\fsmilli10667 \up13 T 
\fs32 \up0 \'b7x +W
\fs21\fsmilli10667 \up13 T 
\fs32 \up0 \'b7x ) 
\fs24 \

\fs32 (4.33) 
\fs24 \

\fs32 (4.34) 
\fs24 \

\fs32 (4.35) 
\fs24 \

\fs32 (4.36) 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 (t) \dn3 xz \up0 (t) \dn3 hz \up0 (t\uc0\u8722 1) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 r =\uc0\u963 (W
\fs21\fsmilli10667 \up13 T 
\fs32 \up0 \'b7x +W
\fs21\fsmilli10667 \up13 T 
\fs32 \up0 \'b7x ) 
\fs21\fsmilli10667 (t) \dn3 xr \up0 (t) \dn3 hr \up0 (t\uc0\u8722 1) 
\fs24 \

\fs32 g =tanh(W
\fs21\fsmilli10667 \up13 T 
\fs32 \up0 \'b7x +W
\fs21\fsmilli10667 \up13 T 
\fs32 \up0 \'b7(r 
\f2 \uc0\u8855 
\f0 h )) 
\fs21\fsmilli10667 (t) \dn3 xg \up0 (t) \dn3 hg \up0 (t) (t\uc0\u8722 1) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \up5 h
\fs21\fsmilli10667 \up0 (t) 
\fs32 \up5 = (1 \uc0\u8722  z
\fs21\fsmilli10667 \up0 (t)
\fs32 \up5 ) 
\f2 \uc0\u8855 
\f0  (h
\fs21\fsmilli10667 \up0 (t\uc0\u8722 1)
\fs32 \up5 ) + z
\fs21\fsmilli10667 \up0 (t) 
\f2\fs32 \up5 \uc0\u8855 
\f0  g
\fs21\fsmilli10667 \up0 t 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Due to the light-weight nature of the GRU cell, it is common practice to use GRU cells in place of LSTM cells. This precedence achieves the much desired lighter com- putation load on the actual hardware performing the RNN training. As each of the gates required in an LSTM cell comprises high density matrix multiplication opera- tions in themselves, the condensation of two gates into one and the omission of the output gate within GRU cells pushes towards halving the architectural complexity and coupled with the equally efficient performance of the GRU when compared to the LSTM cell ultimately serves as an overall improvement on the LSTM architec- ture. For these reasons, GRUs have highly appealing features when compared to LSTMs and was the RNN cell of choice used for the study in this report. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 4.3 Deep speech architecture 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 This work makes use of an enhanced RNN architecture called the Bi-directional Re- current Neural Network (BiRNN). While Hannun et al. (2014b) assert that forward recurrent connections does reflect the sequential relationships of an audio waveform, perhaps the BiRNN model achieves a more robust sequence model. 
\fs24 \

\fs32 The BiRNN is a preferred end to end mechanism due to the length of sequence 88 Chapter 4 I. J. Alamina 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 over which temporal relationships can be captured. This implies that BiRNNs will be suited for capturing temporal relationships over much longer sequences than a forward only RNN, because hidden state information is preserved in both forwards and backwards direction. 
\fs24 \

\fs32 In addition, such a model has a notion of complete sentence or utterance inte- gration, having information over the entire temporal extent of the input features when making each prediction. 
\fs24 \

\fs32 The formulation of the BiRNN is derived by starting off with the basic RNN 
\fs24 \

\fs32 architecture which is referred to as the forward architecture. From the forward ar- 
\fs24 \

\fs32 chitecture we derive the backward architecture. If we choose a temporally recurrent 
\fs24 \

\fs32 layer j, the BiRNN forward and backward intermediate hidden representation h
\fs21\fsmilli10667 \up16 (f) \up0 t 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 and h
\fs21\fsmilli10667 \up16 (b) 
\fs32 \up0 is given as. 
\fs21\fsmilli10667 t 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \dn16 h
\fs21\fsmilli10667 \up0 (f) 
\fs32 \dn16 = \uc0\u963 (W
\fs21\fsmilli10667 \dn3 (j)T 
\fs32 \dn16 h
\fs21\fsmilli10667 \up0 (i\uc0\u8722 1) 
\fs32 \dn16 + W
\fs21\fsmilli10667 \up0 (f)T 
\fs32 \dn16 h
\fs21\fsmilli10667 \up0 (j) t tkt\uc0\u8722 1 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 backward in time respectively.\uc0\u8232 Hannun et al. (2014b) points out that the recurrent forward and backward com- 
\fs24 \

\fs32 ponents are evaluated entirely independent of each other and for optimal training, a modified non linearity function \uc0\u963 (z) = min(max(z, 0), 20) is recommended. 
\fs24 \

\fs32 The final BiRNN representation h
\fs21\fsmilli10667 \up16 (j) 
\fs32 \up0 for the layer is now the superposition of the 
\fs21\fsmilli10667 t 
\fs24 \

\fs32 two RNN components, 
\fs24 \

\fs32 h
\fs21\fsmilli10667 \up16 (j) 
\fs32 \up0 =h
\fs21\fsmilli10667 \up16 (f) 
\fs32 \up0 +h
\fs21\fsmilli10667 \up16 (b) 
\fs32 \up0 (4.39) 
\fs21\fsmilli10667 ttt 
\fs24 \

\fs32 Also note that back propagation through time (BPTT) sub gradient evaluations are computed from the combined BiRNN structure directly during training. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 4.3.1 Connectionist Temporal Classification (CTC) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The term CTC stands for Connectionist Temporal classification. This algorithm was designed to solve the problem of fuzzy alignment between the source input 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \dn16 h
\fs21\fsmilli10667 \up0 (b) 
\fs32 \dn16 = \uc0\u963 (W
\fs21\fsmilli10667 \dn3 (j)T 
\fs32 \dn16 h
\fs21\fsmilli10667 \up0 (i\uc0\u8722 1) 
\fs32 \dn16 + W
\fs21\fsmilli10667 \up0 (b)T 
\fs32 \dn16 h
\fs21\fsmilli10667 \up0 (b) t tkt+1 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 + b
\fs21\fsmilli10667 \up13 (j)
\fs32 \up0 ))\uc0\u8232 Temporal weight matrices W
\fs21\fsmilli10667 \up10 (f) 
\fs32 \up0 and W
\fs21\fsmilli10667 \up10 (b) 
\fs32 \up0 propagate h
\fs21\fsmilli10667 \up16 (f) 
\fs32 \up0 and h
\fs21\fsmilli10667 \up16 (b) 
\fs32 \up0 forward and 
\fs24 \

\fs32 + b
\fs21\fsmilli10667 \up13 (j)
\fs32 \up0 )) 
\fs24 \

\fs32 (4.37) 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 tt 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 (4.38) 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 4 I. J. Alamina 89 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 data and the output classification desired from the Machine Learning system. This type of fuzzy alignment is observed in speech recognition systems since the same speech in either the same individual or different individuals will have different signal forms. This is a many to one relationship between the input signal and the output classification that is also dependent on the style of speaking at the moment when the utterance is said. Unlike hybrid DNN-HMM networks the CTC algorithm deploys an end-to-end framework that models all aspects of the input sequence in a single neural network, therefore discarding the need for an HMM interpretation of the input sequence. In addition, the CTC method does not require pre-segmented training data at the same time output classification is made independent of post-processing. 
\fs24 \

\fs32 CTC works by making predictions at any point in the input sequence. For the case of speech modelling, CTC makes a character prediction for every time step of the raw audio input speech signal. Although this initially seems counter intuitive, this method models the many to one relationship seen in the fuzzy audio speech to text alignment. 
\fs24 \

\fs32 For hybrid DNN-HMM systems, speech or more accurately, acoustic models, re- quire separate training of targets for every time-slice in the input sequence. Secondly, and as a consequence of this, it becomes necessary to segment the audio sequence, in order to provide targets for every time-slice. A third consequence is the limitation of DNNs previously discussed. As the DNN network only outputs local classifications, global aspects such as the likelihood of two consecutive labels appearing together cannot be directly modelled. Without an external model, usually in the form of a language model, the hybrid speech model will significantly degrade performance. 
\fs24 \

\fs32 In the CTC case, so long as the overall sequence of labels is correct the network can be optimised to correct the temporal or fuzzy alignments. Since this many to one fuzzy alignment is simultaneously modelled in CTC, then there is no need for pre- segmented data. At the same time, CTC computes probabilities of complete label sequences, hence external post-processing required by hybrid models is eliminated. 
\fs24 \

\fs32 Similar to the HMM sequence model, the CTC algorithm is a sequence model that predicts the next label in a sequence as a cumulative of previous sequences. This section develops the CTC loss function borrowing concepts used in HMM models such as the forward backward algorithm as outlined in (Graves et al., 2006). In 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 90 Chapter 4 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 the following paragraph we introduce terminology associated with the CTC loss function. 
\fs24 \

\fs32 Given two symbols A and B such that A has a many to one relationship with B, signifying the temporal nature of the classification. The symbol A represents an alphabet from which a sequence of the output classifications are drawn from. This CTC output consists of a soft-max layer in a BiRNN (bidirectional recurrent neural network). 
\fs24 \

\fs32 This output models the probability distribution of a complete sequence of arbi- trary length |A| over all possible labels in A from activations within |A|. An extra activation is given to represent the probability of outputting a blank, or no label. At each time-step leading up to the final step, the probability distribution estimated as distribution over all possible label sequences of length leading up to that of the input sequence. 
\fs24 \

\fs32 It is now possible to define the extended alphabet A
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0  
\fs32 \up0 = A 
\f2 \uc0\u8746 
\f0  \{blank\}, also, y
\fs21\fsmilli10667 \dn6 t,p 
\fs32 \up0 as 
\fs24 \

\fs32 the activation of network output p at time t. Therefore y
\fs21\fsmilli10667 \dn6 t,p 
\fs32 \up0 is the probability that 
\fs24 \

\fs32 the network will output element p 
\f2 \uc0\u8712 
\f0  A
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0  
\fs32 \up0 at time t given that x is the input sequence 
\fs24 \

\fs32 of length T . The distribution sought after P r(\uc0\u960 |x), is the conditionally-independent 
\fs24 \

\fs32 distribution over the subset A
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0 T 
\fs32 \up0 where A
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0 T 
\fs32 \up0 denotes the set of length T sequences in 
\f2\fs21\fsmilli10667 \uc0\u8242 
\f0  
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 T\uc0\u8232 
\fs32 Pr(\uc0\u960  | x) = 
\f1 \up29 \uc0\u56319 \u56336 
\f0  \up0 y
\fs21\fsmilli10667 \dn6 t,\uc0\u960 
\fs16 \dn8 t 
\fs32 \up0 (4.40) 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 A. 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 \up2 T \up0 \uc0\u8804 T 
\f2 \uc0\u8242 
\f0 T\uc0\u8232 
\fs32 B : A\'91 
\f2 \uc0\u8594 
\f0  A . This is the mapping of a set A , which indicates the paths, onto 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 another set A
\fs21\fsmilli10667 \up10 \uc0\u8804 T 
\fs32 \up0 , that of possible labels in x. B then becomes a sequence of symbols 
\fs24 \

\fs32 with length less than or equal to T over A. Note that B is a set containing sequential 
\fs24 \

\fs32 symbols belonging to the set A and not. A
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0  
\fs32 \up0 because there is no blank symbol in B. 
\fs24 \

\fs32 This is achieved when first take out all repeated labels and then take out all the 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 t=1\uc0\u8232 
\fs32 From Equation (4.40), it is now possible to define the many-to-one mapping 
\fs24 \

\f2\fs21\fsmilli10667 \uc0\u8242 
\f0 T\uc0\u8232 
\fs32 blanks from the sequence A . For instance, 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 B(a \uc0\u8722  ab\u8722 ) = aab B(\u8722 aa \u8722  \u8722 abb) = aab. 
\fs24 \

\fs32 (4.41) 
\fs24 \

\fs32 The mapping obtained by B is equivalent to when the output switches from not predicting a new symbol to predicting a symbol or from predicting one symbol to 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 4 I. J. Alamina 91 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 another symbol assuming this was also possible. Intuitively, the probability of B which is the labelling of l 
\f2 \uc0\u8712 
\f0  A
\fs21\fsmilli10667 \up10 \uc0\u8804 T 
\fs32 \up0 being a many to one of A\'91
\fs21\fsmilli10667 \up10 T 
\fs32 \up0 is determined by summing over all the paths in A
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0 T 
\fs32 \up0 mapped onto it by B. Thus: 
\fs24 \

\fs32 Pr(l|x) = 
\f1 \up29 \uc0\u56319 \u56335 
\f0  \up0 Pr(\uc0\u960 |x) (4.42) 
\fs21\fsmilli10667 \uc0\u960 
\f2 \uc0\u8712 
\f0 B
\fs16 \up5 \uc0\u8722 1
\fs21\fsmilli10667 \up0 (l) 
\fs24 \

\fs32 This mapping makes CTC robust to unsegmented data as it predicts all the la- bels where they occur and later the \'91collapsed\'92 sequence will be extended over the approximate period where the previous extended sequence occurred thus aligning labels to input sequences on-the-fly without knowing in advance where label to input sequence alignments occur. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 4.3.2 Forward-backward algorithm 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The forward-backward algorithm is used to estimate the probability of a point in the sequence as the product of all point leading up to that point from the initial state, the forward variable (\uc0\u945 ), multiplied by the probability of all the points from that state to the end of the sequence, the backward variable (\u946 ). 
\fs24 \

\fs32 The difference between this estimation and that determined from equation (4.42) is the fact that the forward-backward algorithm converts equation (4.42) into a form that is both recursive as well as reduces the computational complexity from an otherwise intractable computation to one that is readily computable. 
\fs24 \

\fs32 With CTC, consider a modified \'94label sequence\'94 l
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0\fs32 \up0 , that caters for blank char- acters in between regular ones l, as defined in A. Thus, if U is defined as the length of l. Then U
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0  
\fs32 \up0 is of length 2U + 1. CTC therefore integrates probability distributions of transitions between blank and non-blank labels at the same time CTC calculates those transition occurring between pairs of distinct non-blank la- bels. The forward variable, \uc0\u945 (t,u) now becomes the summed probability of all length t paths that are mapped by B onto the length 
\f4 \uc0\u8970 
\f0 u/2
\f4 \uc0\u8971 
\f0  prefix of l. (Note, 
\f4 \uc0\u8970 
\f0 u/2
\f4 \uc0\u8971 
\f0  is the floor of u/2, the greatest integer less than or equal to u/2.) For some sequences,lets
\fs21\fsmilli10667 \dn6 p:q 
\fs32 \up0 denotethesub-sequences
\fs21\fsmilli10667 \dn6 p
\fs32 \up0 ,s
\fs21\fsmilli10667 \dn6 p+1
\fs32 \up0 ,...,s
\fs21\fsmilli10667 \dn6 q\uc0\u8722 1
\fs32 \up0 ,s
\fs21\fsmilli10667 \dn6 q
\fs32 \up0 ,anddefinetheset 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 92 Chapter 4 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 V(t,u)
\f2 \uc0\u8801 
\f0 \{\uc0\u960 
\f2 \uc0\u8712 
\f0 A
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0 t 
\fs32 \up0 :B(\uc0\u960 )=l
\fs21\fsmilli10667 \dn6 1:
\f4 \uc0\u8970 
\f0 u/2
\f4 \uc0\u8971 
\f0  
\fs32 \up0 and\uc0\u960 
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 =l
\fs21\fsmilli10667 \dn8 u
\f2 \up10 \uc0\u8242 
\f0\fs32 \up0 \}. \uc0\u945 (t,u)thenbecomes 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 t\uc0\u8232 
\fs32 \uc0\u945 (t, u) 
\f2 \uc0\u8801 
\f0  
\f1 \up29 \uc0\u56319 \u56335 
\f0  
\f1 \uc0\u56319 \u56336 
\f0  \up0 y
\fs21\fsmilli10667 \dn6 i,\uc0\u960 
\fs16 \dn8 i 
\fs32 \up0 (4.43) 
\fs24 \

\fs21\fsmilli10667 \uc0\u960 
\f2 \uc0\u8712 
\f0 V (t,u) \up2 i=1 
\fs24 \up0 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The forward variables at time t is calculated recursively from the preceding values at time t \uc0\u8722  1 and expressed as the sum of the forward variables with and without the final blank at time T. 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\f2\fs21\fsmilli10667 \cf2 \uc0\u8242 \u8242 
\f0  
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \uc0\u945 (1, 1) = y
\fs21\fsmilli10667 \dn6 1,b\up0 \uc0\u8232 
\fs32 \uc0\u945 (1, 2) = y
\fs21\fsmilli10667 \dn6 1,l
\fs16 \dn8 1\up0 \uc0\u8232 
\fs32 \uc0\u945 (1, u) = 0, 
\f2 \uc0\u8704 
\f0 u > 2 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 where 
\fs24 \

\f2\fs32 \uc0\u63729 
\f0 \uc0\u8232 
\f2 \up18 \uc0\u63732 \up8 \uc0\u63730 
\f0 \up0 u \uc0\u8722  1, 
\fs24 \

\fs32 f(u) = 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 i=f (u) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 if l
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0  
\fs32 \up0 = blank or l
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0  
\fs32 \up0 = l
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0  
\fs24 \up0 \

\fs32 Pr(l|x)=\uc0\u945 (T,U)+\u945 (T,U \u8722 1) (4.44) For the initial conditions, correct paths begin with a blank symbol (b) and the 
\fs24 \

\fs32 first symbol l (l
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 ): 
\fs24 \

\fs32 The forward variable then takes the following recursive form: 
\fs24 \

\fs32 (4.45) 
\fs24 \

\fs32 (4.46) 
\fs24 \

\fs32 (4.47) 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 u 
\fs32 \uc0\u945 (t,u)=y
\fs21\fsmilli10667 \dn6 t,l
\fs16 \dn11 u
\f2 \up0 \uc0\u8242 
\f0  
\f1\fs32 \up29 \uc0\u56319 \u56335 
\f0  \up0 \uc0\u945 (t\u8722 1,i) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f2\fs32 \cf2 \up18 \uc0\u63732 \up10 \uc0\u63731 
\f0 \up0 u \uc0\u8722  2, otherwise 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 u 
\fs24 \

\fs21\fsmilli10667 u\uc0\u8722 2 u 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Figure 4.4 expresses the recurrence relation for \uc0\u945 (t,u). While t is expressed 
\fs24 \

\fs32 on the x axis, u is illustrated on the y axis. The CTC algorithm assumes that 
\fs24 \

\fs32 outputs of the network potentially alternate between blank symbols indicated as 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\f2\fs21\fsmilli10667 \cf2 \uc0\u8242 
\f0  
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 black circles and non-blank elements, the white circles, all in l . The sequential graph constructed from this 2-dimensional matrix show computational dependencies between sequential pairs of the recurrence relation for \uc0\u945 (t, u). Therefore, the value \u945 (2, 3), formed from \u945 (1, 2), corresponds to the blank symbol at t = 2 and u = 3, is . Also, \u945 (2,2), equivalent to the symbol c at t = 2 and u = 2, is gotten from \u945 (1,2) and \u945 (1,1). Note that there are not enough time steps when u < U
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0\fs32 \up0 2(Tt)1, therefore, \uc0\u945 (t, u) = 0. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 4 I. J. Alamina 93 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 4.4: Beam Search Lattice Structure (Graves et al., 2006) 
\fs24 \

\fs32 Also note the boundary condition;\uc0\u8232 \u945 (t, 0) = 0 
\f2 \uc0\u8704 
\f0 t (4.48) 
\fs24 \

\fs32 The backward variable \uc0\u946 (t, u) ais built similarly to the forward variable. Rather than moving from the start of the sequence to we define the path starting at t + 1 that completes the sequence at T when appended to any path \u960 \'88 that generates \u945 (t, u). Then,assumingW(t,u)
\f2 \uc0\u8801 
\f0 \{\uc0\u960 
\f2 \uc0\u8712 
\f0 A
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0 T\uc0\u8722 t 
\fs32 \up0 :B(\uc0\u960 \'88+\u960 )=l
\f2 \uc0\u8704 
\f0 \uc0\u960 \'88
\f2 \uc0\u8712 
\f0 V(t,u)\},therefore 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page94image6945824.png \width5106 \height2437 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 T\uc0\u8722 t\u8232 
\fs32 \uc0\u946 (t, u) 
\f2 \uc0\u8801 
\f0  
\f1 \up29 \uc0\u56319 \u56335 
\f0  
\f1 \uc0\u56319 \u56336 
\f0  \up0 y
\fs21\fsmilli10667 \dn6 t+i,\uc0\u960 
\fs16 \dn8 i 
\fs24 \up0 \

\fs21\fsmilli10667 \uc0\u960 
\f2 \uc0\u8712 
\f0 W(t,u) \up2 i=1 
\fs24 \up0 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The backward variable is therefore equivalently initialised as thus 
\fs24 \

\fs32 \uc0\u946 (T,U
\f2\fs21\fsmilli10667 \up13 \uc0\u8242 
\f0\fs32 \up0 ) = 1 
\f2\fs21\fsmilli10667 \uc0\u8242 
\f0  
\fs24 \

\fs32 (4.49) 
\fs24 \

\fs32 (4.50) 
\fs24 \

\fs32 (4.51) 
\fs24 \

\fs32 (4.52) 
\fs24 \

\fs32 The recursion rule is defined as follows: 
\fs24 \

\fs32 similarly, 
\fs24 \

\fs32 g(u) = 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 u u+2 
\fs32 otherwise 
\fs24 \

\fs21\fsmilli10667 u 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \uc0\u946 (T,U \u8722 1)=1 \u946 (T,u)=0,
\f2 \uc0\u8704 
\f0 u<U \uc0\u8722 1 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\f2\fs21\fsmilli10667 \cf2 \uc0\u8242 
\f0  
\fs24 \

\fs21\fsmilli10667 g(u)\uc0\u8232 
\fs32 \uc0\u946 (t, u) = 
\f1 \up29 \uc0\u56319 \u56335 
\f0  \up0 \uc0\u946 (t + 1, i)y
\fs21\fsmilli10667 \dn6 t+1,l
\fs16 \dn14 i
\f2 \up2 \uc0\u8242 
\f0  
\fs24 \up0 \

\fs21\fsmilli10667 i=u 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f2\fs32 \cf2 \up18 \uc0\u63732 \up10 \uc0\u63731 
\f0 \up0 u + 2, 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f2\fs32 \cf2 \uc0\u63729 
\f0 \uc0\u8232 
\f2 \up18 \uc0\u63732 \up8 \uc0\u63730 
\f0 \up0 u+1, ifl
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0  
\fs32 \up0 =blankorl
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0  
\fs32 \up0 =l
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0  
\fs24 \up0 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 94 
\fs24 \

\fs32 Chapter 4 
\fs24 \

\fs32 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 4.3.3 CTC Loss function 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The cross entropy error is a loss function used to measure accuracy of probabilistic measures. It is calculated as the negative log probability of a likelihood measure. The CTC loss function L(S) uses the cross entropy loss function of and is defined as the cross entropy error of correctly labelling all the training samples in some training set S: 
\fs24 \

\f1\fs32 \uc0\u56319 \u56336 \u56319 \u56335 
\f0  
\fs24 \

\fs32 L(S) = \uc0\u8722 ln Pr(z|x) = \u8722  lnPr(z|x) (4.53) 
\fs21\fsmilli10667 (x,z)
\f2 \uc0\u8712 
\f0 S (x,z)
\f2 \uc0\u8712 
\f0 S 
\fs24 \

\fs32 where z is the output label and x is the input sequence. Since L(S) in equation 4.53 is differentiable, this loss function can be back propagated to the softmax layer in the BiRNN configuration discussed in section 4.3. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 and therefore 
\fs24 \

\fs32 L(x,z) 
\f2 \uc0\u8801 
\f0  \uc0\u8722 lnPr(z|x) (4.54) 
\fs24 \

\fs32 L(S)= 
\f1 \up29 \uc0\u56319 \u56335 
\f0  \up0 L(x,z) (4.55) 
\fs21\fsmilli10667 (x,z)
\f2 \uc0\u8712 
\f0 S 
\fs24 \

\fs32 From the definition of the forward and backward variables (\uc0\u945 (t,u) and \u946 (t,u)), we also establish that X(t,u) 
\f2 \uc0\u8801 
\f0  \{\uc0\u960  
\f2 \uc0\u8712 
\f0  A
\f2\fs21\fsmilli10667 \up10 \uc0\u8242 
\f0 T 
\fs32 \up0 : B(\uc0\u960 ) = z, \u960 
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 = z
\fs21\fsmilli10667 \dn8 u
\f2 \up10 \uc0\u8242 
\f0  
\fs32 \up0 \}, such that 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 T\uc0\u8232 
\fs32 \uc0\u945 (t, u)\u946 (t, u) = 
\f1 \up29 \uc0\u56319 \u56335 
\f0  
\f1 \uc0\u56319 \u56336 
\f0  \up0 y
\fs21\fsmilli10667 \dn6 t,\uc0\u960 
\fs16 \dn8 t 
\fs24 \up0 \

\fs21\fsmilli10667 \uc0\u960 
\f2 \uc0\u8712 
\f0 X(t,u) \up2 t=1 
\fs24 \up0 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 then substituting Pr(\uc0\u960  | x) from the expression in equation ??, we have \u945 (t, u)\u946 (t, u) = 
\f1 \up29 \uc0\u56319 \u56335 
\f0  \up0 Pr(\uc0\u960  | x) 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 \uc0\u960 
\f2 \uc0\u8712 
\f0 X(t,u) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 (4.56) 
\fs24 \

\fs32 (4.57) 
\fs24 \

\fs32 Also observe that Pr(l | x) is equivalent to the total probability Pr(z | x). Paths going through z
\fs21\fsmilli10667 \dn8 u
\f2 \up10 \uc0\u8242 
\f0  
\fs32 \up0 at time t can be obtained as summed over all u to get 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 |z
\f2\fs16 \up8 \uc0\u8242 
\f0\fs21\fsmilli10667 \up0 |\uc0\u8232 
\fs32 Pr(z | x) = 
\f1 \up29 \uc0\u56319 \u56335 
\f0  \up0 \uc0\u945 (t, u)\u946 (t, u) (4.58) 
\fs24 \

\fs21\fsmilli10667 u=1\uc0\u8232 
\fs32 Chapter 4 I. J. Alamina 95 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Thus a sample loss is determined by 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 |z
\f2\fs16 \up8 \uc0\u8242 
\f0\fs21\fsmilli10667 \up0 |\uc0\u8232 
\fs32 L(x, z) = \uc0\u8722  ln 
\f1 \up29 \uc0\u56319 \u56335 
\f0  \up0 \uc0\u945 (t, u)\u946 (t, u) 
\fs24 \

\fs21\fsmilli10667 u=1 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 and therefore the overall loss is given by 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 |z
\f2\fs16 \up8 \uc0\u8242 
\f0\fs21\fsmilli10667 \up0 | 
\f1\fs32 \uc0\u56319 \u56335 \u56319 \u56335 
\f0  
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 L(S) = \uc0\u8722  ln \u945 (t, u)\u946 (t, u) 
\fs21\fsmilli10667 (x,z)
\f2 \uc0\u8712 
\f0 S \up2 u=1 
\fs24 \up0 \

\fs32 (4.59) 
\fs24 \

\fs32 (4.60) 
\fs24 \

\fs32 In the model described in this work, the gradient L(x, z)is computed using Ten- sorFlow\'92s automatic differentiation capabilities. In practice, computations soon lead to underflow. However, the log scale, being used in the above loss function calcula- tions avoids this situation and another useful equation in this context is 
\fs24 \

\fs32 ln(a + b) = ln(a) + ln(1 + e
\fs21\fsmilli10667 \up13 ln b\uc0\u8722 ln a
\fs32 \up0 ) (4.61) 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 4.4 Chapter Summary 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Deep Neural Networks (DNNs) are at the centre of the models developed within this research. They are able to overcome the challenge of complex modelling of latent information when discriminating directly from the data. To this extent, they tend to be data intensive in nature. In this chapter, neural network architectures and algorithms were considered. The Chapter begins with the rudimentary per- ceptron algorithm which is the precursor to the logistic regression algorithm. The Neural Network and Multi-Layer Peceptron (MLP), uses logistic regression concept and adds an extra layer of neurons and back propagation algorithm to optimise classifications. 
\fs24 \

\fs32 The Deep Neural Networks used within this research are special DNNs which are able to identify patterns in data having sequential patterns. These are the deep Recurrent Neural Networks (RNNs). It is shown in this Chapter that RNNs are able to learn the recurrent relationship information by modifying their architecture such that the data paths among the neurons are modified so that hidden states are also used as inputs. In addition the back propagation algorithm is also modified in 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 96 Chapter 4 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 terms of datapaths of the algorithm output to reflect the sequential structure of the neural network. 
\fs24 \

\fs32 Special categories of RNNs used to build speech and language models developed in this thesis are the Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) RNN cells. In this research were utilised for development of a character- based language model and the Bidirectional RNN (Bi-RNN) used for development of the speech model. 
\fs24 \

\fs32 A special algorithm, the Connectionist Temporal Classification (CTC) algorithm, also employed by the BiRNN is described in this Chapter. The CTC algorithm over- comes the challenge of a character-based speech model when considering misaligned nature of audio data between specific points in the audio that correspond to equiv- alent characters in the transcription. In addition, it is also discussed how the CTC algorithm utilises the forward-backward algorithm to perform classification. In a later Chapter 7, the prefix beam search algorithm is described, and, in combination with output probabilities obtained from the CTC algorithm and a language model, performs decoding of the output into the actual speech-to-text translations. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 4 I. J. Alamina 97 
\fs24 \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 Chapter 5 
\fs24 \

\fs66\fsmilli33333 Background 2: Deep Scattering network 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Curve fitting is a very common theme in pattern recognition. The concept of invari- ant functions convey mapping functions that approximate a discriminating function when a parent function is reduced from a high dimensional space to a low dimen- sional space Mallat (2016). In this chapter an invariance function called a scattering transform enables invariance of groups of deformations that could apply to speech signals thereby preserving higher level characterisations useful for classifying speech sounds. Works done by (And\uc0\u32 \u769 en and Mallat, 2011, Peddinti et al., 2014, Sainath et al., 2014, Zeghidour et al., 2016) have shown that when the scattering spectrum are applied to speech signals and used as input to speech systems have state of the art performance. In particular Sainath et al. (2014) shows 4-7% relative improve- ment in word error rates (WER) over Mel frequencies cepstral coefficients (MFCCs) for 50 and 430 hours of English Broadcast News speech corpus. While experiments have been performed with hybrid HMM-DNN systems in the past, this thesis focuses on the use of scatter transforms in end-to-end RNN speech models. 
\fs24 \

\fs32 This chapter iterates the use of the Fourier transform as the starting analysis function for building invariant functions and then discusses the Mel filter bank solu- tion and then establishes why the scattering transform through the wavelet modulus operator provides better invariance features over the Mel filters. 
\fs24 \

\fs32 98 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 5.1: Fourier Equation 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 5.1 Fourier transform 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The Fourier transform often referred to as the power spectrum, allows us to dis- cover frequencies contained within a signal. The Fourier transform is a convolution between a signal and a complex sinusoid from \uc0\u8722 \u8734  to +\u8734  (Figure 5.1). 
\fs24 \

\fs32 From the orthogonal property of complex exponential function, two functions are orthogonal if 
\f1 \up26 \uc0\u56319 \u56334 
\f0  \up0 f(x)g(x) = 0 where f(x) and g(x) are complementary functions, one being referred to as the analysis equation and the other referred to as the synthesis function. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page99image6945200.png \width3960 \height1650 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 If the discrete form of the Fourier transform analysis equation is given by 1 
\f1 \up21 \uc0\u56319 \u56337 
\f0  
\fs21\fsmilli10667 \up13 T/2 
\fs32 \dn11 (
\fs21\fsmilli10667 \dn8 \uc0\u8722 j
\fs16 \up0 2\uc0\u960 kt
\fs32 \dn11 ) 
\fs24 \up0 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \dn11 x(t) = 
\f1 \up18 \uc0\u56319 \u56335 
\f0  \dn11 a e\up0 (
\fs21\fsmilli10667 \up2 j 
\fs16 \up10 2\uc0\u960 kt 
\fs32 \up0 ) 
\fs21\fsmilli10667 \dn6 k
\fs16 \up5 T 
\fs24 \up0 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 k=\uc0\u8722 \u8734  
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \up5 a
\fs21\fsmilli10667 \up0 k 
\fs32 \up5 = \dn16 T 
\fs24 \up0 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 x(t)e 
\fs16 \up5 T\up0 \uc0\u8232 
\fs32 Then, the corresponding synthesis equation is given by 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 \uc0\u8734  
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 (5.1) 
\fs24 \

\fs32 (5.2) 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 \uc0\u8722 T /2 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Recall that x(t) is the original signal while a
\fs21\fsmilli10667 \dn6 k 
\fs32 \up0 is the Fourier Series coefficient. This coefficient indicates the amplitude and phase of the original signal\'92s higher order harmonics indexed by k such that higher values of k correspond to higher frequency components. In a typical spectrogram (Figure 5.2), it can be seen that the energy of the signal is concentrated about a central region and then harmonic spikes of energy content exponentially decrease and taper off. Therefore in Figure 5.2, the energies are concentrated at frequencies of about 100, 150 and 400 hertz. 
\fs24 \

\fs32 The Fourier transform discussed in the preceding paragraph constitutes a valu- able tool for the analysis of the frequency component of a signal. However is not able to determine when in time a frequency occurs hence is not able to analyse time 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 5 I. J. Alamina 99 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 5.2: Sample Spectrogram cwt (2015) 
\fs24 \

\fs32 related signal deformations. The Short-time Fourier Transform (STFT) attempts to salvage this by windowing the signal in time signal and performing Fourier trans- forms over sliding windows sections of the original signal rather than the whole signal. There is however, a resolution trade off that ensues from this operation such that, the higher the resolution in time accuracy, the lower the frequency accuracy and vice versa. In the next section on the continuous wavelet transform, how the wavelet transform improves on the weaknesses of the Fourier Transform and the STFT is reviewed. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 5.2 Wavelet transform 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The continuous wavelet transform can be defined as a signal multiplied by scaled and shifted version of a wavelet function \uc0\u968 (t) referred to as the mother wavelet. The time-frequency tile-allocation of the three basic transforms examined in the first part of this chapter is illustrated in Figure 5.3 
\fs24 \

\fs32 It can be seen here that for the Fourier transform there is no time information obtained. In the STFT, as there is no way of telling where in time the frequencies are contained, the STFT makes a blanket range of the resolution of the window and is therefore equally tiled potentially losing information based on this setup. For the case of the wavelet, because it is a scaled and shifted convolution, it takes care of the this problem providing a good resolution in both time and frequency. The 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page100image6945616.png \width3971 \height2977 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 100 Chapter 5 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 5.3: Time frequency tiling for (a) Fourier Transform (b) Short-time Fourier Transform (STFT) (c) Wavelet transform 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page101image6986864.png \width7920 \height2892 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 fundamental representation of the continuous wavelet function is: 
\f1 \up21 \uc0\u56319 \u56337 
\f0  \up0 1 
\f1 \up24 \uc0\u56319 \u56325 
\f0 \up0 t\uc0\u8722 b
\f1 \up24 \uc0\u56319 \u56326 
\f0  
\fs24 \up0 \

\fs32 C(a,b) = f(t)\uc0\u8730 \dn22 a\up0 \uc0\u968  \dn22 a \up0 dt (5.3) 
\fs24 \

\fs32 In this equation, a and b respectively represent the scaling and shifting resolution variables of the wavelet function. This is referred to as a mother wavelet. A few other mother wavelet functions discussed later in this chapter. Generally a mother wavelet is identified as being energy spikes in an infinite signal whose accumulative energy sums to zero. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 5.3 Discrete and Fast wavelet transform 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Synthesis and analysis equations (5.2 and 5.1) can be formulated as a linear com- bination of the basis \uc0\u966 
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 (t) such that the basis, \uc0\u966 
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 (t) = e
\fs21\fsmilli10667 \up10 j2\uc0\u960 kt
\fs32 \up0 , and it\'92s conjugate or 
\fs24 \

\fs32 \uc0\u771  orthonormal basis, \u966 
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 (t) = e 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 \uc0\u8722 j2\u960 kt 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 , equations (5.2 and 5.1) now become 
\fs24 \

\fs32 x(t) = 
\f1 \up29 \uc0\u56319 \u56335 
\f0 \up0 a
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 \uc0\u966 
\fs21\fsmilli10667 \dn6 k \up0 k 
\fs24 \

\f1\fs32 \uc0\u56319 \u56337 
\f0  
\fs24 \

\fs32 a
\fs21\fsmilli10667 \dn6 k 
\fs32 \up0 = 
\fs24 \

\fs32 \uc0\u771  x(t)\u966 
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 (t) 
\fs24 \

\fs32 (5.4) 
\fs24 \

\fs32 (5.5) 
\fs24 \

\fs32 With respect to scaling and shifting variables of continuous wavelet transforms in equation (5.3), a similar linear combination transformation can be applied by constructing orthonormal bases parameters, referred to as scaling (\uc0\u966 ) and translating 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 5 I. J. Alamina 101 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 (\uc0\u968 ) functions. For example, a simple Haar mother wavelet transform associated with a delta function, it is seen that: 
\fs24 \

\fs32 \uc0\u966 
\fs21\fsmilli10667 \dn6 j,k
\fs32 \up0 (t) = 2
\fs21\fsmilli10667 \up13 j/2
\fs32 \up0 \uc0\u966 (2
\fs21\fsmilli10667 \up13 j
\fs32 \up0 t \uc0\u8722  k) (5.6) 
\fs24 \

\fs32 \uc0\u968 
\fs21\fsmilli10667 \dn6 j,k
\fs32 \up0 (t) = 2
\fs21\fsmilli10667 \up13 j/2
\fs32 \up0 \uc0\u968 (2
\fs21\fsmilli10667 \up13 j
\fs32 \up0 t \uc0\u8722  k) (5.7) 
\fs24 \

\fs32 where j is associated with the dilation (scaling) parameter and k is associated with \uc0\u8730 \u8730  
\fs24 \

\fs32 the position (shifting) parameter. If the Haar coefficients h
\fs21\fsmilli10667 \dn6 (\'b7)
\fs32 \up0 [n] = \{1/ 2,1/ 2\} are extracted we have the following dilation and position parameters. 
\fs24 \

\fs32 \uc0\u8730  
\fs24 \

\fs32 \uc0\u8730  
\fs24 \

\fs32 \uc0\u968 (t) = h
\fs21\fsmilli10667 \dn6 \uc0\u966 
\fs32 \up0 [n]\uc0\u8232 For any signal, a discrete wavelet transform in l
\fs21\fsmilli10667 \up10 2
\fs32 \up0 (Z)
\fs21\fsmilli10667 \up10 1 
\fs32 \up0 can be approximated by 
\fs24 \

\fs32 11
\fs21\fsmilli10667 \up18 \uc0\u8734 \up0 \uc0\u8232 
\fs32 f[n]=\uc0\u8730  
\f1 \up29 \uc0\u56319 \u56335 
\f0 \up0 W
\fs21\fsmilli10667 \dn6 \uc0\u966 
\fs32 \up0 [j
\fs21\fsmilli10667 \dn6 0
\fs32 \up0 ,k]\uc0\u966 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 0
\fs21\fsmilli10667 \dn6 ,k
\fs32 \up0 [n]+\uc0\u8730  
\f1 \up29 \uc0\u56319 \u56335 \u56319 \u56335 
\f0 \up0 W
\fs21\fsmilli10667 \dn6 \uc0\u968 
\fs32 \up0 [j,k]\uc0\u968 
\fs21\fsmilli10667 \dn6 j,k
\fs32 \up0 [n] (5.10) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \up10 M
\fs21\fsmilli10667 \up0 k 
\fs32 \up10 M
\fs21\fsmilli10667 \up0 j=j
\fs16 \dn3 0 
\fs21\fsmilli10667 \up0 k 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Here f [n], \uc0\u966 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 0 
\fs21\fsmilli10667 \dn6 ,k 
\fs32 \up0 [n] and \uc0\u968 
\fs21\fsmilli10667 \dn6 j,k 
\fs32 \up0 [n] are discrete functions defined in [0,M - 1], having a total of M points. Because the sets \{\uc0\u966 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 0
\fs21\fsmilli10667 \dn6 ,k
\fs32 \up0 [n]\}
\fs21\fsmilli10667 \dn6 k
\f2 \uc0\u8712 
\f0 Z 
\fs32 \up0 and \{\uc0\u968 
\fs21\fsmilli10667 \dn6 (j,k)
\f2 \uc0\u8712 
\f0 Z
\fs16 \up0 2
\fs21\fsmilli10667 \dn6 ,j\uc0\u8805 j
\fs16 \dn8 0 
\fs32 \up0 \} are orthogonal to each other. We can simply take the inner product to obtain the wavelet coefficients. 
\fs24 \

\fs32 W
\fs21\fsmilli10667 \dn6 \uc0\u966 
\fs32 \up0 [j
\fs21\fsmilli10667 \dn6 0
\fs32 \up0 , k] = \uc0\u8730 \up21 1 
\f1 \up29 \uc0\u56319 \u56335 
\f0  \up0 f[n]\uc0\u966 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 0
\fs21\fsmilli10667 \dn6 ,k
\fs32 \up0 [n] (5.11) \up5 M
\fs21\fsmilli10667 \dn6 n 
\fs24 \up0 \

\fs32 W
\fs21\fsmilli10667 \dn6 \uc0\u968 
\fs32 \up0 [j,k] = \uc0\u8730 \up21 1 
\f1 \up29 \uc0\u56319 \u56335 
\f0 \up0 f[n]\uc0\u968 
\fs21\fsmilli10667 \dn6 j,k
\fs32 \up0 [n] j \uc0\u8805  j
\fs21\fsmilli10667 \dn6 0 
\fs32 \up0 (5.12) 
\fs24 \

\fs32 M 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 n 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Equation (5.11) is called approximation coefficient while (5.12) is called detailed coefficients. 
\fs24 \

\fs32 These two components show that the approximation coefficient, W
\fs21\fsmilli10667 \dn6 \uc0\u966 
\fs32 \up0 [j
\fs21\fsmilli10667 \dn6 0
\fs32 \up0 , k], models a low pass filter and the detailed coefficient,W
\fs21\fsmilli10667 \dn6 \uc0\u968 
\fs32 \up0 [j
\fs21\fsmilli10667 \dn6 0
\fs32 \up0 ,k], models a high pass filter. It is possible to determine the approximation and detailed coefficients without the 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \uc0\u966 (t) = h
\fs21\fsmilli10667 \dn6 \uc0\u966 
\fs32 \up0 [n] 
\fs24 \

\fs32 2\uc0\u966 (2t \u8722  n) (5.8) 
\fs24 \

\fs32 2\uc0\u968 (2t \u8722  n) (5.9) 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 102 Chapter 5 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 scaling and dilating parameters. The resulting coefficients, called the fast wavelet transform, are a convolution between the wavelet coefficients and a down-sampled version of the next order coefficients. The fast wavelet transform was first postulated in (Mallat, 1989). 
\fs24 \

\fs32 W
\fs21\fsmilli10667 \dn6 \uc0\u966 
\fs32 \up0 [j, k] = h
\fs21\fsmilli10667 \dn6 \uc0\u966 
\fs32 \up0 [\uc0\u8722 n] 
\f2 \uc0\u8727 
\f0  W
\fs21\fsmilli10667 \dn6 \uc0\u966 
\fs32 \up0 [j + 1, n]|
\fs21\fsmilli10667 \dn6 n=2k,k\uc0\u8805 0 
\fs32 \up0 (5.13) 
\fs24 \

\fs32 W
\fs21\fsmilli10667 \dn6 \uc0\u968 
\fs32 \up0 [j
\fs21\fsmilli10667 \dn6 0
\fs32 \up0 , k] = h
\fs21\fsmilli10667 \dn6 \uc0\u968 
\fs32 \up0 [\uc0\u8722 n] 
\f2 \uc0\u8727 
\f0  W
\fs21\fsmilli10667 \dn6 \uc0\u966 
\fs32 \up0 [j + 1, n]|
\fs21\fsmilli10667 \dn6 n=2k,k\uc0\u8805 0 
\fs32 \up0 (5.14) For analysis of the Haar wavelet and the derivation of equations (5.13 and 5.14) 
\fs24 \

\fs32 see Appendix I. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 5.4 Mel filter banks 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The Fourier and wavelet transform are general means of extracting information from continuous signals using the frequency domain and in the case of the Wavelet trans- form using both time and frequency domain. The objective in Machine Learning, however, is to extract patterns from the derived information. In this chapter, in particular, the Mel filter bank and the scatter transform are elaborated on as speech feature extractors. They process high dimensional information obtained from the Fourier and wavelet transform signal processing techniques and reducing the in- formation obtained as lower dimension features. All this aimed towards loss-less encoding of speech signals relevant for speech recognition. 
\fs24 \

\fs32 The Mel filter banks form the basis of the Mel Frequency Cepstral Coeffi- cients (MFCCs) described by (Davis and Mermelstein, 1980). MFCCs are state- of-the-art speech feature engineering drivers behind automatic speech recognition acoustic models. Other common speech features used in speech recognition in- clude, Linear Prediction Coefficients (LPCs) and Linear Prediction Cepstral Co- efficients (LPCCs),Perceptual Linear Prediction coefficients (PLP), (Dines et al., 2010, McLoughlin, 2009). The following paragraphs describe how the Mel filters are derived. 
\fs24 \

\fs32 The Mel scale as described by Stevens et al. (1937) is a perceptual scale which Chapter 5 I. J. Alamina 103 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 measures sound frequencies as perceived by human subjects equidistant from a sound source as compared to the actual frequency. This scale is non-linear as the human ear processes sound non-linearly both in frequency as well as amplitude. 
\fs24 \

\fs32 For the case of frequency, the human ear can discriminate lower frequencies more accurately than the higher frequencies. The Mel scale model this behaviour by utilising frequency bins. The frequency bin ranges are narrow at low frequencies and become wide in higher frequencies. In the case of the speech signal amplitude, a similar process is observed where the ear discriminates softer sounds better than louder sounds. Generally, sound will be required to be 8 times as loud for significant perception by the ear. While the Mel scales handle the frequency non-linearity in the speech signal, the signal amplitude is linearised during feature extraction by taking the log of the power spectrum of the signal, also known as the cepstral values. Furthermore, using a log scale also allows for a channel normalisation technique that employs cepstral mean subtraction. (Becchetti, 1999). 
\fs24 \

\fs32 The minimum frequency number of bins used for the Mel scale is 26 bins. In order to determine the frequency ranges we use the following formula to convert between the Mel scale and the regular frequency scale: 
\fs24 \

\fs32 M (f ) = 1125 ln(1 + f /700) (5.15) 
\fs24 \

\fs32 M
\fs21\fsmilli10667 \up13 1
\fs32 \up0 (m) = 700exp(m/1125)1 (5.16) 
\fs24 \

\fs32 A simple approximation for the Mel scale is obtained by applying linear scale for the first ten filters and for the first 1kHz of the speech frequency range then applying the following formula for the rest (Becchetti, 1999)(Becchetti, 1999): 
\fs24 \

\fs32 \uc0\u8710 
\fs21\fsmilli10667 \dn6 m 
\fs32 \up0 = 1.2 \'d7 \uc0\u8710 
\fs21\fsmilli10667 \dn6 m\uc0\u8722 1 
\fs32 \up0 (5.17) 
\fs24 \

\fs32 where m is the frequency bin index and \uc0\u8710 
\fs21\fsmilli10667 \dn6 m 
\fs32 \up0 is the frequency range between the start and end frequencies for the m-th bin. The resulting filters are overlapping filters shown in Figure 5.4. For speech recognition, we compute a statistical value or coefficient for each Mel frequency bin from the inverse discrete fourier transform (IDFT) of the Mel filters. The coefficients are also concatenated with their delta 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 104 Chapter 5 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 5.4: Mel filter plot (Lyons, 2012) 
\fs24 \

\fs32 and delta-delta values. The delta and delta-delta values are determined from the following equation: 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f1\fs32 \cf2 \up29 \uc0\u56319 \u56333 
\f0\fs21\fsmilli10667 \up21 N\dn6 n=1 
\fs32 \up5 n(c
\fs21\fsmilli10667 \up0 t+n
\fs32 \up5 c
\fs21\fsmilli10667 \up0 tn
\fs32 \up5 )\up0 \uc0\u8232 \up2 d
\fs21\fsmilli10667 \dn3 t 
\fs32 \up2 = \dn24 2
\f1 \up0 \uc0\u56319 \u56333 
\f0\fs21\fsmilli10667 \dn8 N\dn32 n=1
\fs32 \dn24 n
\fs21\fsmilli10667 \dn16 2 
\fs32 \up2 (5.18) 
\fs24 \up0 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 where c
\fs21\fsmilli10667 \dn6 x 
\fs32 \up0 is the x-th coefficient and 2n is the delta range which is usually 2 \uc0\u8722  4. The delta values are first order derived coefficients obtained from the original Mel filter coefficients while the delta-delta values are second-order derived coefficients obtained from the first-order derived delta coefficients. 
\fs24 \

\fs32 There are two reasons for obtaining the IDFT from the filter banks. The first is that since the bins use overlapping windows, the filter bin outputs tend to be corre- lated and obtaining the IDFT helps to decorrelate the outputs. Secondly, decorre- lated signals optimise algorithm computation efficiency involving matrix operations such that rather than using full co variance matrix, it is much simpler to compute the matrix operations from the matrix diagonal. Also note that for cepstral values obtained from taking the log of the power power spectrum, the discrete cosine trans- form (DCT) is used to obtain the IDFT. This is as a result of the cepstral values being real and symmetric(Gales et al., 2008). 
\fs24 \

\fs32 As an attempt for MFCCs to incorporate dynamic frequency changes of the signal, the deltas and the delta-deltas are obtained from the coefficient computation 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page105image6941248.png \width6803 \height5102 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page105image9132992.png \width1131 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 5 I. J. Alamina 105 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 in equation 5.18. However, it is worthy to note that only the first 13 of the coefficients and the resulting dynamic coefficients are used as speech features as it is observed that higher frequency dynamic coefficients rather degrade ASR performance (Gales et al., 2008). 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 5.5 Deep scattering spectrum 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Scattering wavelets are interpreted from Mel Frequency Spectral Coefficients (MFCC) which are the Mel filters without applying the IDFT or the DCT.In this section ref- erence is made to (And\uc0\u32 \u769 en and Mallat, 2011, 2014, Zeghidour et al., 2016) for the scattering wavelet definition. 
\fs24 \

\fs32 The Mel scale can be interpreted as dilations of a Wavelet. In the case of the MFSC it lacks the ability to capture non-stationary structures when outside the frame window. Such a filter is constructed by dilating one filter having an octave bandwidth of 1/Q as follows 
\fs24 \

\fs32 \uc0\u968 
\fs21\fsmilli10667 \dn6 j
\fs32 \up0 (t) = a
\fs21\fsmilli10667 \up13 \uc0\u8722 j
\fs32 \up0 \uc0\u968 (a
\fs21\fsmilli10667 \up13 \uc0\u8722 j
\fs32 \up0 t) | a = 2
\fs21\fsmilli10667 \up13 1/Q 
\fs32 \up0 and j \uc0\u8804  J (5.19) 
\fs24 \

\fs32 The transfer function of the constructed filter approximately ranges between 2Q\uc0\u960  \u8722  \u960  and Q\u960  + \u960 . For low frequencies below 2
\fs21\fsmilli10667 \up10 \uc0\u8722 J 
\fs32 \up0 a simple low pass filter is em- ployed. Non-linear invariance is induced by applying a contracting modulus opera- tion and the equivalent result is similar to deriving extracting a contracted envelope at different resolutions while filtering out the complex phase information. Therefore, for a signal x we define the zeroth order Scattering transform as follows: 
\fs24 \

\fs32 |W |x = (x 
\f1 \uc0\u8902 
\f0  \uc0\u966 (t), |x 
\f1 \uc0\u8902 
\f0  \uc0\u968 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 1 
\fs32 \up0 (t)|)
\fs21\fsmilli10667 \dn11 t
\f2 \uc0\u8712 
\f0 R,j
\fs16 \dn14 1 
\f2\fs21\fsmilli10667 \dn11 \uc0\u8712 
\f0 J
\fs16 \dn14 1 
\fs32 \up0 (5.20) 
\fs24 \

\fs32 However, the difference between the MFCC and the Deep Scattering Network (DSN) is that the MFCC is a shallow architecture while the DSN is a deep ar- chitecture. Therefore, it is observed that time-averaging of the low-pass filter loses information contained in the high frequencies. Nevertheless, since the wavelet trans- form is invertible, the DSN is able to go deeper by applying the next level scattering operation. The transform is therefore reapplied over S
\fs21\fsmilli10667 \dn6 0
\fs32 \up0 x = x 
\f1 \uc0\u8902 
\f0  \uc0\u966 (t) on the residue 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 106 Chapter 5 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 wavelet coefficients x 
\f1 \uc0\u8902 
\f0  \uc0\u968 
\fs21\fsmilli10667 \dn6 \uc0\u955 
\fs16 \dn8 1 
\fs32 \up0 | and the process of time averaging to obtain invariant contracted envelopes reapplied. The resulting first order scattering coefficient were obtained from 
\fs24 \

\fs32 S
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 x(t, \uc0\u968 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 1 
\fs32 \up0 ) = |x 
\f1 \uc0\u8902 
\f0  \uc0\u968 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 1 
\fs32 \up0 | 
\f1 \uc0\u8902 
\f0  \uc0\u966 (t)) (5.21) 
\fs24 \

\fs32 It is shown in And\uc0\u32 \u769 en and Mallat (2014) that if the wavelets \u968 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 1 
\fs32 \up0 have the same fre- quency resolution as the standard Mel-filters, then the S
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 x coefficients approximate the Mel-filter coefficients. Unlike the Mel-filter banks however, there is a means of regaining discriminating feature information, lost in high frequencies in a DSN. This can be observed when first order wavelets \uc0\u968 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 2 
\fs32 \up0 are applied to DSN coefficients |x
\f1 \uc0\u8902 
\f0 \uc0\u966 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 1
\fs32 \up0 |: 
\fs24 \

\fs32 |W
\fs21\fsmilli10667 \dn6 2
\fs32 \up0 ||x 
\f1 \uc0\u8902 
\f0  \uc0\u966 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 1 
\fs32 \up0 | = (|x 
\f1 \uc0\u8902 
\f0  \uc0\u968 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 1 
\fs32 \up0 | 
\f1 \uc0\u8902 
\f0  \uc0\u966 , ||x 
\f1 \uc0\u8902 
\f0  \uc0\u968 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 1 
\fs32 \up0 | 
\f1 \uc0\u8902 
\f0  \uc0\u968 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 2 
\fs32 \up0 |)
\fs21\fsmilli10667 \dn11 j
\fs16 \dn14 2
\f2\fs21\fsmilli10667 \dn11 \uc0\u8712 
\f0 J
\fs16 \dn14 2 
\fs32 \up0 ) (5.22) 
\fs24 \

\fs32 The second order DSN coefficients also requires time averaging to derive local invariance stability, hence the resulting coefficients are averaged again with a low- pass filter \uc0\u966  and the final second order DSN scattering parameters are obtained. 
\fs24 \

\fs32 S
\fs21\fsmilli10667 \dn6 2
\fs32 \up0 x(t, j
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 , j
\fs21\fsmilli10667 \dn6 2
\fs32 \up0 ) = ||x 
\f1 \uc0\u8902 
\f0  \uc0\u968 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 1 
\fs32 \up0 | 
\f1 \uc0\u8902 
\f0  \uc0\u968 
\fs21\fsmilli10667 \dn6 j
\fs16 \dn8 2 
\fs32 \up0 | 
\f1 \uc0\u8902 
\f0  \uc0\u966 (t) (5.23) 
\fs24 \

\fs32 Figure 5.5 shows how the process of obtaining scatter coefficients can be succes- sively made deeper computing higher-order invariance by retrieving the lost charac- teristics and thus culminating a deep scattering spectrum. And\uc0\u32 \u769 en and Mallat (2014) shows that speech signals can be analysed using the first two DSN layers and the coefficients obtained are generally stable to deformation and translation invariant while possessing better discriminating features than MFCCs. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 5.6 Chapter Summary 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 This chapter highlights the characteristics of the Deep Scattering Network that en- able it as a candidate rich in pattern recognition discriminators for speech recogni- tion. It is shown also that features required for speech feature invariance is recovered through successive layers of the deep scattering network. 
\fs24 \

\fs32 The development of this algorithm in this chapter introduces the Fourier trans- form as a means of determining frequency contents in a speech wave. While the 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 5 I. J. Alamina 107 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Figure 5.5: Scattering network - 2 layers deep And\uc0\u32 \u769 en and Mallat (2011) 
\fs24 \

\fs32 Fourier transform has high resolution for frequency, it has no temporal resolution and temporal frequencies or the instantaneous frequency within speech signals there- fore cannot be resolved in the Fourier Transform. The Short-time Fourier Transform (STFT) attempts to solve this but there is a trade-off to be made between the tem- poral and frequency resolution. Better resolutions of time and frequencies however, can be obtained using the Wavelet transform. This chapter also discussed the char- acteristics of the Wavelet transform that enable better time and frequency filtering. 
\fs24 \

\fs32 Finally, MFCCs and Deep Scattering Networks (DSNs) are discussed and com- pared. It was shown here that through wavelet operations employed by the Deep Scattering Networks frequency resolution lost in MFCC is gained by the DSN and these frequencies contain information relevant for speech invariance. In turn, invari- ance information is highly useful for better speech discrimination. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page108image6945408.png \width7946 \height4957 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 108 Chapter 5 I. J. Alamina 
\fs24 \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 Chapter 6 
\fs24 \

\fs66\fsmilli33333 Empirical Analysis 1: Wakirike Language Model 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Neural networks have become increasingly popular due to their ability to model non- linear system dynamics. Since their inception, there have been many modifications made to the original design of having linear affine transformations terminated with a nonlinear functions as the means to capture both linear and non-linear features of the target system. In particular, one of such neural network modifications, namely the recurrent neural network, has been shown to overcome the limitation of varying lengths in the inputs and outputs of the classic feed-forward neural network. In addition the RNN is not only able to learn non-linear features of a system but has also been shown to be effective at capturing the patterns in sequential data. 
\fs24 \

\fs32 A language model for the Wakirike language is developed in this chapter. This model draws upon the premise that the grammar of a language is expressed in the character sequence pattern ultimately revealed in the words rendered by the charac- ter sequences. Therefore, abstract grammar rules can be extracted and learned by a character-based RNN neural network. Specialised implementations of the RNN called the Long Short Term Memory (LSTM) and also the Gated Recurrent Unit (GRU), discussed in chapter 4, are designed to capture patterns over particularly long data sequences and are thus, ideal candidates for generating character sequences while preserving syntactic language rules in the words formed from generated charac- ter sequences. These long-term relationships and patterns are learned by the neural network model from the training data it ingests. 
\fs24 \

\fs32 109 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 6.1 Data Preparation 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The Wakirike New Testament Bible served as the source of data for the deep neural network training. There is no readily available soft or on-line copy of the Wakirike new testament bible. As such, the Wakirike New Testament Bible was typed to form a text corpus, giving rise to a complete corpus word size of 668,522 words and a character count of 6,539,176 characters. The data set was then divided into 11 parts. Two parts dedicated for testing and validation and the remaining nine parts were used for training. 
\fs24 \

\fs32 The Unicode representations of the character set consisting of letters and punc- tuation marks are one-hot encoded and batched for sequential input, each batch having a character sequence length of 30 characters. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 6.2 GRU Training 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The modified LSTM RNN known as the Gated Recurrent Unit (GRU) is employed for the neural network model built in this work, in order to optimise network per- formance while conserving computation resources. GRUs have been shown to give similar performance to regular LSTMs however, with a lighter system resource foot- print (Cho et al., 2014). The GRU RNN used to train the Wakirike text corpus comprised an internal network size of 512 nodes for each layer and was 3 layers deep. Externally, 30 GRUs represented the number of recurrent connections each connection representing a time step bearing contextual for the recurrent input se- quence. 
\fs24 \

\fs32 To mitigate for over-fitting, due to the multi-layered high-dimensional depth of this neural network, a small learning rate of 0.001 was used. To further marginalise over-fitting the popular and effective dropout method (Srivastava et al., 2014) for regularising deep neural networks was kept at 20% such that only 80% of neural net- work activations are propagated from one layer to the next, whereas the remaining 20% were randomly zeroed out. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 110 Chapter 6 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 6.3 Output Language Generation 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The neural network was trained for 10 epochs and achieved a prediction accuracy of 85% on held-out data. With this GRU character-based language model, it is possible to seed this network with an input character and select from the top-N candidates thus causing the Neural network to generate its own sentences. In this scenario, the network is said to perform language generation by immanently constructing its own sentences. The generated language output from the GRU language model was found to be intelligible and a reflection of the overall context of the training data. 
\fs24 \

\fs32 A clever use of this new corpus generated by the GRU language model of this work was to determine a word-based perplexity metric for the GRU neural language model. In this work, the word-based perplexity metric was determined from the output language generated by first estimating the word based perplexity on the training data. The same perplexity calculation was then used on the generated neural language model corpus. The corpus size of the neural language model was made to be equivalent to that of the training data, that is containing 6,539,176 characters. The perplexity calculation was based on a modified Kneser-Key 5-gram model with smoothing (Heafield et al., 2013). The results discussed below showed that the GRU-based RNN model generated a superior model compared to the n- gram model that better matched the training data. 
\fs24 \

\fs32 The evaluation of the GRU language model of the Wakirike language was per- formed using a perplexity measurement metric. The Perplexity metric applies the language model to a test data-set and measures how probable the test data-set is. Perplexity is a relative measure given by the formula: 
\fs24 \
\pard\pardeftab720\sl180\sa240\partightenfactor0

\fs16 \cf2 1 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 PP(W) = P(w
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 ,w
\fs21\fsmilli10667 \dn6 2 
\fs32 \up0 ...w
\fs21\fsmilli10667 \dn6 N
\fs32 \up0 )
\fs16 \up5 N 
\fs32 \up0 (6.1) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f1\fs32 \cf2 \up37 \uc0\u56319 \u56342 \up18 \uc0\u56319 \u56341 \up0 \uc0\u56319 \u56341 \dn19 \uc0\u56319 \u56340 
\f0\fs16 \dn16 N 
\f1\fs32 \up0 \uc0\u56319 \u56336 
\f0\fs21\fsmilli10667 \up10 N 
\fs32 \dn8 1\up0 \uc0\u8232 P P (W ) = \dn22 P (w
\fs21\fsmilli10667 \dn27 i
\fs32 \dn22 |w
\fs21\fsmilli10667 \dn27 i\uc0\u8722 1
\fs32 \dn22 ) \up0 (6.2) 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 i=1\uc0\u8232 
\fs32 Where w
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 , . . . , w
\fs21\fsmilli10667 \dn6 N 
\fs32 \up0 are the sequence of words. The language model with the lower 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 relative perplexity score is therefore expected to yield better approximation of the data when applied to unseen data generally. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page72image9092864.png \width1058 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page72image9092672.png \width783 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 6 I. J. Alamina 111 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Table 6.1: Perplexity Calculation results Perplexity 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page112image9145728.png \width7175 \height20 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Language Model\uc0\u8232 Held-out data size (characters) 
\fs24 \

\fs32 LSTM RNN\uc0\u8232 3-gram with Keysner Soothing and interpolation 
\fs24 \

\fs32 998 
\fs24 \

\fs32 1.6398 1.8046 
\fs24 \

\fs32 99 
\fs24 \

\fs32 1.7622 1.9461 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page112image9145920.png \width5669 \height15 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page112image9145920.png \width5669 \height15 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page112image9145728.png \width7175 \height20 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The result of the training of the GRU-Cell Recurrent Neural Network on low- resourced Wakirike Language gave impressive and intelligible results and showed better results when measured with standard n-gram language models. The results showed that it is indeed possible to derive a language model using a GRU-cell RNN on a low resource character sequence corpus for the Wakirike language. 
\fs24 \

\fs32 A character based perplexity metric is possible using the negative log likelihood of the character sequence. 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f1\fs32 \cf2 \up32 \uc0\u56319 \u56331 \up24 \uc0\u56319 \u56333 
\f0\fs21\fsmilli10667 \up16 T\dn8 t=1 
\fs32 \up0 logP(x
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 |x
\fs21\fsmilli10667 \dn6 1:t1
\fs32 \up0 )
\f1 \up32 \uc0\u56319 \u56332 
\f0 \up0 \uc0\u8232 PP(X) = exp \dn22 T \up0 (6.3) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 However, our base-line language model is a 5-gram word-based language model. Therefore, comparing a word based model to a character based model requires a conversion step. In this work, the conversion step involved using the GRU language model generated a corpus which was rescored by re-estimating with a 5-gram word- based language model 
\fs24 \

\fs32 Table 6.1 shows the Results of the Perplexity model of the LSTM Wakirike Language model and an equivalent 5-gram Language model with interpolation and Keysner smoothing (Heafield et al., 2013) for various lengths of the held-out data. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 6.4 Chapter Summary 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 This chapter shows the application of a character-based Gated Recurrent Unit RNN on the low resource language of Wakirike to generate a language model for the Wakirike language. The data-set and preparation and the details of the network were discussed. The output of this model was used to hallucinate the Wakirike language which was then scored against word-based perplexity to obtain a metric against the baseline language model. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page112image9146496.png \width1378 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 112 Chapter 6 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 It can be inferred that the GRU character-model developed has an improved language model and because it is based on a character-model, which is fine-grained when compared to a word model, it is likely to generalise data better when used in practice and is less biased than a word-based model. This can be observed from the fact that the output corpus produced a larger vocabulary size. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 6 I. J. Alamina 113 
\fs24 \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 Chapter 7 
\fs24 \

\fs66\fsmilli33333 Empirical Analysis 2: Deep Recurrent Speech Recognition models 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 This work explores the prospects of deep recurrent end-to-end architectures applied to speech recognition. Complementary aspects of developing speech recognition systems are eliminated by focusing on end-to-end speech units as a two-step process requiring a Connectionist Temporal Character Classification (CTC)(Graves et al., 2006) model and Language Model (LM) rather than a three-step process requiring an Acoustic model(AM), LM and phonetic dictionary. Employing a two-step process rather than a three-step process is particularly desirable for low resource languages as less effort is required developing fewer simplified models. 
\fs24 \

\fs32 Earlier in chapter one, deep learning was defined as a type of representational learning whereby different levels of complexity are captured in internal layer-wise encapsulations. It has also been noted that layer-wise stacking of neural and neu- ral network type architectures such as the Restricted Boltzmann Machine (RBM) deep belief networks (DBMs) were used to implement such representations. In this chapter, the end-to-end Bi-directional Recurrent Neural Network model is described. Here, the development of the features using the deep scattering convolution network is first elaborated on. The model parameters and architecture are described and the decoding algorithm are all detailed in sections contained within this chapter. 
\fs24 \

\fs32 114 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 7.1 Deep Scattering Features 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The fast wavelet transform is derived in Chapter 5 from a low pass filter and a high pass filter. The speech features used in this research using a deep scattering network 2 layers deep was created using the wavelet modulus operator comprising a low pass filter and a band pass filter. Hyper parameters of the system included the window period for each sampled sub section, T; The Q-band value for the band pass filter and the number of wavelets J at each scattering layer for the total number of layers, M = 2. 
\fs24 \

\fs32 The matlab scatnet toolbox (And\uc0\u32 \u769 en et al., 2014), used to determine the scatter coefficient features for this research, provides optimal values for hyper parameters for audio signal processing into scatter features. In this regime the value for the hyper parameter T = 512 samples per window. This corresponds to a window of 50milliseconds for the audio signals sampled at 8000Hz. For the first scattering layer the Q-band parameter was Q = 8 and the second scattering layer took the value Q = 1. Finally J is pre-calculated based on the value of T. These after Scat-Net processing produce a feature-vector having 165 dimensions. These feature vectors in turn are used as inputs to the bi-direction neural network model whose architecture is described in succeeding sections. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 7.2 CTC-BiRNN Architecture 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The core of the system is a bidirectional recurrent neural network (BiRNN) trained to ingest scatter coefficients described in the previous section, in order to generate English text transcriptions. An end-to-end system therefore specifies that utterances x and the corresponding label y be sampled from a training set such that the sample S = (x
\fs21\fsmilli10667 \up10 (1)
\fs32 \up0 , y
\fs21\fsmilli10667 \up10 (1)
\fs32 \up0 ), (x
\fs21\fsmilli10667 \up10 (2)
\fs32 \up0 , y
\fs21\fsmilli10667 \up10 (2)
\fs32 \up0 ), .... In our end-to-end model, each utterance, x
\fs21\fsmilli10667 \up10 (i) 
\fs32 \up0 is a processed feature vector consisting of 165 dimensions. Recall, every window passes through a scattering transform to yield an input of vector of p = 165 features; consequently, x
\fs21\fsmilli10667 \up16 (i) 
\fs32 \up0 denotes the p-th feature in a scatter transform at time t. 
\fs24 \

\fs32 GPU training of the speech model architecture developed above was conducted using Mozilla deepspeech (moz, 2019a) CTC bi-directional RNN implementation along with the accompanying Mozilla Common voice dataset (moz, 2019b). The 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 t,p 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 7 I. J. Alamina 115 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Common Voice Dataset project consists of voice samples in short recordings ap- proximately 4 seconds each. The complete dataset is about 250 hours of record- ing divided into training, test and development subsets. The BiRNN, given the input sequence, x, outputs a sequence of probabilities y
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 = P(c
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 |x), where c
\fs21\fsmilli10667 \dn6 t 
\f2\fs32 \up0 \uc0\u8712 
\f0  a, b, c, . . . , z, space, apostrophe, blank. 
\fs24 \

\fs32 The actual architecture of our core Bi-RNN is similar to the deepspeech system described in Hannun et al. (2014a). This structure constitutes 5 hidden layers and one output layer. The first three layers are regular DNNs followed by a bi-directional recurrent layer. As such, the output of the first three layers are computed by: 
\fs24 \

\fs32 h
\fs21\fsmilli10667 \up16 (l) 
\fs32 \up0 = g(W 
\fs21\fsmilli10667 \up13 (l)
\fs32 \up0 h
\fs21\fsmilli10667 \up16 (l1) 
\fs32 \up0 + b
\fs21\fsmilli10667 \up13 (l)
\fs32 \up0 ) (7.1) 
\fs21\fsmilli10667 tt 
\fs24 \

\fs32 g(\'b7) = min\{max\{0,z\},20\} is the clipped rectified linear unit and W
\fs21\fsmilli10667 \up10 (l)
\fs32 \up0 ,b
\fs21\fsmilli10667 \up10 (l) 
\fs32 \up0 are weight matrix and bias parameters for layer as described in sections 4.2.1 and 4.3 respectively. 
\fs24 \

\fs32 It was shown in chapter 4 the recurrent layer comprise a forward and backward RNNs whose equations are repeated here for reference 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \dn14 h
\fs21\fsmilli10667 \up2 (f) 
\fs32 \dn14 = g(W
\fs21\fsmilli10667 \up0 (4)
\fs32 \dn14 h
\fs21\fsmilli10667 \up2 (3) 
\fs32 \dn14 + W
\fs21\fsmilli10667 \up0 (f)
\fs32 \dn14 h
\fs21\fsmilli10667 \up2 (f) 
\fs32 \dn14 + b
\fs21\fsmilli10667 \up0 (4)
\fs32 \dn14 ) (7.2) 
\fs21\fsmilli10667 \up0 t trt1 
\fs24 \

\fs32 \dn14 h
\fs21\fsmilli10667 \up2 (b) 
\fs32 \dn14 = g(W 
\fs21\fsmilli10667 \up0 (4)
\fs32 \dn14 h
\fs21\fsmilli10667 \up2 (3) 
\fs32 \dn14 + W 
\fs21\fsmilli10667 \up0 (b)
\fs32 \dn14 h
\fs21\fsmilli10667 \up2 (b) 
\fs32 \dn14 + b
\fs21\fsmilli10667 \up0 (4)
\fs32 \dn14 ) (7.3) 
\fs21\fsmilli10667 \up0 t trt+1 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Consequently, h
\fs21\fsmilli10667 \up10 (f ) 
\fs32 \up0 is the sequential computation from t = 1 to t = T 
\fs21\fsmilli10667 \up10 (i) 
\fs32 \up0 for the i- th utterance and h
\fs21\fsmilli10667 \up10 (b) 
\fs32 \up0 is the reverse computation from t = T 
\fs21\fsmilli10667 \up10 (i) 
\fs32 \up0 to t = 1. In addition the output from layer five is summarily given as the combined outputs from the recurrent layer: 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 h
\fs21\fsmilli10667 \up13 (5) 
\fs32 \up0 = g(W 
\fs21\fsmilli10667 \up13 (5)
\fs32 \up0 h
\fs21\fsmilli10667 \up13 (4) 
\fs32 \up0 + b
\fs21\fsmilli10667 \up13 (5)
\fs32 \up0 ) (7.4) where h = h +h . The output of the Bi-RNN on layer 6 is a standard soft-max 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 (4) (f) (b)\uc0\u8232 
\fs32 layer that outputs a predicted character over probabilities for each time slice t and 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 116 Chapter 7 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page117image6942704.png \width7937 \height3581 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Figure 7.1: Deep scattering Bi-RNN Model character k in the alphabet: 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \dn11 exp
\f1 \up24 \uc0\u56319 \u56323 
\f0 \dn11 (W
\fs21\fsmilli10667 \up0 (6)
\fs32 \dn11 h
\fs21\fsmilli10667 \up5 (5)
\fs32 \dn11 ) +b
\fs21\fsmilli10667 \up5 (6)
\f1\fs32 \up24 \uc0\u56319 \u56324 
\f0  
\fs24 \up0 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 b
\fs21\fsmilli10667 \up16 (6) 
\fs32 \up0 takes on the -th bias and (W
\fs21\fsmilli10667 \up10 (6)
\fs32 \up0 h
\fs21\fsmilli10667 \up16 (5)
\fs32 \up0 ) is the matrix product of the k-th 
\fs21\fsmilli10667 \dn3 k\up0 t\up2 k 
\fs24 \up0 \

\fs32 element. The error of the outputs are then computed using the CTC loss function Graves (2014) as described in chapter 4. A summary of our model is illustrated in Figure 7.1. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 7.3 CTC Decoding 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 In chapter three the CTC loss function algorithm was established as being able to maximise the probability of two cases. The first case of transiting to a blank and the second case of transiting to a non blank. In this section, this concept is used to enable decoding of the network output from posterior distribution output to character sequences which can be measured against a reference transcription using either character error rate (CER) or word error rate (WER). 
\fs24 \

\fs32 Recall, all the output symbols are in the alphabet \uc0\u931  and augmented with the blank symbol. The posterior output of the CTC network is the probability of the symbol given the speech feature input p(c|x
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 ) at time t for t = 1,...,T and T is the length of the input sequence. Also recall two further sets of probabilities also being 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 (6) \up10 t\up13 k\up8 k\up0 \uc0\u8232 
\fs32 h =y\'88 
\f2 \uc0\u8801 
\f0 P(c=k|x)= 
\f1 \up2 \uc0\u56319 \u56323 
\f0  
\f1 \uc0\u56319 \u56324 
\f0 \up0 ) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 (7.5) 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 \up8 t,k \up13 t,k t 
\f1\fs32 \up8 \uc0\u56319 \u56333 
\f0  \dn16 exp (W
\fs21\fsmilli10667 \dn8 (6)
\fs32 \dn16 h
\fs21\fsmilli10667 \up0 (5)
\fs32 \dn16 ) +b
\fs21\fsmilli10667 \up0 (6) jt\up2 j\up0 j 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page117image9146880.png \width1935 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 7 I. J. Alamina 117 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 maintained by the model are the probability of a blank character p
\fs21\fsmilli10667 \dn6 b 
\fs32 \up0 and that of a non blank character p
\fs21\fsmilli10667 \dn6 nb
\fs32 \up0 . 
\fs24 \

\fs32 Several strategies have been employed to obtain a translation string from the output of the deep neural network. The prefix beam search employed by the CTC decoder of this research is derived from an initial greedy approximation, where at each time step determine the argument that maximises the probability p(c|x
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 ) at each time step. Let C = (c
\fs21\fsmilli10667 \dn6 1
\fs32 \up0 , . . . , c
\fs21\fsmilli10667 \dn6 T 
\fs32 \up0 be the character string then, the greedy approach has 
\fs24 \

\fs32 c
\fs21\fsmilli10667 \dn6 t 
\fs32 \up0 =argmaxp(c|x
\fs21\fsmilli10667 \dn6 t
\fs32 \up0 ) (7.6) 
\fs21\fsmilli10667 c
\f2 \uc0\u8712 
\f0 \uc0\u931  
\fs24 \

\fs32 However, this simple approximation is unable to collapse repeating sequences and remove blank symbols. In addition, the approximation is unable to include the constraint of a lexicon or language model. 
\fs24 \

\fs32 The prefix beam search algorithm Hannun et al. (2014b) adopted in this work incorporates a language model derived from a lexicon in addition to keeping track of the various likelihoods used for decoding. For the language model constraint, the transcription W is recovered from acoustic input X at time t by choosing the word which maximising the posterior probability: 
\fs24 \

\fs32 W
\fs21\fsmilli10667 \dn6 i 
\fs32 \up0 =arg max p
\fs21\fsmilli10667 \dn6 net
\fs32 \up0 (W;X)p
\fs21\fsmilli10667 \dn6 lm
\fs32 \up0 (W) (7.7) 
\fs21\fsmilli10667 W
\fs16 \dn3 i 
\f2\fs21\fsmilli10667 \up0 \uc0\u8712 
\f0 \uc0\u931 
\fs16 \dn3 W 
\fs24 \up0 \

\fs32 In equation 7.7, the Bayes product of language model prior p
\fs21\fsmilli10667 \dn6 lm 
\fs32 \up0 and the network output p
\fs21\fsmilli10667 \dn6 net 
\fs32 \up0 are utilised to maximise the probability of a particular character-word sequence in the lexicon given by \uc0\u931 
\fs21\fsmilli10667 \dn6 W 
\fs32 \up0 . The overall calculation used to derive the final posterior distribution includes word insertion factors (\uc0\u945  and \u946 ) used to balance the highly constrained n-gram language model. 
\fs24 \

\fs32 The second strategy adopted by the prefix beam search which improves the decoding algorithm is the beam search strategy. With this approach, the search maintains all possible paths; however, it retains only k number paths which maximise the output sequence probability. Improvements gained with this method are seen when certain maximal paths are made obsolete owing to new information derived from the multiple paths in being maintained in memory. 
\fs24 \

\fs32 The recursive prefix beam search algorithm illustrated in Figure 7.2 attempts to 118 Chapter 7 I. J. Alamina 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 find the string formulated in equation 7.7. Two sets prefixes A
\fs21\fsmilli10667 \dn6 prev 
\fs32 \up0 and A
\fs21\fsmilli10667 \dn6 nxet 
\fs32 \up0 are initialised, such that at A
\fs21\fsmilli10667 \dn6 nxet 
\fs32 \up0 maintains the prefixes in the current time-step while A
\fs21\fsmilli10667 \dn6 prev 
\fs32 \up0 maintains only k-prefixes from the previous time-step. Note that at the end of each time step A
\fs21\fsmilli10667 \dn6 prev 
\fs32 \up0 is updated with only -most probable prefixes from A
\fs21\fsmilli10667 \dn6 nxet
\fs32 \up0 . Therefore while, A
\fs21\fsmilli10667 \dn6 nxet 
\fs32 \up0 contains all the possible new paths from based on A
\fs21\fsmilli10667 \dn6 prev 
\fs32 \up0 as a Cartesian product of A
\fs21\fsmilli10667 \dn6 prev 
\fs32 \up0 \'d7 \uc0\u931  
\f2 \uc0\u8712 
\f0  Z
\fs21\fsmilli10667 \up10 k 
\fs32 \up0 \'d7 Z
\fs21\fsmilli10667 \up10 |\uc0\u931 | 
\fs32 \up0 where |\uc0\u931 | is the length of \u931 . The probabilities of each prefix obtained at each time step are the sum of the probability of non-blank plus the probability of a blank symbol. 
\fs24 \

\fs32 At every time step and for every prefix l currently in A
\fs21\fsmilli10667 \dn6 prev
\fs32 \up0 , a character from the alphabet \uc0\u931  is presented to the prefix. The prefix is only extended only when the presented symbol is not a blank or a space. A
\fs21\fsmilli10667 \dn6 nxet 
\fs32 \up0 and A
\fs21\fsmilli10667 \dn6 prev 
\fs32 \up0 maintain a list of active prefixes at the previous time step and proposed prefixes at the next time step respectively, The prefix probability is given by multiplying the word insertion term by the sum of the blank and non-blank symbol probabilities. 
\fs24 \

\fs32 p(l|x
\fs21\fsmilli10667 \dn6 1:t
\fs32 \up0 ) = (p
\fs21\fsmilli10667 \dn6 nb
\fs32 \up0 (l|x
\fs21\fsmilli10667 \dn6 1:t
\fs32 \up0 ) + p
\fs21\fsmilli10667 \dn6 b
\fs32 \up0 (l|x
\fs21\fsmilli10667 \dn6 1:t
\fs32 \up0 ))|W(l)|
\fs21\fsmilli10667 \up13 \uc0\u946  
\fs32 \up0 (7.8) 
\fs24 \

\fs32 W (\'b7) is obtained by segmenting all the characters in the sequence with the space- character symbol and truncating any characters trailing the set of words in the sequence. The prefix distribution however varies slightly depending on network output character being presented. 
\fs24 \

\fs32 l
\fs21\fsmilli10667 \dn6 end 
\fs32 \up0 is the variable representing the last symbol in the prefix sequence in A
\fs21\fsmilli10667 \dn6 prev
\fs32 \up0 . If the symbol presented is the same as l
\fs21\fsmilli10667 \dn6 end 
\fs32 \up0 then the probability of a non-blank symbol,p
\fs21\fsmilli10667 \dn6 nb 
\fs32 \up0 = 0 . If the symbol being presented is blank then we do not extend the prefix. Finally, if the symbol being presented is a space then we invoke the language model as follows 
\fs24 \

\fs32 p(l
\fs21\fsmilli10667 \up13 +
\fs32 \up0 |x
\fs21\fsmilli10667 \dn6 1:t
\fs32 \up0 ) = p(W(l
\fs21\fsmilli10667 \up13 +
\fs32 \up0 )|W(l))
\fs21\fsmilli10667 \up13 \uc0\u945 
\fs32 \up0 (p
\fs21\fsmilli10667 \dn6 nb
\fs32 \up0 (l|x
\fs21\fsmilli10667 \dn6 1:t
\fs32 \up0 ) + p
\fs21\fsmilli10667 \dn6 b
\fs32 \up0 (l|x
\fs21\fsmilli10667 \dn6 1:t
\fs32 \up0 ))|W(l)|
\fs21\fsmilli10667 \up13 \uc0\u946  
\fs32 \up0 (7.9) 
\fs24 \

\fs32 Note that p(W(l
\fs21\fsmilli10667 \up10 +
\fs32 \up0 )|W(l)) is set to 0 if the current word W(l
\fs21\fsmilli10667 \up10 +
\fs32 \up0 ) is not in the lexicon. This becomes a constraint to enforce all character strings to consist only of words in the lexicon. Furthermore, p(W(l
\fs21\fsmilli10667 \up10 +
\fs32 \up0 )|W(l)) is extended to include all the character sequences representing number of words considered by the n-gram 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 7 I. J. Alamina 119 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 language model by constituting the last n \uc0\u8722  1 words in character sequence W (l). 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 7.4 Model Hyper parameters 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The hidden layer matrix for each layer comprised 1024 hidden units (6.6M free parameters). The weights are initialised from a uniform random distribution having a standard deviation of 0.046875. The Adam optimisation algorithm (Kingma and Ba, 2014) was used with initial learning rate of, and a momentum of 0.95 was deployed to optimise the learning rate. 
\fs24 \

\fs32 The network was trained for a total of five to fifty epochs over the training set for experiments conducted. The training time for Python GPU implementation is shown in Table 7.1. For decoding with prefix search we use a beam size of 200 and cross-validated with a held-out set to find optimal settings for the parameters \uc0\u945  and \u946 . Figure 7.4 shows word error rates for various GPU configurations and audio data-set sizes. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 7.5 Model Baseline 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The study by Hannun et al. (2014b) reported successful character error rate (CER) using deep neural network (DNN), recurrent deep neural network with only forward temporal connections (RDNN), and also bi-directional recurrent neural networks (BRDNN). The models used in this their study had 5 hidden layers having either 1,824 or 2,048 hidden units in each hidden layer. For a baseline, the model produced by the Mozilla DeepSpeech team was adopted. This model had a similar architecture with 5 hidden units and 2048 hidden units and was trained on the Librespeech corpus and the common voice data corpora (moz, 2019a, Panayotov et al., 2015). 
\fs24 \

\fs32 Word Error Rates by this model were optimised after 75 epochs, learning rate of 0.0001 and a dropout rate of 15%. In addition, the language model hyper parameters for alpha and beta were 0.75 and 1.85 respectively. This achieved 8% WER. This model was developed using MFCC features of the training corpus. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 120 Chapter 7 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and 
\fs24 \

\fs32 Experiment 
\fs24 \

\fs32 1. 2xGPU 10GB RAM 2. 2xGPU 10GB RAM 3. 5xGPU 15GB RAM 4. 5xGPU 15GB RAM 5. 1xCPU 16GB RAM 6. 1xGPU 2GB RAM\uc0\u8232 7. 1xCPU 16GB RAM 8. 1xCPU 16GB RAM 9. ESPNet 1xGPU 2GB 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 7.6 Results 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Table 7.1: GPU Experiments 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page121image9147456.png \width8680 \height20 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Hours 
\fs24 \

\fs32 of speech 
\fs24 \
\pard\pardeftab720\sl360\partightenfactor0

\fs32 \cf2  1\
10\
10\
40\
20\
20\
20\
20\
 1\
\pard\pardeftab720\sl360\sa240\partightenfactor0
\cf2 Total training time 
\fs24 \

\fs32 7 days 355 days 17 hours 12 days 4+ days 17+ days 4+ days 4+ days 1 hour 
\fs24 \

\fs32 Estimated training 
\fs24 \

\fs32 Completed Completed Completed Completed 70 days 100 days 70 days 
\fs24 \

\fs32 70 days Completed 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page121image9165952.png \width6852 \height15 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page121image9147456.png \width8680 \height20 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Experiments were carried out on different GPU configurations. A set of experi- ments was performed a GPU configuration consisting of 2 GPUs having a total of 10 gigabytes of memory. The second set of experiments was carried out on a GPU configuration comprising 5 GPUs having a total of 15 gigabytes of memory. Ex- periments were also performed using single GPUs having 2GB and another single GPU having 8GB. For each GPU configuration experiments were carried out on varying-size subsets of the common voice corpus being utilised. The various GPU configurations along with the training times are shown in Table 7.1. 
\fs24 \

\fs32 In addition to the GPU configuration, experiments involving CPU and multi- node training were carried out. Although quite a number of configuration did not reach a stopping condition, the multi node configurations which made use of Ten- sorflow distributed feature in particularly required regular user-intervention, and as such, was short-lived. In table 7.1, the first four configurations trained to saturation. For these results, the training loss reduced significantly once the data was increased to ten hours of training. However word error rates (WER) only showed improve- ment on the 40 hours data set. This seems to indicate that a threshold of about 40 hours is required for the model to begin to converge for a Large Scale Vocabulary Continuous Speech Recognition (LSVCSR) system 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 7 I. J. Alamina 121 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Table 7.2: Summary of GPU Experiments 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page122image9136832.png \width7289 \height20 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Experiment Hours of speech 
\fs24 \

\fs32 1. 2xGPU 10GB RAM 1 2. 2xGPU 10GB RAM 10 3. 5xGPU 15GB RAM 10 4. 5xGPU 15GB RAM 40 5. ESPNet 1xGPU 2GB 1 
\fs24 \

\fs32 Corpus 
\fs24 \

\fs32 CV LVCSR CV LVCSR CV LVCSR CV LVCSR AN4 MVCSR 
\fs24 \

\fs32 Metric Score 
\fs24 \

\fs32 WER(%) 100+ WER(%) 100+ WER(%) 100 WER(%) 87 
\fs24 \

\fs32 CER(%) 9.5 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page122image9137024.png \width5760 \height15 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page122image9136832.png \width7289 \height20 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 7.7 Preliminary ESPNet Experiment 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Preliminary experiments were carried out using the ESPNet (Watanabe et al., 2018) an overview of which is described in Chapter 3 and detailed some more in this sec- tion and Chapter ??. A much smaller audio corpus guaranteed to converge however was used for these experiments. The AN4 (alphanumeric) corpus by Carnegie Mel- lon University (Acero, 1990), is a small vocabulary speech corpus having only 948 training utterances and 140 test utterances. 
\fs24 \

\fs32 The corpus utterances are 16-bit linearly sampled at 16kHz, each recording made with near-field microphone quality. The compressed tar file comes with a variety of audio formats including raw wav format, the NIST sphere format and those already encoded as Mel cepstral coefficients. 
\fs24 \

\fs32 Experiments were carried out using ESPNet default parameters which included those for character based-Recurrent Neural Network language model RNN-LM, multi-channel feature input and multi-objective learning using both CTC-Transformer and Attention-Transducer networks. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 7.7.1 ESPNet Speech model architecture, parameters and results 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The end-to-end architecture at the core of ESPNet is the CTC-Transformer+Attention Transducer model. Together these two architectures achieve joint multi-objective speech training and decoding. The CTC-Transformer model is based on a Bi-RNN similar to what is obtainable in the DeepSpeech model. The attention transducer model is further explained in Chapter 8. There are up to 11 variants of Attention networks implemented in ESPNet, however, the results of the ESPNet experiment 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 122 Chapter 7 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 performed was determined from the model described in Chorowski et al. (2015). Moreover, the multi-objective training was performed with equal weights on both the CTC-transformer and the Attention-Transducer. Finally the system was trained for 20 epochs only. 
\fs24 \

\fs32 With this minimal default setting, the test set had a final recognition score of 9.5% character error rate (CER). The next Chapter discusses how the baseline can be scaled and remodelled for integrating scattering features. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 7.8 Chapter Summary 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 In this chapter the details of the novel structure having the end-to-end deep bi- RNN architecture and deep scattering features were elaborated on. The architecture which follows a five-layer structure consisting of a feed-forward neural network in the first three layers and the last two consisting of recurrent structures flowing in two different directions. The network is then fed in with a 165-dimension feature vector containing deep-scattering encoding derived from a sampled raw audio file. 
\fs24 \

\fs32 The results showed that the training of the model was moving towards a very slow convergence as indicated by the slow decrements in training loss. However, we speculate that on the complete data-set, the model will not only converge but show improvements in word error rates. Already this is seen from results from preliminary baseline experiments with ESPNet. With an advanced architecture, yet having the CTC-Transformer Bi-RNN at its core, the ESPNet baseline model yielded a competitive 9.5% CER using multi-channel features derived from features integrated with 83 log MFCC feature vector. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 7 I. J. Alamina 123 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page124image6921328.png \width9367 \height12471 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 124 Chapter 7 I. J. Alamina 
\fs24 \

\fs32 Figure 7.2: Prefix beam search algorithm 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page125image6987280.png \width7937 \height3670 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Figure 7.3: Training Loss, where w < x < y < z are taken arbitrarily across the total number of epochs 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page125image6988112.png \width7937 \height3193 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Figure 7.4: WER, where wxyz are taken arbitrarily across the total number of epochs 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 7 I. J. Alamina 125 
\fs24 \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 Chapter 8\uc0\u8232 Conclusion and Future Work 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The advancement of Machine Learning has a direct impact on the development of more efficient speech recognition algorithms and at the same time the advancement of speech recognition helps with the improvement of Machine Learning algorithms, as in general, the methods used in Machine Learning usually are directly transferable to speech processing and vice-versa. This mutual relationship implies that speech recognition is a blossoming research field because there is a tremendous amount of work being done in the Machine Learning community. Particularly in the area of deep learning and neural networks, there is quite a vast array of neural network solutions that have been applied or are yet to be applied to speech recognition. Two models worthy of mentioning are Generative Adversarial Networks (GANs) and Attention-based models. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 8.1 Generative adversarial networks (GAN) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 GANs consists of two Networks working as adversaries to one another. The first, being a generative network, generates content. The second network is a discrimi- native network intended to determine the accuracy of the first generative network. Hence the generative network is generating output less distinguishable for the dis- criminator while the discriminator uses output from the generator to improve the ability to discriminate output from the generator with the original source data. 
\fs24 \

\fs32 GAN networks can have applications where the generative network consists of a speech synthesis network and the discriminating network is a speech recogniser. 
\fs24 \

\fs32 126 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 However successive training of these two networks from a data-resource perspective would require an immense amount of data resources for expected performances. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 8.2 Attention-based Models 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The objective of attention-based networks highlighted by Vaswani et al. (2017) is to reduce sequential computation while attaining hidden representation across arbi- trary lengths of sequential input. Mechanisms which have been deployed to achieve this includes a combination of convolutional and recurrent schemes (Gehring et al., 2017, Kaiser and Bengio, 2016, Kalchbrenner et al., 2016). Vaswani et al. (2017) introduces a transduction model known as a Transformer based on self attention net- work with the ability to compute long term dependencies while eliminating sequence aligned RNN and convolutional architectures. 
\fs24 \

\fs32 Self attention is a network that intrinsically reduces the need for intensive re- source training. Vaswani et al. (2017) reports that state of the art BLEU score of 41.0 having used a small fraction of training resources. While GANs might not be attractive for low resource speech recognition, they still remain an important tool for verification of the output of other networks. At the same time self attention networks can help to reduce the resource requirements of GANs when used within the context of a GAN. 
\fs24 \

\fs32 As a study to further this thesis, these networks are likely candidates for network training using scatter features as input discriminatory functions. Attention based networks as a means reduce training resources required, while GANs can be used as a means to generate training data. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 8.3 Joint Training with ESPNet 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 7 has already seen promising results based on preliminary experiments using the ESPNet model. This model does fulfil major objectives outlined by this research. The inclusion of Attention-based models and the warp-CTC decoder implemented in ESPNet ensures faster convergence and ultimately faster time to train, in addition to the end-to-end architecture presented by ESPNet. The next step required for a further study is an integration of scattering features into ESPNet. This scattering 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 8 I. J. Alamina 127 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 features implementation is the subject of a paper publication immediately sought after based on this work. The features of ESPNet include state-of-the-art end-to-end architectures including variants of Attention-Transducers and CTC-decoders. Using a weighting function one can control how much bias either the CTC-Transform or the Attention-Transducer will get during training. The joint training helps to improve robustness as well as achieve fast convergence. 
\fs24 \

\fs32 L = \uc0\u945 L
\fs21\fsmilli10667 \up13 ctc 
\fs32 \up0 + (1 \uc0\u8722  \u945 )L
\fs21\fsmilli10667 \up13 att 
\fs32 \up0 (8.1) 
\fs24 \

\fs32 At the same time joint decoding of labels is integrated with the character based RNN-language modelling. The log probability of the RNNLM-integrated decoding of character labels is as follows 
\fs24 \

\fs32 log p(y
\fs21\fsmilli10667 \dn6 n
\fs32 \up0 |y
\fs21\fsmilli10667 \dn6 1:n1
\fs32 \up0 , h
\fs21\fsmilli10667 \dn6 1:T \'91
\fs32 \up0 ) = log p
\fs21\fsmilli10667 \up13 hyp
\fs32 \up0 (y
\fs21\fsmilli10667 \dn6 n
\fs32 \up0 |y
\fs21\fsmilli10667 \dn6 1:n1
\fs32 \up0 , h
\fs21\fsmilli10667 \dn6 1:T \'91
\fs32 \up0 ) + \uc0\u946  log p
\fs21\fsmilli10667 \up13 lm
\fs32 \up0 (y
\fs21\fsmilli10667 \dn6 n
\fs32 \up0 |y
\fs21\fsmilli10667 \dn6 1:n1
\fs32 \up0 ) (8.2) Where joint decoding, log p
\fs21\fsmilli10667 \up10 hyp
\fs32 \up0 (y
\fs21\fsmilli10667 \dn6 n
\fs32 \up0 |y
\fs21\fsmilli10667 \dn6 1:n1
\fs32 \up0 , h
\fs21\fsmilli10667 \dn6 1:T \'91
\fs32 \up0 ) is given by 
\fs24 \

\fs32 log p
\fs21\fsmilli10667 \up13 hyp
\fs32 \up0 (y
\fs21\fsmilli10667 \dn6 n
\fs32 \up0 |y
\fs21\fsmilli10667 \dn6 1:n1
\fs32 \up0 , h
\fs21\fsmilli10667 \dn6 1:T \'91
\fs32 \up0 ) = \uc0\u945  log p
\fs21\fsmilli10667 \up13 ctc
\fs32 \up0 (y
\fs21\fsmilli10667 \dn6 n
\fs32 \up0 |y
\fs21\fsmilli10667 \dn6 1:n\uc0\u8722 1
\fs32 \up0 , h
\fs21\fsmilli10667 \dn6 1:T 
\f2\fs16 \up0 \uc0\u8242 
\f0  
\fs32 ) + (1 \uc0\u8722  \u945 ) log p
\fs21\fsmilli10667 \up13 att
\fs32 \up0 (y
\fs21\fsmilli10667 \dn6 n
\fs32 \up0 |y
\fs21\fsmilli10667 \dn6 1:n\uc0\u8722 1
\fs32 \up0 , h
\fs21\fsmilli10667 \dn6 1:T 
\f2\fs16 \up0 \uc0\u8242 
\f0  
\fs32 ) (8.3) Furthermore, multi-channel training integrates noise robust and far-field speech recognition tasks which can accommodate joint 83-dimension MFCC and scatter transform training in addition to speech enhancement features such as beam form- ing and STFT masking Ochiai et al. (2017). Both single and multi-channel scatter 
\fs24 \

\fs32 transform features are proposed as subjects in the future paper publication. 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 8.4 Model Pretraining 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The major setback this work suffered was from a reasonable time for training. This work therefore recommends that speech models or indeed artificial intelligence mod- els should be trainable within a maximum of 30 days and shouldn\'92t generally exceed 20 days training. 
\fs24 \

\fs32 An area of neural network training optimisation not developed in this report is that of layer-wise greedy pre-training. In this process, rather than train the deep 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 128 Chapter 8 I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 neural network structure all in one stage, the network layers are successively added and trained layer by layer, one layer at a time (Goodfellow et al., 2016). The intuition behind this is that this makes the layers saturate much faster as the previous layer has already been saturated before the new layer is being added. 
\fs24 \

\fs32 This layer-wise pretraining procedure is thought to speed up training, than train- ing when done with the fully connected network and there have been a few different approaches to pretraining in for deep neural network architectures for speech recog- nition. 
\fs24 \

\fs32 Hendrycks et al. (2019) introduces an advanced pretraining method in which existing models are retrained in a Generative Adversarial Network (GAN) fashion in order to optimise performance and model robustness. This is an instance where GANs are being deployed in speech recognition. This method however is not likely going to help improve model training convergence time. 
\fs24 \

\fs32 Another method described in (Ramachandran et al., 2016, ?) uses a knowledge transfer mechanism where hidden layers in an already existing related network is re-trained with new extended layers to complete training in the new domain. The effectiveness of such a transfer method will be measured of the how the two domains correlate with one another. It therefore, would be logical to conclude that the more the domains correlate the faster the pretraining model is likely to converge faster. 
\fs24 \

\fs32 Finally, Wang et al. (2019) proposes a Tandem Connectionist Encoding Network (TCEN) for bridging the gap for fine tuning CTC-Transformer networks along with pretraining of Attention-transducers. 
\fs24 \

\fs32 As a further study, amongst other techniques, the most viable method for this study is to investigate the knowledge transfer mechanism by approaching the feature engineering problem as a latent space analysis problem. Given that during the process of mapping acoustic speech sequences to the MFCC reduced the latent space from a high dimension to a low dimension. It is reasonable therefore to hypothesize that training from hidden layers of an MFCC deep RNN would converge faster than weights initialised through generic means. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter 8 I. J. Alamina 129 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 8.5 Conclusion 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 End-to-end discriminative neural network speech models have now become a well established method in Automatic Speech Recognition. 
\fs24 \

\fs32 Bi-directional Recurrent neural network (Bi-RNN) end-to-end system, is aug- mented by features derived from a deep scattering network as opposed to the stan- dard Mel Frequency Cepstral Coefficients(MFCC) features used in state of the art acoustic models. These specialised deep scattering features, consumed by the Bi- RNN, model a light-weight convolution network. This work shows that it is possible to build a speech model from a combination of deep scattering features and a Bi- RNN. There has been no record of deep scattering features being used in end-to-end bi-RNN speech models as far as we are aware. 
\fs24 \

\fs32 This thesis shows that Deep Scattering features derived from wavelet filter oper- ations on audio data produce viable candidates for end-to-end training of Automatic speech recognition models. 
\fs24 \

\fs32 The objective of this research to facilitate fast and efficient speech recognition is achieved using the ESPNet system. The major advantage of this system is that in addition to robust end-to-end models is the intrinsic integration of a character-based language model enabling it to satisfy both low-resource challenge criteria of top level word and sentence modelling through the character-based language model and the sub-word and acoustic modelling of input features. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 130 Chapter 8 I. J. Alamina 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 Appendix I - Haar Wavelet 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 A fundamental purpose of analysing functions such as the Fourier and wavelet func- tions are the reconstruction of signals from it\'92s decomposition. Certain criteria or properties are therefore required for analysis functions. 
\fs24 \

\fs32 In Chapter 5.1, the orthogonal properties of the Fourier transform equations was introduced. In the case of wavelets the following properties ensue. In addition to orthogonal properties, wavelets are required to perform localised analysis of a function. Hence, unlike their Fourier counterparts, they need to be bounded in time. It is also seen that when the energy contained within the wavelet bases sum to zero (sometimes normalised to 1)i.e. 
\fs24 \

\fs32 tion forms a recurrence relation which is a solution to the dilation equation as follows: \uc0\u966 (t) = 
\f1 \up29 \uc0\u56319 \u56335 
\f0  \up0 c
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 \uc0\u966 (2t \u8722  k) (5) 
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 k 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Where \uc0\u966 (2t \u8722  k) is a contracted version of \u966 (t) shifted along the time axis by an integer step k and factored by an associated coefficient c
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 . At the same time it is also observed that it is possible to setup this recurrence relation to becoming dyadic such that the sum of coefficients, c
\fs21\fsmilli10667 \dn6 k 
\fs32 \up0 equals 2, i.e. \uc0\u931 
\fs21\fsmilli10667 \dn6 k
\fs32 \up0 c
\fs21\fsmilli10667 \dn6 k 
\fs32 \up0 = 2. Haar, wavelets constitute the simplest of this family of wavelets. 
\fs24 \

\fs32 The mother wavelet of the Haar wavelet has only two coefficients c
\fs21\fsmilli10667 \dn6 0 
\fs32 \up0 = c
\fs21\fsmilli10667 \dn6 1 
\fs32 \up0 = 1 and is given by: 
\fs24 \

\fs32 \uc0\u968 (t) = \u966 (2t) + \u966 (2t \u8722  1) (6) 
\fs24 \

\fs32 Observe here that c
\fs21\fsmilli10667 \dn6 0 
\fs32 \up0 + c
\fs21\fsmilli10667 \dn6 1 
\fs32 \up0 = 2, i.e. dyadic. The solution to this recurrence equation and the resulting plot is given in Figure 1. 
\fs24 \

\fs32 Through multi-resolution analysis, the following reconstruction of the Haar wavelet is derived: 
\fs24 \

\fs32 \uc0\u966 
\fs21\fsmilli10667 \dn6 j,k
\fs32 \up0 [n] = 2
\fs21\fsmilli10667 \up13 j/2
\fs32 \up0 \uc0\u966 [2
\fs21\fsmilli10667 \up13 j
\fs32 \up0 n \uc0\u8722  k] (7) The parameter j, controls the resolution of the signal reconstruction and the 
\fs24 \

\fs32 following wavelets and function representation are given in Figure 2 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f1\fs32 \cf2 \up5 \uc0\u56319 \u56337 
\f0\fs21\fsmilli10667 \dn6 \uc0\u8734  \up0 \uc0\u8722 \u8734  
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 |x|
\fs21\fsmilli10667 \up13 2
\fs32 \up0 dt = ||x(t)||
\fs21\fsmilli10667 \up13 2 
\fs32 \up0 = 0 (4) Then, such wavelet bases are orthonormal and the fundamental or scaling equa- 
\fs24 \

\fs32 E = 
\fs24 \

\fs32 131 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page132image6939376.png \width7930 \height7108 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Figure 1: Haar wavelet 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page132image6938752.png \width7924 \height4460 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Figure 2: Multi resolution analysis of Haar wavelets 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 132 Chapter I. J. Alamina 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 Appendix II - Gabor and Morlet Wavelets 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 Gabor Wavelet filter 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 The 1D-Gabor wavelet is defined spatially by 
\fs24 \

\fs32 The Fourier-transform is 
\fs24 \

\fs32 \uc0\u968 (x) = exp \u8722  
\fs24 \

\fs32 \'88\uc0\u8232 \u968 (\u969 ) = exp 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f1\fs32 \cf2 \up18 \uc0\u56319 \u56325 
\f0 \dn6 x
\fs21\fsmilli10667 \up5 2 
\f1\fs32 \up18 \uc0\u56319 \u56326 
\f0  
\fs24 \up0 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 + i\uc0\u958 x (8) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f1\fs32 \cf2 \up24 \uc0\u56319 \u56325 
\f0  \up0 \uc0\u963 
\fs21\fsmilli10667 \up10 2
\fs32 \up0 (\uc0\u969 \u8722 \u958 )
\fs21\fsmilli10667 \up10 2
\f1\fs32 \up24 \uc0\u56319 \u56326 
\f0 \up0 \uc0\u8232 \u8722  (9) 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 2\uc0\u963 
\fs21\fsmilli10667 \up8 2 
\fs24 \up0 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page133image9168256.png \width754 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 2\uc0\u8232 It\'92s value bounded at 0 is \u968 (0) = exp(\u8722 \u963  \u958  /2), so we have 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \dn3 \'88
\fs21\fsmilli10667 \up0 22 
\f1\fs32 \uc0\u56319 \u56339 
\f0  
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \'88\uc0\u8232 \u8722 2 log(\u968 (0)) (10) 
\fs24 \

\fs32 \uc0\u968 
\fs21\fsmilli10667 \dn6 j 
\fs32 \up0 = \up21 1 
\f1 \up34 \uc0\u56319 \u56323 
\f0 \up21 x
\f1 \up34 \uc0\u56319 \u56324 
\f0  \up0 (11) a
\fs21\fsmilli10667 \up8 j
\fs32 \up0 \uc0\u963  a
\fs21\fsmilli10667 \up8 j 
\fs24 \up0 \

\fs32 Where a is the scale factor. if we call \uc0\u964 , the value of the two Gabor where the plots intersect, we have, as in figure 3. 
\fs24 \

\fs32 (12) 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page133image9168448.png \width897 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 \uc0\u958 \u963  = The wavelets are therefore computed as 
\fs24 \

\fs32 \uc0\u958  g
\fs21\fsmilli10667 \up10 \uc0\u8722 1
\fs32 \up0 (\uc0\u964 ) g
\fs21\fsmilli10667 \up10 \uc0\u8722 1
\fs32 \up0 (\uc0\u964 ) + =\u958 \u8722  
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page133image9168640.png \width465 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page133image9168640.png \width465 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 aa\uc0\u963  \u963 \u8232 where g(x) = exp(\u8722 x
\fs21\fsmilli10667 \up10 2
\fs32 \up0 /2) i.e. g
\fs21\fsmilli10667 \up10 \uc0\u8722 1
\fs32 \up0 (\uc0\u964 ) = 
\f1 \up26 \uc0\u56319 \u56338 
\f0 \up0 \uc0\u8722 2log(\u964 ) So we have 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page133image9169024.png \width672 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 a+1\uc0\u8232 \u958 \u963 = \u8722 2log(\u964 )\dn22 a\uc0\u8722 1 \up0 (13) 
\fs24 \

\fs32 The value of \uc0\u958  is fixed by the fact that we need the frequency information so we set 
\fs24 \

\fs32 \uc0\u958  = 3\u960 /4 (14) Equations (10 and 13) show that the choice of \u963  is trade off between two antag- 
\fs24 \

\fs32 onist requirement on the wavelet\uc0\u8232 i. a zero-mean: \u963  should be large. 
\fs24 \

\fs32 ii. tau should be large therefore \uc0\u963  should be small 133 
\fs24 \

\f1\fs32 \uc0\u56319 \u56338 
\f0  
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page133image9169024.png \width672 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page134image6946240.png \width7937 \height9506 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Figure 3: Fourier transform of adjacent scale Gabor wavelet. \uc0\u964  has been set to 0.8 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 134 Chapter I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 With these requirements met we see that 
\fs24 \

\fs32 So we can see that these two requirements are compatible as we take more and more bands per octave. 
\fs24 \

\fs32 In the implementation, the only parameter about the wavelet that the user can set is \uc0\u964 
\fs21\fsmilli10667 \dn6 as 
\fs32 \up0 = \uc0\u964 , where \'92as\'92 stands for adjacent scales. The implementation of param- eters is the following 
\fs24 \

\fs32 i. The value of \uc0\u958  is set to 3\u960 /4.\u8232 ii. The user chooses values for \u964 ,a and j. 
\fs24 \

\fs32 iii. The value of \uc0\u963  is computed with 
\f1 \up26 \uc0\u56319 \u56338 
\f0 \up0 \uc0\u8722 2log(\u964 )a+1 
\fs24 \

\fs32 \uc0\u963 = \dn22 \uc0\u958  a\u8722 1 \up0 (16) 
\fs24 \

\fs32 There is another parameter called \uc0\u964 
\fs21\fsmilli10667 \dn6 lc
\fs32 \up0 . \'92lc\'92 stands for low-coarse. It controls the value crossing between the low pass filters \uc0\u968 
\fs21\fsmilli10667 \dn6 j 
\fs32 \up0 (a Gaussian) and the coarsest scale wavelet \uc0\u968 
\fs21\fsmilli10667 \dn6 J\uc0\u8722 1 
\fs32 \up0 and this parameter determines the value of the bandwidth of the low pas filter. 
\fs24 \
\pard\pardeftab720\sl440\sa240\partightenfactor0

\fs37\fsmilli18667 \cf2 Morlet wavelet filter 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Morlet filters are modified Gabor filters that have zero-mean: The idea is to subtract a Gabor, its envelop times a constant so that the results has zero mean: 
\fs24 \

\fs32 \uc0\u966 (x) = exp \u8722  It\'92s Fourier transform is 
\fs24 \

\fs32 2\uc0\u963 
\fs21\fsmilli10667 \up8 2 
\fs24 \up0 \

\fs32 \'88\uc0\u8232 \u968 (\u969 )=\u969 exp \u8722  
\fs24 \

\fs32 \'88\uc0\u8232 And we can see it has zero \u968 (0) = 0 
\fs24 \

\fs32 \uc0\u8722 exp \u8722  (18) 22 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f1\fs32 \cf2 \up18 \uc0\u56319 \u56325 
\f0  \dn6 x
\fs21\fsmilli10667 \up5 2 
\f1\fs32 \up18 \uc0\u56319 \u56326 
\f0  
\fs24 \up0 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 (exp(i\uc0\u958 x) \u8722  exp(\u8722 \u963  
\fs24 \
\pard\pardeftab720\sl260\sa240\partightenfactor0

\fs21\fsmilli10667 \cf2 22 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 /2)) (17) 
\fs24 \
\pard\pardeftab720\sl180\sa240\partightenfactor0

\fs16 \cf2 (a+1)\up8 2 
\fs32 \up0 \'88 
\fs16 (a\uc0\u8722 1)\up5 2 
\fs24 \up0 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 (15) 
\fs24 \

\fs32 \uc0\u968 (0) = \u964  
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page133image9169024.png \width672 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page135image9181568.png \width838 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\f1\fs32 \cf2 \up24 \uc0\u56319 \u56325 
\f0  \up0 \uc0\u963 
\fs21\fsmilli10667 \up10 2
\fs32 \up0 (\uc0\u969 \u8722 \u958 )
\fs21\fsmilli10667 \up10 2
\f1\fs32 \up24 \uc0\u56319 \u56326 
\f0  
\f1 \uc0\u56319 \u56325 
\f0  \up0 \uc0\u963 
\fs21\fsmilli10667 \up10 2
\fs32 \up0 (\uc0\u969 +\u958 )
\fs21\fsmilli10667 \up10 2
\f1\fs32 \up24 \uc0\u56319 \u56326 \u56319 \u56326 
\f0  
\fs24 \up0 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page133image9168256.png \width754 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page135image9181952.png \width746 \height13 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬} {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter I. J. Alamina 135 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page136image6937920.png \width7937 \height6122 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Figure 4: Fourier transform of adjacent scale Gabor wavelet. \uc0\u964  has been set to 0.8 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 136 Chapter I. J. Alamina 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 Appendix III - Matlab listing for scattering net- work 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 function scatter ()\uc0\u8232 %UNTITLED Summary of this function goes here\u8232 % Detailed explanation goes here\u8232 T = readtable(\'92data/cv\u8722 valid\u8722 dev.xlsx\'92,\'92ReadRowNames\'92,true); F=table2cell(T(:,\{\'92scatterc\'92,\'92wav filename\'92\})); 
\fs24 \

\fs32 all files=size(F,1);\uc0\u8232 o f m = \'92 h h : mm : s s \'92 ; ifm=\'92dd\u8722 mmm\u8722 yy HH:MM:SS.FFF\'92; 
\fs24 \

\fs32 tic ;\uc0\u8232 for i = 1:all files 
\fs24 \

\fs32 wav file=strjoin (F(i ,2)); dss file=strjoin(F(i ,1));\uc0\u8232 if exist(wav file,\'92file\'92)>0 
\fs24 \

\fs32 if exist(dss file,\'92file\'92)==0\uc0\u8232 st = transpose(scatter audio(wav file )); csvwrite( dss file , st ); 
\fs24 \

\fs32 end else 
\fs24 \

\fs32 fprintf(\'92\\nNot found:%s\'92,wav file); end 
\fs24 \

\fs32 pg=i/all files
\f2 \uc0\u8727 
\f0 100; ts=datestr (now, ifm ); tv=toc; 
\fs24 \

\fs32 d= d u r a t i o n ( s e c o n d s ( t v ) , \'92 F o r m a t \'92 , o f m ) ; pc=(all files/i
\f2 \uc0\u8727 
\f0 tv)\uc0\u8722 tv; eta=duration(seconds(pc), \'92Format\'92 ,ofm); 
\fs24 \

\fs32 if mod(i,500)==0 || i==1 || i==10 || i==100\uc0\u8232 fileID = fopen( \'92 log/dss180625 . log \'92 , \'92w+\'92 );\u8232 s=sprintf(\'92\\n%s: processing file %s\'92,ts,wav file);\u8232 f p r i n t f ( f i l e I D , \'92%s \'92 , s ) ;\u8232 f p r i n t f ( \'92%s \'92 , s ) ;\u8232 s=sprintf(\'92\\n%s : processing %d of %d files %3.2f%% complete.. ti f p r i n t f ( f i l e I D , \'92%s \'92 , s ) ;\u8232 f p r i n t f ( \'92%s \'92 , s ) ;\u8232 fclose(fileID ); 
\fs24 \

\fs32 137 
\fs24 \

\fs32 m 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 end end 
\fs24 \

\fs32 end 
\fs24 \

\fs32 function st= scatter audio(inputArg1) y=audioread ( inputArg1 ) ; 
\fs24 \

\fs32 N=length(y); T=2\'889; 
\fs24 \

\fs32 filt opt=default filter options(\'92audio\'92,T); Wop=wavelet factory 1d(N, filt opt ); 
\fs24 \

\fs32 S=scat(y,Wop); S=renorm scat(S); S=log scat(S); 
\fs24 \

\fs32 st=format scat(S); 
\fs24 \

\fs32 end 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 138 
\fs24 \

\fs32 Chapter 
\fs24 \

\fs32 I. J. Alamina 
\fs24 \
\pard\pardeftab720\sl520\sa240\partightenfactor0

\fs45\fsmilli22667 \cf2 Appendix IV - Code listing for Section 3.2.5 - Sam- ple TensorFlow client code 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 #!/ usr/bin/env python #\uc0\u8722 
\f2 \uc0\u8727 
\f0 \uc0\u8722 coding: utf\u8722 8\u8722 
\f2 \uc0\u8727 
\f0 \uc0\u8722  
\fs24 \

\fs32 \'94\'94\'94 A one\uc0\u8722 hidden\u8722 layer\u8722 MLP MNIST\u8722 classifier . \'94\'94\'94\u8232 from future import absolute import from future import division from future import print function 
\fs24 \

\fs32 # Import the training data (MNIST) 
\fs24 \

\fs32 from tensorflow . examples . tutorials . mnist import input data import tensorflow as tf 
\fs24 \

\fs32 # Possibly download and extract the MNIST data set . # Retrieve the labels as one\uc0\u8722 hot\u8722 encoded vectors . 
\fs24 \

\fs32 mnist = input data.read data sets(\'94/tmp/mnist\'94,one hot=True) 
\fs24 \

\fs32 # Create a new graph 
\fs24 \

\fs32 graph = tf.Graph() 
\fs24 \

\fs32 # Set our graph as the one to add nodes to 
\fs24 \

\fs32 with graph . as default (): 
\fs24 \

\fs32 # Placeholder for input examples (None = variable dimension) 
\fs24 \

\fs32 examples = tf.placeholder(shape=[None, 784], dtype=tf.float32) 
\fs24 \

\fs32 # Placeholder for labels 
\fs24 \

\fs32 labels = tf.placeholder(shape=[None, 10], dtype=tf.float32) 
\fs24 \

\fs32 weights = tf.Variable(tf.truncated normal(shape=[784, 10], stddev =0.1)) 
\fs24 \

\fs32 bias = tf.Variable(tf.constant(0.1, shape=[10])) 
\fs24 \

\fs32 # Apply an affine transformation to the input features 
\fs24 \

\fs32 logits = tf .matmul(examples , weights)\uc0\u8232 + bias estimates = tf.nn.softmax(logits) 
\fs24 \

\fs32 # Compute the cross\uc0\u8722 entropy\u8232 cross entropy =\u8722 tf.reduce sum(labels 
\f2 \uc0\u8727 
\f0  tf.log(estimates), 
\fs24 \

\fs32 reduction indices =[1]) 
\fs24 \

\fs32 # And finally the loss 
\fs24 \

\fs32 loss = tf .reduce mean(cross entropy) 139 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 # Create a gradient\uc0\u8722 descent optimizer that minimizes the loss . # We choose a learning rate of 0.01 
\fs24 \

\fs32 optimizer = tf . train . GradientDescentOptimizer (0.5). minimize( loss ) 
\fs24 \

\fs32 # Find the indices where the predictions were correct 
\fs24 \

\fs32 correct predictions = tf.equal(\uc0\u8232 tf .argmax(estimates , dimension=1), tf .argmax(labels , dimension=1)) 
\fs24 \

\fs32 accuracy = tf.reduce mean(\uc0\u8232 tf.cast(correct predictions , tf.float32)) 
\fs24 \

\fs32 with tf.Session(graph=graph) as session: tf.initialize all variables().run() for step in range(1001): 
\fs24 \

\fs32 example batch , label batch = mnist . train . next batch (100) feed dict = \{examples : example batch , labels : label batch\} if step % 100 == 0: 
\fs24 \

\fs32 , loss value , accuracy value =\uc0\u8232 session.run([optimizer , loss , accuracy], feed dict=feed dict) 
\fs24 \

\fs32 print(\'94Loss at time \{0\}: \{1\}\'94.format(step , loss value )) 
\fs24 \

\fs32 print(\'94Accuracy at time \{0\}:\{1\}\'94.format(step , accuracy value )) else : 
\fs24 \

\fs32 optimizer.run(feed dict) 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 140 
\fs24 \

\fs32 Chapter I. J. Alamina 
\fs24 \
\pard\pardeftab720\sl760\sa240\partightenfactor0

\fs66\fsmilli33333 \cf2 Bibliography 
\fs24 \
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Continuous wavelet transform, 2015.\uc0\u8232 2019a. URL https://github.com/mozilla/DeepSpeech#common-voice- 
\fs24 \

\fs32 training-data. 
\fs24 \

\fs32 Mozilla deepspeech, 2019b. URL https://voice.mozilla.org/en. 
\fs24 \

\fs32 Mart\uc0\u32 \u769 \u305 n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensor- flow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016. 
\fs24 \

\fs32 Mart\uc0\u32 \u769 \u305 n Abadi, Michael Isard, and Derek G Murray. A computational model for ten- sorflow: an introduction. In Proceedings of the 1st ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 1\'967. ACM, 2017. 
\fs24 \

\fs32 Alejandro Acero. Acoustical and environmental robustness in automatic speech recognition. In Proc. of ICASSP, 1990. 
\fs24 \

\fs32 Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fr\uc0\u32 \u769 ed\u32 \u769 eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, et al. Theano: A python framework for fast computation of mathematical expressions. arXiv preprint arXiv:1605.02688, 2016. 
\fs24 \

\fs32 Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-to-end speech recognition in english and man- darin. In International Conference on Machine Learning, pages 173\'96182, 2016. 
\fs24 \

\fs32 J And\uc0\u32 \u769 en, L Sifre, S Mallat, M Kapoko, V Lostanlen, and E Oyallon. Scatnet (v0. 2). Computer Software. Available: http://www.di.ens.fr/data/software/scatnet/. [Accessed: December 10, 2013], 0.2, 2014. 
\fs24 \

\fs32 Joakim And\uc0\u32 \u769 en and St\u32 \u769 ephane Mallat. Multiscale scattering for audio classification. In ISMIR, pages 657\'96662. Miami, FL, 2011. 
\fs24 \

\fs32 Joakim And\uc0\u32 \u769 en and St\u32 \u769 ephane Mallat. Deep scattering spectrum. IEEE Transactions on Signal Processing, 62(16):4114\'964128, 2014. 
\fs24 \

\fs32 Claudio Becchetti and Lucio P. Ricotti. Speech recognition: theory and C++ imple- mentation. Wiley, New York, 1998. ISBN 0471977306;9780471977308;. 
\fs24 \

\fs32 141 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 L Becchetti. The behaviour of financial time series: stylised features, theoreti- cal interpretations and proposals for hidden markov model applications. Speech recognition. Theory and C++ implementation, 1999. 
\fs24 \

\fs32 Yoshua Bengio, R\uc0\u32 \u769 ejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137\'96 1155, 2003. 
\fs24 \

\fs32 Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer- wise training of deep networks. In Advances in neural information processing systems, pages 153\'96160, 2007. 
\fs24 \

\fs32 Laurent Besacier, Etienne Barnard, Alexey Karpov, and Tanja Schultz. Automatic speech recognition for under-resourced languages: A survey. Speech Communica- tion, 56:85\'96100, 2014a. 
\fs24 \

\fs32 Laurent Besacier, Etienne Barnard, Alexey Karpov, and Tanja Schultz. Introduction to the special issue on processing under-resourced languages. 2014b. 
\fs24 \

\fs32 Mikael Boden. A guide to recurrent neural networks and backpropagation. the Dallas project, 2002. 
\fs24 \

\fs32 Peter F. Brown, Peter V. Desouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. Class-based n-gram models of natural language. Computational linguistics, 18(4):467\'96479, 1992. URL http://citeseerx.ist.psu.edu/viewdoc/ download?doi=10.1.1.13.9919&rep=rep1&type=pdf. 
\fs24 \

\fs32 Kyunghyun Cho, Bart Van Merri\uc0\u32 \u776 enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. 
\fs24 \

\fs32 Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. Attention-based models for speech recognition. In Advances in neural information processing systems, pages 577\'96585, 2015. 
\fs24 \

\fs32 Ronan Collobert, Christian Puhrsch, and Gabriel Synnaeve. Wav2letter: an end-to- end convnet-based speech recognition system. arXiv preprint arXiv:1609.03193, 2016. 
\fs24 \

\fs32 Jack D Cowan. Discussion: Mcculloch-pitts and related neural nets from 1943 to 1989. Bulletin of mathematical biology, 52(1-2):73\'9697, 1990. 
\fs24 \

\fs32 George E Dahl, Dong Yu, Li Deng, and Alex Acero. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. IEEE Transactions on audio, speech, and language processing, 20(1):30\'9642, 2012. 
\fs24 \

\fs32 Steven Davis and Paul Mermelstein. Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences. IEEE transac- tions on acoustics, speech, and signal processing, 28(4):357\'96366, 1980. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 142 Chapter I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Li Deng and Xiao Li. Machine learning paradigms for speech recognition: An overview. IEEE Transactions on Audio, Speech, and Language Processing, 21(5): 1060\'961089, 2013. 
\fs24 \

\fs32 Li Deng, Dong Yu, et al. Deep learning: methods and applications. Foundations and Trend
\f3 s\uc0\u8413 
\f0\fs21\fsmilli10667 R 
\fs32 in Signal Processing, 7(3\'964):197\'96387, 2014. 
\fs24 \

\fs32 John Dines, Junichi Yamagishi, and Simon King. Measuring the gap between hmm- based asr and tts. IEEE Journal of Selected Topics in Signal Processing, 4(6): 1046\'961058, 2010. 
\fs24 \

\fs32 Sadaoki Furui. Speaker-independent isolated word recognition using dynamic fea- tures of speech spectrum. IEEE Transactions on Acoustics, Speech, and Signal Processing, 34(1):52\'9659, 1986. 
\fs24 \

\fs32 Mark Gales, Steve Young, et al. The application of hidden markov models in speech recognition. Foundations and Trend
\f3 s\uc0\u8413 
\f0\fs21\fsmilli10667 R 
\fs32 in Signal Processing, 1(3):195\'96304, 2008. 
\fs24 \

\fs32 Mark JF Gales, Kate M Knill, Anton Ragni, and Shakti P Rath. Speech recognition and keyword spotting for low-resource languages: Babel project research at cued. In Spoken Language Technologies for Under-Resourced Languages, 2014. 
\fs24 \

\fs32 Mark John Francis Gales, Shinji Watanabe, and Eric Fosler-Lussier. Structured dis- criminative models for speech recognition: An overview. IEEE Signal Processing Magazine, 29(6):70\'9681, 2012. 
\fs24 \

\fs32 Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In Proceedings of the 34th Interna- tional Conference on Machine Learning-Volume 70, pages 1243\'961252. JMLR.org, 2017. 
\fs24 \

\fs32 Arnab Ghoshal, Pawel Swietojanski, and Steve Renals. Multilingual training of deep neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 7319\'967323. IEEE, 2013. 
\fs24 \

\fs32 Jean-Philippe Goldman. Easyalign: an automatic phonetic alignment tool under praat. 2011. 
\fs24 \

\fs32 Peter Goldsborough. A tour of tensorflow. arXiv preprint arXiv:1610.01178, 2016. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 
\fs24 \

\fs32 2016. http://www.deeplearningbook.org.\uc0\u8232 Alex Graves. Supervised sequence labelling with recurrent neural networks. Springer, 
\fs24 \

\fs32 2014. 
\fs24 \

\fs32 Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recur- rent neural networks. In International Conference on Machine Learning, pages 1764\'961772, 2014. 
\fs24 \

\fs32 Alex Graves, Santiago Ferna\uc0\u32 \u769 ndez, Faustino Gomez, and Ju\u32 \u776 rgen Schmidhuber. Con- nectionist temporal classification: labelling unsegmented sequence data with re- current neural networks. In Proceedings of the 23rd international conference on Machine learning, pages 369\'96376. ACM, 2006. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter I. J. Alamina 143 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Alex Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. Hybrid speech recog- nition with deep bidirectional lstm. In Automatic Speech Recognition and Under- standing (ASRU), 2013 IEEE Workshop on, pages 273\'96278. IEEE, 2013. 
\fs24 \

\fs32 Frantisek Grezl and Petr Fousek. Optimizing bottle-neck features for lvcsr. In ICASSP, volume 8, pages 4729\'964732, 2008. 
\fs24 \

\fs32 Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567, 2014a. 
\fs24 \

\fs32 Awni Y Hannun, Andrew L Maas, Daniel Jurafsky, and Andrew Y Ng. First- pass large vocabulary continuous speech recognition using bi-directional recurrent dnns. arXiv preprint arXiv:1408.2873, 2014b. 
\fs24 \

\fs32 Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. Scal- able modified Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690\'96696, Sofia, Bulgaria, August 2013. URL https://kheafield.com/papers/ edinburgh/estimate paper.pdf. 
\fs24 \

\fs32 Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness and uncertainty. arXiv preprint arXiv:1901.09960, 2019. 
\fs24 \

\fs32 Hynek Hermansky. Perceptual linear predictive (plp) analysis of speech. The Journal of the Acoustical Society of America, 87(4):1738\'961752, 1990. 
\fs24 \

\fs32 Hynek Hermansky and Nelson Morgan. Rasta processing of speech. IEEE transac- tions on speech and audio processing, 2(4):578\'96589, 1994. 
\fs24 \

\fs32 Herbert Jaeger. Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the\'94 echo state network\'94 approach, volume 5. GMD- Forschungszentrum Informationstechnik Bonn, 2002. 
\fs24 \

\fs32 F. Jelinek. Continuous speech recognition by statistical methods. Proceedings of the IEEE, 64(4):532\'96556, 1976. doi: 10.1109/PROC.1976.10159. 
\fs24 \

\fs32 Bing-Hwang Juang and S. Furui. Automatic recognition and understanding of spoken language - a first step toward natural human-machine communication. Proceedings of the IEEE, 88(8):1142\'961165, 2000. doi: 10.1109/5.880077. URL http://ieeexplore.ieee.org/document/880077. 
\fs24 \

\fs32 L
\f1 \uc0\u56319 \u56320 
\f0 ukaszKaiserandSamyBengio.Canactivememoryreplaceattention?InAdvances in Neural Information Processing Systems, pages 3781\'963789, 2016. 
\fs24 \

\fs32 Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099, 2016. 
\fs24 \

\fs32 Herman Kamper, Aren Jansen, and Sharon Goldwater. Unsupervised word seg- mentation and lexicon discovery using acoustic word embeddings. IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), 24(4):669\'96 679, 2016. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page44image9003072.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 144 Chapter I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Nikhil Ketkar. Introduction to pytorch. In Deep learning with python, pages 195\'96208. Springer, 2017. 
\fs24 \

\fs32 Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language models. In AAAI, pages 2741\'962749, 2016. 
\fs24 \

\fs32 Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 
\fs24 \

\fs32 Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\'961105, 2012. 
\fs24 \

\fs32 Julius Kunze, Louis Kirsch, Ilia Kurenkov, Andreas Krug, Jens Johannsmeier, and Sebastian Stober. Transfer learning for speech recognition on a budget. arXiv preprint arXiv:1706.00290, 2017. 
\fs24 \

\fs32 Paul Lamere, Philip Kwok, Evandro Gouv\'88ea, Bhiksha Raj, Rita Singh, William Walker, Manfred Warmuth, and Peter Wolf. The cmu sphinx-4 speech recognition system, 2003. 
\fs24 \

\fs32 Julia A Lasserre, Christopher M Bishop, and Thomas P Minka. Principled hybrids of generative and discriminative models. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\'9206), volume 1, pages 87\'9694. IEEE, 2006. 
\fs24 \

\fs32 Thang Luong, Richard Socher, and Christopher D Manning. Better word represen- tations with recursive neural networks for morphology. In CoNLL, pages 104\'96113, 2013. 
\fs24 \

\fs32 J Lyons. Mel frequency cepstral coefficient (mfcc) tutorial, 2012. URL 
\fs24 \

\fs32 http://practicalcryptography.com/miscellaneous/machine-learning/ guide-mel-frequency-cepstral-coefficients-mfccs/. 
\fs24 \

\fs32 St\uc0\u32 \u769 ephane Mallat. Understanding deep convolutional networks. Phil. Trans. R. Soc. A, 374(2065):20150203, 2016. 
\fs24 \

\fs32 Stephane G Mallat. A theory for multiresolution signal decomposition: the wavelet representation. IEEE transactions on pattern analysis and machine intelligence, 11(7):674\'96693, 1989. 
\fs24 \

\fs32 Ian McLoughlin. Applied speech and audio processing: with Matlab examples. Cam- bridge University Press, 2009. 
\fs24 \

\fs32 \uc0\u711 \u8232 Tom\u32 \u769 a\u711 s Mikolov, Anoop Deoras, Stefan Kombrink, Luka\u32 \u769 \u711 s Burget, and Jan Cernocky`. 
\fs24 \

\fs32 Empirical evaluation and combination of advanced language modeling techniques. In Twelfth Annual Conference of the International Speech Communication Asso- ciation, 2011. 
\fs24 \

\fs32 Abdel-rahman Mohamed, George Dahl, and Geoffrey Hinton. Deep belief networks for phone recognition. In Nips workshop on deep learning for speech recognition and related applications, volume 1, page 39. Vancouver, Canada, 2009. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter I. J. Alamina 145 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Abdel-rahman Mohamed, George E Dahl, Geoffrey Hinton, et al. Acoustic modeling using deep belief networks. IEEE Trans. Audio, Speech & Language Processing, 20(1):14\'9622, 2012. 
\fs24 \

\fs32 Scott Novotney and Richard Schwartz. Analysis of low-resource acoustic model self- training. In Tenth Annual Conference of the International Speech Communication Association, 2009. 
\fs24 \

\fs32 Jay F Nunamaker Jr, Minder Chen, and Titus DM Purdin. Systems development in information systems research. Journal of management information systems, 7 (3):89\'96106, 1990. 
\fs24 \

\fs32 Tsubasa Ochiai, Shinji Watanabe, Takaaki Hori, and John R Hershey. Multichannel end-to-end speech recognition. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2632\'962641. JMLR. org, 2017. 
\fs24 \

\fs32 Travis Oliphant. NumPy: A guide to NumPy. USA: Trelgol Publishing, 2006\'96. URL http://www.numpy.org/. [Online; accessed \'a1today\'bf]. 
\fs24 \

\fs32 Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Lib- rispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5206\'965210. IEEE, 2015. 
\fs24 \

\fs32 Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\'96318. Association for Computational Linguistics, 2002. 
\fs24 \

\fs32 Douglas B Paul and Janet M Baker. The design for the wall street journal-based csr corpus. In Proceedings of the workshop on Speech and Natural Language, pages 357\'96362. Association for Computational Linguistics, 1992. 
\fs24 \

\fs32 Vijayaditya Peddinti, TaraN Sainath, Shay Maymon, Bhuvana Ramabhadran, David Nahamoo, and Vaibhava Goel. Deep scattering spectrum with deep neural net- works. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE Inter- national Conference on, pages 210\'96214. IEEE, 2014. 
\fs24 \

\fs32 Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vec- tors for word representation. In Proceedings of the 2014 Conference on Em- pirical Methods in Natural Language Processing (EMNLP), pages 1532\'961543, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1162. 
\fs24 \

\fs32 Joseph Picone. Fundamentals of speech recognition: A short course. Institute for Signal and Information Processing, Mississippi State University, 1996. 
\fs24 \

\fs32 Daniel Povey, Luka\uc0\u32 \u769 \u711 s Burget, Mohit Agarwal, Pinar Akyazi, Feng Kai, Arnab Ghoshal, Ond\u711 rej Glembek, Nagendra Goel, Martin Karafi\u32 \u769 at, Ariya Rastrow, et al. The subspace gaussian mixture model\'97a structured model for speech recognition. Computer Speech & Language, 25(2):404\'96439, 2011a. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 146 Chapter I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al. The kaldi speech recognition toolkit. In IEEE 2011 workshop on automatic speech recognition and understanding, number CONF. IEEE Signal Processing Society, 2011b. 
\fs24 \

\fs32 Anton Ragni and Mark JF Gales. Automatic speech recognition system development in the\'94 wild\'94. In Interspeech, pages 2217\'962221, 2018. 
\fs24 \

\fs32 Anton Ragni, Katherine Mary Knill, Shakti P Rath, and Mark John Gales. Data augmentation for low resource languages. 2014. 
\fs24 \

\fs32 Prajit Ramachandran, Peter J Liu, and Quoc V Le. Unsupervised pretraining for sequence to sequence learning. arXiv preprint arXiv:1611.02683, 2016. 
\fs24 \

\fs32 A. Rosenberg, K. Audhkhasi, A. Sethy, B. Ramabhadran, and M. Picheny. End- to-end speech recognition and keyword search on low-resource languages. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5280\'965284, March 2017. doi: 10.1109/ICASSP.2017.7953164. 
\fs24 \

\fs32 Charles Ogan D. S. Okrika: A kingdom of the Niger Delta. Onyoma Research Publications, Port Harcourt, Rivers State, Nigeria, 1 edition, 2008. 
\fs24 \

\fs32 Tara N Sainath, Vijayaditya Peddinti, Brian Kingsbury, Petr Fousek, Bhuvana Ram- abhadran, and David Nahamoo. Deep scattering spectra with deep neural net- works for lvcsr tasks. In Fifteenth Annual Conference of the International Speech Communication Association, 2014. 
\fs24 \

\fs32 Hasim Sak, Andrew W. Senior, and Fran\uc0\u32 \u807 coise Beaufays. Long short-term memory based recurrent neural network architectures for large vocabulary speech recogni- tion. CoRR, abs/1402.1128, 2014. URL http://arxiv.org/abs/1402.1128. 
\fs24 \

\fs32 George Saon, Hong-Kwang J Kuo, Steven Rennie, and Michael Picheny. The ibm 2015 english conversational telephone speech recognition system. arXiv preprint arXiv:1505.05899, 2015. 
\fs24 \

\fs32 Ralf Schluter and Hermann Ney. Model-based mce bound to the true bayes\'92 error. IEEE Signal Processing Letters, 8(5):131\'96133, 2001. 
\fs24 \

\fs32 Laurent Sifre and St\uc0\u32 \u769 ephane Mallat. Rotation, scaling and deformation invariant scattering for texture discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1233\'961240, 2013. 
\fs24 \

\fs32 Laurent Sifre and St\uc0\u32 \u769 ephane Mallat. Rigid-motion scattering for image classification. Ph. D. dissertation, 2014. 
\fs24 \

\fs32 Gary F. Simons and Charles D. Fennig. Ethnologue: Languages of the world, twenty- first edition., 2018. URL http://www.ethnologue.com. 
\fs24 \

\fs32 Paul Smolensky. Information processing in dynamical systems: Foundations of har- mony theory. Technical report, COLORADO UNIV AT BOULDER DEPT OF COMPUTER SCIENCE, 1986. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter I. J. Alamina 147 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfit- ting. The journal of machine learning research, 15(1):1929\'961958, 2014. 
\fs24 \

\fs32 Adriana Stan, Yoshitaka Mamiya, Junichi Yamagishi, Peter Bell, Oliver Watts, Robert AJ Clark, and Simon King. Alisa: An automatic lightly supervised speech segmentation and alignment tool. Computer Speech & Language, 35:116\'96133, 2016. 
\fs24 \

\fs32 Stanley Smith Stevens, John Volkmann, and Edwin B Newman. A scale for the measurement of the psychological magnitude pitch. The Journal of the Acoustical Society of America, 8(3):185\'96190, 1937. 
\fs24 \

\fs32 Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\'963112, 2014. 
\fs24 \

\fs32 Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1\'969, 2015. 
\fs24 \

\fs32 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,L
\f1 \uc0\u56319 \u56320 
\f0 ukaszKaiser,andIlliaPolosukhin.Attentionisallyouneed.InAdvances in Neural Information Processing Systems, pages 5998\'966008, 2017. 
\fs24 \

\fs32 Maarten Versteegh, Roland Thiolliere, Thomas Schatz, Xuan Nga Cao, Xavier Anguera, Aren Jansen, and Emmanuel Dupoux. The zero resource speech chal- lenge 2015. In Sixteenth Annual Conference of the International Speech Commu- nication Association, 2015. 
\fs24 \

\fs32 Ngoc Thang Vu and Tanja Schultz. Multilingual multilayer perceptron for rapid language adaptation between and across language families. In Interspeech, pages 515\'96519, 2013. 
\fs24 \

\fs32 Willie Walker, Paul Lamere, Philip Kwok, Bhiksha Raj, Rita Singh, Evandro Gou- vea, Peter Wolf, and Joe Woelfel. Sphinx-4: A flexible open source framework for speech recognition. 2004. 
\fs24 \

\fs32 Chengyi Wang, Yu Wu, Shujie Liu, Zhenglu Yang, and Ming Zhou. Bridging the gap between pre-training and fine-tuning for end-to-end speech translation. arXiv preprint arXiv:1909.07575, 2019. 
\fs24 \

\fs32 Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchintala, and Tsubasa Ochiai. Espnet: End-to-end speech processing toolkit. In Interspeech, pages 2207\'962211, 2018. doi: 10.21437/ Interspeech.2018-1456. URL http://dx.doi.org/10.21437/Interspeech.2018- 1456. 
\fs24 \

\fs32 Shinji (. e. Watanabe and Jen-Tzung Chien. Bayesian speech and lan- guage processing. Cambridge University Press, Cambridge, 2015. ISBN 1107055571;9781107055575;. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 148 Chapter I. J. Alamina 
\fs24 \

\fs32 Deep Scattering and End-to-End Speech Models towards Low Resource Speech Recognition 
\fs24 \

\fs32 PC Woodland and Daniel Povey. Large scale discriminative training for speech recognition. In ASR2000-Automatic Speech Recognition: Challenges for the new Millenium ISCA Tutorial and Research Workshop (ITRW), 2000. 
\fs24 \

\fs32 Ping Xu and Pascale Fung. Cross-lingual language modeling for low-resource speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 21 (6):1134\'961144, 2013. 
\fs24 \

\fs32 Steve Young. A review of large-vocabulary continuous-speech. IEEE Signal Pro- cessing Magazine, 13(5):45, 1996. doi: 10.1109/79.536824. 
\fs24 \

\fs32 Steve Young, Gunnar Evermann, Mark Gales, Thomas Hain, Dan Kershaw, Xunying Liu, Gareth Moore, Julian Odell, Dave Ollason, Dan Povey, et al. The htk book. Cambridge university engineering department, 3:175, 2002. 
\fs24 \

\fs32 Dong Yu and Li Deng. AUTOMATIC SPEECH RECOGNITION. Springer, 2016. 
\fs24 \

\fs32 Dong Yu, Li Deng, and George Dahl. Roles of pre-training and fine-tuning in context-dependent dbn-hmms for real-world speech recognition. In Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2010. 
\fs24 \

\fs32 Neil Zeghidour, Gabriel Synnaeve, Maarten Versteegh, and Emmanuel Dupoux. A deep scattering spectrum\'97deep siamese network pipeline for unsupervised acous- tic modeling. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pages 4965\'964969. IEEE, 2016. 
\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\sl280\partightenfactor0
\cf2  {{\NeXTGraphic page7image9077504.png \width5372 \height12 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\sl360\sa240\partightenfactor0

\fs32 \cf2 Chapter I. J. Alamina 149 
\fs24 \
}